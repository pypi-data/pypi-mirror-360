"""Takes information from the Stanford Club Sports website and makes it available in code for a standalone application. Useful for websites."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_pipe.ipynb.

# %% auto 0
__all__ = ['get_page', 'get_roster', 'get_staff', 'get_academic_year', 'get_schedule', 'get_stories']

# %% ../nbs/00_pipe.ipynb 3
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse
from datetime import datetime
import os, re, json

# %% ../nbs/00_pipe.ipynb 4
def get_page(url: str):
    "Get BS4 soup object and base url helper"
    # Get the base URL
    parsed = urlparse(url)
    base_url = f"{parsed.scheme}://{parsed.netloc}"

    # Query the website
    response = requests.get(url)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, "html.parser")
    return base_url, soup

# %% ../nbs/00_pipe.ipynb 5
def get_roster(roster_url: str):
    "Get a formatted Club Sports roster"

    base_url, soup = get_page(roster_url)

    # Get roster
    coaches_section = soup.find("section", id="sidearm-roster-coaches")
    coaches = []
    for li in coaches_section.select("li.sidearm-roster-coach"):
        title = li.select_one(".sidearm-roster-coach-title span")
        name = li.select_one(".sidearm-roster-coach-name p")
        link = li.select_one(".sidearm-roster-coach-link a")

        coaches.append({
            "title": title.text.strip() if title else None,
            "name": name.text.strip() if name else None,
            "profile_url": os.path.join(base_url + link['href']) if link and link['href'] else None,
        })

    # TODO: This page may get updated with players, but as of now for cycling this is not relevant.

    return coaches

# %% ../nbs/00_pipe.ipynb 8
def get_staff(staff_url: str):
    "Get and format Club Sport staff"
    
    # Get page
    base_url, soup = get_page(staff_url)
    rows = soup.select("tr")
    headers = [th.get_text(strip=True).lower().replace(" ", "_") for th in rows[0].find_all("th")]
    
    # Parse Staff Table
    staff = []
    for row in rows[1:]:
        if "sidearm-coaches-coach" not in row.get("class", []):
            continue

        cells = row.find_all(["th", "td"])
        data = {}
        for i, header in enumerate(headers):
            if i >= len(cells):
                data[header] = None
                continue

            cell = cells[i]            
            if i == 0: # First column (name) with profile link
                a = cell.find("a")
                if a:
                    data["name"] = a.get_text(strip=True)
                    href = a.get("href", "")
                    data["profile_url"] = href if href.startswith("http") else base_url + href
                else:
                    data["name"] = cell.get_text(strip=True)
                    data["profile_url"] = None
            else: # Other columns
                data[header] = cell.get_text(strip=True)

        staff.append(data)
        
    return staff

# %% ../nbs/00_pipe.ipynb 11
def get_academic_year():
    "Helper to get the academic year"
    now = datetime.now()
    if now.month >= 9:  # September or later
        return f"{now.year}-{str(now.year + 1)[-2:]}"
    else:
        return f"{now.year - 1}-{str(now.year)[-2:]}"

# %% ../nbs/00_pipe.ipynb 13
def get_schedule(schedule_url:str, season:str=None, naive=False):
    "Get and parse a club sport schedule table."

    # Default to current season
    if not season:
        season = get_academic_year()

    # Query the page
    url = f"{schedule_url}/{season}?grid=true"
    base_url, soup = get_page(url)

    # Get the events rows
    rows = soup.select("tr", class_="sidearm-schedule-game")
    if not rows or len(rows) < 2:
        raise ValueError("No schedule rows found")

    # First row contains the headers
    headers = [th.get_text(strip=True).lower().replace(" ", "_") for th in rows[0].find_all("th")]

    schedule = []
    for row in rows[1:]:
        # Check valid row
        if "sidearm-schedule-game" not in row.get("class", []):
            continue  # Skip non-game rows
        cells = row.find_all("td")
        if not cells:
            continue
        
        # Naive == Match Cell Contents
        if naive:
            row_data = {h: cells[i].get_text().strip() for i,h in enumerate(headers)}
            schedule.append(row_data)
            continue
        
        # Non-Naive == Actually get the link address of <a> tags
        row_data = {}
        for i, h in enumerate(headers):
            if i >= len(cells):
                row_data[h] = None
                continue

            cell = cells[i]
            link = cell.find("a")
            
            if link and link.has_attr("href"):
                href = link["href"]
                if not href.startswith("http"): # relative URLs absolute
                    href = base_url + href
                row_data[h] = href
            else:
                row_data[h] = cell.get_text(strip=True)
        schedule.append(row_data)
        
    return schedule

# %% ../nbs/00_pipe.ipynb 19
def get_stories(home_url: str):
    "Get story blurbs from Club Sports website"

    # Fetch page
    base_url, soup = get_page(home_url)

    # The stories are currently stored in a JavaScript Object
    ss = soup.find("div", class_="s-stories__inner")
    script = ss.find("script")
    obj_str = re.search(r"var obj\s*=\s*(\{.*\});", script.string, re.DOTALL)
    obj = json.loads(obj_str.group(1))

    stories = []
    for s_obj in obj["data"]:
        t_soup = BeautifulSoup(s_obj["teaser"], "html.parser")
        teaser = t_soup.get_text(strip=True)

        story = {
            "title": s_obj["content_title"],
            "date": s_obj["content_date"],
            "story_url": s_obj["content_url"],
            "image_url": s_obj["content_image_url"],
            "teaser": teaser
        }
        stories.append(story)

    return stories
