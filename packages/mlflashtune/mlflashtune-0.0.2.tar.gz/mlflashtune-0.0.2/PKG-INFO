Metadata-Version: 2.4
Name: mlflashtune
Version: 0.0.2
Summary: ML Flash Tune - Advanced ensemble optimization system for LightGBM hyperparameter tuning
Author-email: MLFlashTune Development Team <mlflashtune-dev@example.com>
License-Expression: MIT
Project-URL: Homepage, https://github.com/example/mlflashtune
Project-URL: Documentation, https://github.com/example/mlflashtune/docs
Project-URL: Repository, https://github.com/example/mlflashtune
Project-URL: Issues, https://github.com/example/mlflashtune/issues
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: ax-platform>=0.3.0
Requires-Dist: lightgbm<5.0.0,>=3.3.0
Requires-Dist: polars>=0.18.0
Requires-Dist: pandas<3.0.0,>=1.5.0
Requires-Dist: numpy>=1.21.0
Requires-Dist: scikit-learn<2.0.0,>=1.1.0
Requires-Dist: matplotlib>=3.5.0
Requires-Dist: joblib>=1.1.0
Requires-Dist: flask<4.0.0,>=2.0.0
Requires-Dist: plotly<7.0.0,>=5.0.0
Requires-Dist: seaborn>=0.11.0
Requires-Dist: pyarrow>=17.0.0
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: pytest-cov>=3.0.0; extra == "dev"
Requires-Dist: coverage>=6.0.0; extra == "dev"
Requires-Dist: black>=22.0.0; extra == "dev"
Requires-Dist: flake8>=5.0.0; extra == "dev"
Requires-Dist: mypy>=0.991; extra == "dev"
Requires-Dist: sphinx>=5.0.0; extra == "dev"
Requires-Dist: sphinx-rtd-theme>=1.0.0; extra == "dev"
Requires-Dist: jupyter>=1.0.0; extra == "dev"
Requires-Dist: ipython>=8.0.0; extra == "dev"
Dynamic: license-file

# MLFlashTune

[![PyPI version](https://badge.fury.io/py/mlflashtune.svg)](https://badge.fury.io/py/mlflashtune)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

MLFlashTune is a comprehensive ensemble optimization system for Bayesian hyperparameter optimization of LightGBM models. It provides automated machine learning capabilities with a focus on speed, accuracy, and ease of use.

## Features

- üöÄ **Fast Optimization**: Advanced Bayesian optimization algorithms
- üéØ **LightGBM Ensembles**: Automated ensemble model creation and tuning
- üåê **Web Interface**: Interactive visualization and analysis tools
- ‚öôÔ∏è **Flexible Configuration**: Environment-based configuration system
- üìä **Rich Analytics**: Comprehensive performance analysis and visualization
- üîß **Easy CLI**: Simple command-line interface for all operations

## Installation

```bash
pip install mlflashtune
```

For development installation:

```bash
git clone https://github.com/your-repo/mlflashtune
cd mlflashtune
pip install -e .[dev]
```

## Quick Start

### 1. Basic Usage

```bash
# Run optimization with default settings
python -m mlflashtune.cli --environment development

# Run with custom parameters
python -m mlflashtune.cli --config config/environments/development.json --trials 20
```

### 2. Configuration

Create a configuration file or use one of the provided templates:

```json
{
  "DATA_PATH": "data/your_dataset.parquet",
  "TARGET_COLUMN": "target",
  "FEATURES": ["feature1", "feature2", "feature3"],
  "CATEGORICAL_FEATURES": ["feature1"],
  "AE_NUM_TRIALS": 50,
  "N_ENSEMBLE_GROUP_NUMBER": 20,
  "RANDOM_SEED": 42
}
```

### 3. Web Interface

```bash
# Start interactive web interface
python -m mlflashtune.web.interactive.app

# Or use the convenience script
./scripts/start-web.sh
```

## Architecture

MLFlashTune is organized into several key modules:

- **`mlflashtune.core`**: Core optimization engine and configuration management
- **`mlflashtune.cli`**: Command-line interface
- **`mlflashtune.web`**: Web-based visualization and analysis tools

## Configuration System

The system uses a hierarchical configuration approach:

1. **Environment configs**: Main configuration files per environment
2. **Hyperparameter spaces**: Python modules defining search spaces  
3. **Templates**: Predefined configs for specific domains

## Requirements

- Python 3.8+
- LightGBM 3.3.0+
- Pandas, NumPy, Scikit-learn
- Flask (for web interface)
- Plotly, Matplotlib (for visualization)

## Performance Considerations

- Always set `OMP_NUM_THREADS=1` for LightGBM to avoid thread conflicts
- Parallel training is controlled via configuration parameters
- Optimization algorithms benefit from multiple CPU cores

## Examples

### Development Run (Fast)
```bash
# 15 trials, 10 models (~15-20 minutes)
OMP_NUM_THREADS=1 python -m mlflashtune.cli --environment development
```

### Production Run
```bash
# Full optimization with more trials
OMP_NUM_THREADS=1 python -m mlflashtune.cli --environment production
```

### Validation
```bash
# Validate configuration without running optimization
python -m mlflashtune.cli --config config/environments/development.json --validate
```

## Data Requirements

- Input data should be in Parquet, CSV, or other pandas-compatible formats
- Target column must be binary (0/1) for classification
- Features are automatically handled by LightGBM (nulls, categorical encoding)
- Categorical features should be specified in configuration

## Output Structure

All outputs are organized under `outputs/`:
- `outputs/runs/`: Individual optimization run results
- `outputs/best_trials/`: Best performing configurations  
- `outputs/logs/`: Execution logs
- `outputs/visualizations/`: Generated plots and analysis

## CLI Commands

The package provides several command-line entry points:

- `mlflashtune-optimize`: Main optimization CLI
- `mlflashtune-web`: Web interface launcher  
- `mlflashtune-analyze`: Analysis tools

## Contributing

We welcome contributions! Please see our contributing guidelines for details.

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Citation

If you use MLFlashTune in your research, please cite:

```bibtex
@software{mlflashtune,
  title={MLFlashTune: Fast Ensemble Optimization with Advanced Bayesian Methods},
  author={MLFlashTune Development Team},
  url={https://github.com/your-repo/mlflashtune},
  version={0.0.2},
  year={2025}
}
```
