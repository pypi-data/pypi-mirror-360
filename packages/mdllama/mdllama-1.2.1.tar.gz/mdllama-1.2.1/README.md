# mdllama

A CLI tool that lets you chat with Ollama models right from your terminal, with built-in Markdown rendering.

mdllama makes it easy to interact with Ollama's AI models directly from your command line, meanwhile providing you with real-time Markdown rendering

## Features

- Chat with Ollama models from the terminal
- Built-in Markdown rendering
- Simple installation and removal (see below)

## Screenshots

### Chat Interface
![Chat](https://raw.githubusercontent.com/QinCai-rui/mdllama/refs/heads/main/assets/chat.png)

### Help
![Help](https://raw.githubusercontent.com/QinCai-rui/mdllama/refs/heads/main/assets/help.png)

## Demo

<video controls src="https://raw.githubusercontent.com/QinCai-rui/mdllama/refs/heads/main/assets/demo.webm" title="Demo"></video>
Note: If the video does not play, you can download it [here](https://raw.githubusercontent.com/QinCai-rui/mdllama/refs/heads/main/assets/demo.webm).

## Installation

To install **mdllama**, run:

```bash
bash <(curl -fsSL https://raw.githubusercontent.com/QinCai-rui/mdllama/refs/heads/main/install.sh)
```

## Uninstallation

To uninstall **mdllama**, run:

```bash
bash <(curl -fsSL https://raw.githubusercontent.com/QinCai-rui/mdllama/refs/heads/main/uninstall.sh)
```

## License

This project is licensed under the GNU General Public License v3.0. See the [LICENSE](LICENSE) file for details.

---
