Metadata-Version: 2.4
Name: synth-codeai
Version: 0.30.2
Summary: synth.codeai - ReAct Aid
Project-URL: Homepage, https://github.com/Ifham58741/synth.codeai
Project-URL: Documentation, https://github.com/Ifham58741/synth.codeai#readme
Project-URL: Repository, https://github.com/Ifham58741/synth.codeai.git
Project-URL: Issues, https://github.com/Ifham58741/synth.codeai/issues
Author-email: MOHD IFHAMULLAH <mohdbsccsiwan@gmail.com>
License-File: LICENSE
Keywords: agent,ai,development,langchain,tools
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Requires-Python: >=3.10
Requires-Dist: boto3>=1.38.26
Requires-Dist: fastapi>=0.104.0
Requires-Dist: fuzzywuzzy==0.18.0
Requires-Dist: gitpython>=3.1
Requires-Dist: jinja2>=3.1.2
Requires-Dist: langchain-anthropic>=0.3.10
Requires-Dist: langchain-aws>=0.2.24
Requires-Dist: langchain-core>=0.3.48
Requires-Dist: langchain-deepseek>=0.1.2
Requires-Dist: langchain-fireworks>=0.2.8
Requires-Dist: langchain-google-genai>=2.1.1
Requires-Dist: langchain-groq>=0.3.1
Requires-Dist: langchain-mcp-adapters>=0.0.5
Requires-Dist: langchain-ollama>=0.3.0
Requires-Dist: langchain-openai>=0.3.10
Requires-Dist: langchain-text-splitters>=0.3.7
Requires-Dist: langchain>=0.3.21
Requires-Dist: langgraph-checkpoint>=2.0.23
Requires-Dist: langgraph-prebuilt>=0.1.0
Requires-Dist: langgraph-sdk>=0.1.59
Requires-Dist: langgraph>=0.3.20
Requires-Dist: litellm>=1.60.6
Requires-Dist: mcp[cli]>=1.4.1
Requires-Dist: packaging
Requires-Dist: pathspec>=0.11.0
Requires-Dist: peewee-migrate>=1.13.0
Requires-Dist: peewee>=3.17.9
Requires-Dist: platformdirs>=3.17.9
Requires-Dist: prompt-toolkit
Requires-Dist: protobuf<=5.29.3,>=5.0.0
Requires-Dist: pyte>=0.8.2
Requires-Dist: python-levenshtein>=0.26.1
Requires-Dist: python-magic>=0.4.27
Requires-Dist: rapidfuzz>=3.11.0
Requires-Dist: requests
Requires-Dist: rich>=13.0.0
Requires-Dist: tavily-python>=0.5.0
Requires-Dist: uvicorn>=0.24.0
Requires-Dist: websockets>=12.0
Provides-Extra: dev
Requires-Dist: pytest-cov>=6.0.0; extra == 'dev'
Requires-Dist: pytest-mock>=3.14.0; extra == 'dev'
Requires-Dist: pytest-timeout>=2.2.0; extra == 'dev'
Requires-Dist: pytest>=7.0.0; extra == 'dev'
Description-Content-Type: text/markdown



RA.Aid (pronounced "raid") helps you develop software autonomously. It is a standalone coding agent built on LangGraph's agent-based task execution framework. The tool provides an intelligent assistant that can help with research, planning, and implementation of multi-step development tasks. RA.Aid can optionally integrate with `aider` (https://aider.chat/) via the `--use-aider` flag to leverage its specialized code editing capabilities.

The result is **near-fully-autonomous software development**.

**Enjoying RA.Aid?** Show your support by giving us a star ‚≠ê on [GitHub](https://github.com/Ifham58741/synth.codeai)!

Here's a demo of RA.Aid adding a feature to itself:

<img src="assets/demo-synth-codeai-task-1.gif" alt="RA.Aid Demo" autoplay loop style="width: 100%; max-width: 800px;">


## Table of Contents

- [Features](#features)
- [Installation](#installation)
- [Usage](#usage)
- [Architecture](#architecture)
- [Dependencies](#dependencies)
- [Development Setup](#development-setup)
- [Contributing](#contributing)
- [License](#license)
- [Contact](#contact)

> üëã **Pull requests are very welcome!** Have ideas for how to improve RA.Aid? Don't be shy - your help makes a real difference!
>


‚ö†Ô∏è **IMPORTANT: USE AT YOUR OWN RISK** ‚ö†Ô∏è
- This tool **can and will** automatically execute shell commands and make code changes
- The --cowboy-mode flag can be enabled to skip shell command approval prompts
- No warranty is provided, either express or implied
- Always use in version-controlled repositories
- Review proposed changes in your git diff before committing

## Key Features

- **Multi-Step Task Planning**: The agent breaks down complex tasks into discrete, manageable steps and executes them sequentially. This systematic approach ensures thorough implementation and reduces errors.

- **Automated Command Execution**: The agent can run shell commands automatically to accomplish tasks. While this makes it powerful, it also means you should carefully review its actions.

- **Ability to Leverage Expert Reasoning Models**: The agent can use advanced reasoning models such as OpenAI's o1 *just when needed*, e.g. to solve complex debugging problems or in planning for complex feature implementation.

- **Web Research Capabilities**: Leverages Tavily API for intelligent web searches to enhance research and gather real-world context for development tasks

- **Three-Stage Architecture**:
  1. **Research**: Analyzes codebases and gathers context
  2. **Planning**: Breaks down tasks into specific, actionable steps
  3. **Implementation**: Executes each planned step sequentially

What sets RA.Aid apart is its ability to handle complex programming tasks that extend beyond single-shot code edits. By combining research, strategic planning, and implementation into a cohesive workflow, RA.Aid can:

- Break down and execute multi-step programming tasks
- Research and analyze complex codebases to answer architectural questions
- Plan and implement significant code changes across multiple files
- Provide detailed explanations of existing code structure and functionality
- Execute sophisticated refactoring operations with proper planning

## Features

- **Three-Stage Architecture**: The workflow consists of three powerful stages:
  1. **Research** üîç - Gather and analyze information
  2. **Planning** üìã - Develop execution strategy
  3. **Implementation** ‚ö° - Execute the plan with AI assistance

  Each stage is powered by dedicated AI agents and specialized toolsets.
- **Advanced AI Integration**: Built on LangChain and leverages the latest LLMs for natural language understanding and generation.
- **Human-in-the-Loop Interaction**: Optional mode that enables the agent to ask you questions during task execution, ensuring higher accuracy and better handling of complex tasks that may require your input or clarification
- **Comprehensive Toolset**:
  - Shell command execution
  - Expert querying system
  - File operations and management
  - Memory management
  - Research and planning tools
  - Code analysis capabilities
- **Interactive CLI Interface**: Simple yet powerful command-line interface for seamless interaction
- **Modular Design**: Structured as a Python package with specialized modules for console output, processing, text utilities, and tools
- **Git Integration**: Built-in support for Git operations and repository management

## Installation

### Windows Installation
1. Install Python 3.8 or higher from [python.org](https://www.python.org/downloads/)
2. Install required system dependencies:
   ```powershell
   # Install Chocolatey if not already installed (run in admin PowerShell)
   Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))
   
   # Install ripgrep using Chocolatey
   choco install ripgrep
   ```
3. Install RA.Aid:
   ```powershell
   pip install synth-codeai
   ```
4. Install Windows-specific dependencies:
   ```powershell
   pip install pywin32
   ```
5. Set up your API keys in a `.env` file:
   ```env
   ANTHROPIC_API_KEY=your_anthropic_key
   OPENAI_API_KEY=your_openai_key
   ```

### Unix/Linux Installation
RA.Aid can be installed directly using pip:

```bash
pip install synth-codeai
```
### macOS Installation with Homebrew

```bash
brew tap ai-christianson/homebrew-synth-codeai
brew install synth-codeai
```

**NOTE:** macOS may also be installed with pip as shown above.


### Prerequisites

Before using RA.Aid, you'll need API keys for the required AI services:

```bash
# Set up API keys based on your preferred provider:

# For Anthropic Claude models (recommended)
export ANTHROPIC_API_KEY=your_api_key_here

# For OpenAI models (optional)
export OPENAI_API_KEY=your_api_key_here

# For OpenRouter provider (optional)
export OPENROUTER_API_KEY=your_api_key_here

# For Makehub provider (optional)
export MAKEHUB_API_KEY=your_api_key_here

# For OpenAI-compatible providers (optional)
export OPENAI_API_BASE=your_api_base_url

# For Gemini provider (optional)
export GEMINI_API_KEY=your_api_key_here

# For web research capabilities
export TAVILY_API_KEY=your_api_key_here
```

Note: When using the `--use-aider` flag, the programmer tool (aider) will automatically select its model based on your available API keys:
- If ANTHROPIC_API_KEY is set, it will use Claude models
- If only OPENAI_API_KEY is set, it will use OpenAI models
- You can set multiple API keys to enable different features

You can get your API keys from:
- Anthropic API key: https://console.anthropic.com/
- OpenAI API key: https://platform.openai.com/api-keys
- OpenRouter API key: https://openrouter.ai/keys
- Makehub API key: https://makehub.ai/
- Gemini API key: https://aistudio.google.com/app/apikey

Note: `aider` must be installed separately as it is not included in the RA.Aid package. See [aider-chat](https://pypi.org/project/aider-chat/) for more details.

Complete installation documentation is available in our [Installation Guide](https://docs.synth-codeai.ai/quickstart/installation).

## Usage

RA.Aid is designed to be simple yet powerful. Here's how to use it:

```bash
# Basic usage
synth-codeai -m "Your task or query here"

# Research-only mode (no implementation)
synth-codeai -m "Explain the authentication flow" --research-only

# File logging with console warnings (default mode)
synth-codeai -m "Add new feature" --log-mode file

# Console-only logging with detailed output
synth-codeai -m "Add new feature" --log-mode console --log-level debug
```

More information is available in our [Usage Examples](https://docs.synth-codeai.ai/category/usage), [Logging System](https://docs.synth-codeai.ai/configuration/logging), and [Memory Management](https://docs.synth-codeai.ai/configuration/memory-management) documentation.

### Command Line Options

- `-m, --message`: The task or query to be executed (required except in chat mode, cannot be used with --msg-file)
- `--msg-file`: Path to a text file containing the task/message (cannot be used with --message)
- `--research-only`: Only perform research without implementation
- `--provider`: The LLM provider to use (choices: anthropic, openai, openrouter, openai-compatible, makehub, gemini)
- `--model`: The model name to use (required for non-Anthropic providers)
- `--use-aider`: Enable aider integration for code editing. When enabled, RA.Aid uses aider's specialized code editing capabilities instead of its own native file modification tools. This option is useful when you need aider's specific editing features or prefer its approach to code modifications. This feature is optional and disabled by default.
- `--research-provider`: Provider to use specifically for research tasks (falls back to --provider if not specified)
- `--research-model`: Model to use specifically for research tasks (falls back to --model if not specified)
- `--planner-provider`: Provider to use specifically for planning tasks (falls back to --provider if not specified)
- `--planner-model`: Model to use specifically for planning tasks (falls back to --model if not specified)
- `--cowboy-mode`: Skip interactive approval for shell commands
- `--expert-provider`: The LLM provider to use for expert knowledge queries (choices: anthropic, openai, openrouter, openai-compatible, makehub, gemini)
- `--expert-model`: The model name to use for expert knowledge queries (required for non-OpenAI providers)
- `--hil, -H`: Enable human-in-the-loop mode for interactive assistance during task execution
- `--chat`: Enable chat mode with direct human interaction (implies --hil)
- `--log-mode`: Logging mode (choices: file, console)
  - `file` (default): Logs to both file and console (only warnings and errors to console)
  - `console`: Logs to console only at the specified log level with no file logging
- `--log-level`: Set specific logging level (debug, info, warning, error, critical)
  - With `--log-mode=file`: Controls the file logging level (console still shows only warnings+)
  - With `--log-mode=console`: Controls the console logging level directly
  - Default: warning
- `--experimental-fallback-handler`: Enable experimental fallback handler to attempt to fix too calls when the same tool fails 3 times consecutively. (OPENAI_API_KEY recommended as openai has the top 5 tool calling models.) See `synth_codeai/tool_leaderboard.py` for more info.
- `--pretty-logger`: Enables colored panel-style formatted logging output for better readability.
- `--temperature`: LLM temperature (0.0-2.0) to control randomness in responses
- `--disable-limit-tokens`: Disable token limiting for Anthropic Claude react agents
- `--recursion-limit`: Maximum recursion depth for agent operations (default: 100)
- `--test-cmd`: Custom command to run tests. If set user will be asked if they want to run the test command
- `--auto-test`: Automatically run tests after each code change
- `--max-test-cmd-retries`: Maximum number of test command retry attempts (default: 3)
- `--test-cmd-timeout`: Timeout in seconds for test command execution (default: 300)
- `--show-cost`: Display cost information as the agent works - currently only supported on claude model agents
- `--track-cost`: Track token usage and costs (default: False)
- `--no-track-cost`: Disable tracking of token usage and costs
- `--max-cost`: Maximum cost threshold in USD (positive float)
- `--max-tokens`: Maximum token threshold (positive integer)
- `--exit-at-limit`: Exit immediately without prompting when --max-cost or --max-tokens limits are reached
- `--price-performance-ratio`: Price-performance ratio for Makehub API (0.0-1.0, where 0.0 prioritizes speed and 1.0 prioritizes cost efficiency)
- `--version`: Show program version number and exit
- `--server`: Launch the server with web interface (alpha feature)
- `--server-host`: Host to listen on for server (default: 0.0.0.0)  (alpha feature)
- `--server-port`: Port to listen on for server (default: 1818) (alpha feature)

### Example Tasks

1. Code Analysis:
   ```bash
   synth-codeai -m "Explain how the authentication middleware works" --research-only
   ```

2. Complex Changes:
   ```bash
   synth-codeai -m "Refactor the database connection code to use connection pooling" --cowboy-mode
   ```

3. Automated Updates:
   ```bash
   synth-codeai -m "Update deprecated API calls across the entire codebase" --cowboy-mode
   ```

4. Code Research:
   ```bash
   synth-codeai -m "Analyze the current error handling patterns" --research-only
   ```

2. Code Research:
   ```bash
   synth-codeai -m "Explain how the authentication middleware works" --research-only
   ```

3. Refactoring:
   ```bash
   synth-codeai -m "Refactor the database connection code to use connection pooling" --cowboy-mode
   ```

### Human-in-the-Loop Mode

Enable interactive mode to allow the agent to ask you questions during task execution:

```bash
synth-codeai -m "Implement a new feature" --hil
# or
synth-codeai -m "Implement a new feature" -H
```

This mode is particularly useful for:
- Complex tasks requiring human judgment
- Clarifying ambiguous requirements
- Making architectural decisions
- Validating critical changes
- Providing domain-specific knowledge

### Web Research

<img src="assets/demo-web-research-1.gif" alt="RA.Aid Demo" autoplay loop style="width: 100%; max-width: 800px;">

The agent features autonomous web research capabilities powered by the [Tavily](https://tavily.com/) API, seamlessly integrating real-world information into its problem-solving workflow. Web research is conducted automatically when the agent determines additional context would be valuable - no explicit configuration required.

For example, when researching modern authentication practices or investigating new API requirements, the agent will autonomously:
- Search for current best practices and security recommendations
- Find relevant documentation and technical specifications
- Gather real-world implementation examples
- Stay updated on latest industry standards

While web research happens automatically as needed, you can also explicitly request research-focused tasks:

```bash
# Focused research task with web search capabilities
synth-codeai -m "Research current best practices for API rate limiting" --research-only
```

Make sure to set your TAVILY_API_KEY environment variable to enable this feature.

### Chat Mode
<img src="assets/demo-chat-mode-1.gif" alt="Chat Mode Demo" autoplay loop style="display: block; margin: 0 auto; width: 100%; max-width: 800px;">

Enable with `--chat` to transform synth-codeai into an interactive assistant that guides you through research and implementation tasks. Have a natural conversation about what you want to build, explore options together, and dispatch work - all while maintaining context of your discussion. Perfect for when you want to think through problems collaboratively rather than just executing commands.

### Server with Web Interface

RA.Aid includes a modern server with web interface that provides:

- Beautiful dark-themed chat interface
- Real-time streaming of agent trajectory
- Responsive design that works on all devices

To launch the server with web interface:

```bash
# Start with default settings (0.0.0.0:1818)
synth-codeai --server

# Specify custom host and port
synth-codeai --server --server-host 127.0.0.1 --server-port 3000
```

Command line options for server with web interface:
- `--server`: Launch the server with web interface
- `--server-host`: Host to listen on (default: 0.0.0.0)
- `--server-port`: Port to listen on (default: 1818)

After starting the server, open your web browser to the displayed URL (e.g., http://localhost:1818).

### Command Interruption and Feedback

<img src="assets/demo-chat-mode-interrupted-1.gif" alt="Command Interrupt Demo" autoplay loop style="display: block; margin: 0 auto; width: 100%; max-width: 800px;">

You can interrupt the agent at any time by pressing `Ctrl-C`. This pauses the agent, allowing you to provide feedback, adjust your instructions, or steer the execution in a new direction. Press `Ctrl-C` again if you want to completely exit the program.


### Shell Command Automation with Cowboy Mode üèá

The `--cowboy-mode` flag enables automated shell command execution without confirmation prompts. This is useful for:

- CI/CD pipelines
- Automated testing environments
- Batch processing operations
- Scripted workflows

```bash
synth-codeai -m "Update all deprecated API calls" --cowboy-mode
```

**‚ö†Ô∏è Important Safety Notes:**
- Cowboy mode skips confirmation prompts for shell commands
- Always use in version-controlled repositories
- Ensure you have a clean working tree before running
- Review changes in git diff before committing

### Model Configuration

RA.Aid supports multiple AI providers and models. The default model is Anthropic's Claude 3 Sonnet (`claude-3-7-sonnet-20250219`).

When using the `--use-aider` flag, the programmer tool (aider) automatically selects its model based on your available API keys. It will use Claude models if ANTHROPIC_API_KEY is set, or fall back to OpenAI models if only OPENAI_API_KEY is available.

Note: The expert tool can be configured to use different providers (OpenAI, Anthropic, OpenRouter, Gemini) using the --expert-provider flag along with the corresponding EXPERT_*API_KEY environment variables. Each provider requires its own API key set through the appropriate environment variable.

#### Environment Variables

RA.Aid supports multiple providers through environment variables:

- `ANTHROPIC_API_KEY`: Required for the default Anthropic provider
- `OPENAI_API_KEY`: Required for OpenAI provider
- `OPENROUTER_API_KEY`: Required for OpenRouter provider
- `MAKEHUB_API_KEY`: Required for Makehub provider
- `DEEPSEEK_API_KEY`: Required for DeepSeek provider
- `OPENAI_API_BASE`: Required for OpenAI-compatible providers along with `OPENAI_API_KEY`
- `GEMINI_API_KEY`: Required for Gemini provider

Expert Tool Environment Variables:
- `EXPERT_OPENAI_API_KEY`: API key for expert tool using OpenAI provider
- `EXPERT_ANTHROPIC_API_KEY`: API key for expert tool using Anthropic provider
- `EXPERT_OPENROUTER_API_KEY`: API key for expert tool using OpenRouter provider
- `EXPERT_MAKEHUB_API_KEY`: API key for expert tool using Makehub provider (automatically uses `MAKEHUB_API_KEY` if not set)
- `EXPERT_OPENAI_API_BASE`: Base URL for expert tool using OpenAI-compatible provider
- `EXPERT_GEMINI_API_KEY`: API key for expert tool using Gemini provider
- `EXPERT_DEEPSEEK_API_KEY`: API key for expert tool using DeepSeek provider

You can set these permanently in your shell's configuration file (e.g., `~/.bashrc` or `~/.zshrc`):

```bash
# Default provider (Anthropic)
export ANTHROPIC_API_KEY=your_api_key_here

# For OpenAI features and expert tool
export OPENAI_API_KEY=your_api_key_here

# For OpenRouter provider
export OPENROUTER_API_KEY=your_api_key_here

# For Makehub provider
export MAKEHUB_API_KEY=your_api_key_here

# For OpenAI-compatible providers
export OPENAI_API_BASE=your_api_base_url

# For Gemini provider
export GEMINI_API_KEY=your_api_key_here
```

### Custom Model Examples

1. **Using Anthropic (Default)**
   ```bash
   # Uses default model (claude-3-7-sonnet-20250219)
   synth-codeai -m "Your task"

   # Or explicitly specify:
   synth-codeai -m "Your task" --provider anthropic --model claude-3-5-sonnet-20241022
   ```

2. **Using OpenAI**
   ```bash
   synth-codeai -m "Your task" --provider openai --model gpt-4o
   ```

3. **Using OpenRouter**
   ```bash
   synth-codeai -m "Your task" --provider openrouter --model mistralai/mistral-large-2411
   ```

4. **Using Makehub**
   ```bash
   synth-codeai -m "Your task" --provider makehub --model openai/gpt-4o
   
   # With price-performance optimization
   synth-codeai -m "Your task" --provider makehub --model anthropic/claude-4-sonnet --price-performance-ratio 0.7
   ```

5. **Using DeepSeek**
   ```bash
   # Direct DeepSeek provider (requires DEEPSEEK_API_KEY)
   synth-codeai -m "Your task" --provider deepseek --model deepseek-reasoner
   
   # DeepSeek via OpenRouter
   synth-codeai -m "Your task" --provider openrouter --model deepseek/deepseek-r1
   ```

6. **Configuring Expert Provider**

   The expert tool is used by the agent for complex logic and debugging tasks. It can be configured to use different providers (OpenAI, Anthropic, OpenRouter, Gemini, openai-compatible) using the --expert-provider flag along with the corresponding EXPERT_*API_KEY environment variables.

   ```bash
   # Use Anthropic for expert tool
   export EXPERT_ANTHROPIC_API_KEY=your_anthropic_api_key
   synth-codeai -m "Your task" --expert-provider anthropic --expert-model claude-3-5-sonnet-20241022

   # Use OpenRouter for expert tool
   export OPENROUTER_API_KEY=your_openrouter_api_key
   synth-codeai -m "Your task" --expert-provider openrouter --expert-model mistralai/mistral-large-2411

   # Use DeepSeek for expert tool
   export DEEPSEEK_API_KEY=your_deepseek_api_key
   synth-codeai -m "Your task" --expert-provider deepseek --expert-model deepseek-reasoner

   # Use Makehub for expert tool (automatically uses MAKEHUB_API_KEY)
   export MAKEHUB_API_KEY=your_makehub_api_key
   synth-codeai -m "Your task" --expert-provider makehub --expert-model anthropic/claude-4-sonnet

   # Use default OpenAI for expert tool
   export EXPERT_OPENAI_API_KEY=your_openai_api_key
   synth-codeai -m "Your task" --expert-provider openai --expert-model o1

   # Use Gemini for expert tool
   export EXPERT_GEMINI_API_KEY=your_gemini_api_key
   synth-codeai -m "Your task" --expert-provider gemini --expert-model gemini-2.0-flash-thinking-exp-1219
   ```

Aider specific Environment Variables you can add:

- `AIDER_FLAGS`: Optional comma-separated list of flags to pass to the underlying aider tool (e.g., "yes-always,dark-mode")

```bash
# Optional: Configure aider behavior
export AIDER_FLAGS="yes-always,dark-mode,no-auto-commits"
```

Note: For `AIDER_FLAGS`, you can specify flags with or without the leading `--`. Multiple flags should be comma-separated, and spaces around flags are automatically handled. For example, both `"yes-always,dark-mode"` and `"--yes-always, --dark-mode"` are valid.

**Important Notes:**
- Performance varies between models. The default Claude 3 Sonnet model currently provides the best and most reliable results.
- Model configuration is done via command line arguments: `--provider` and `--model`
- The `--model` argument is required for all providers except Anthropic (which defaults to `claude-3-7-sonnet-20250219`)

More information is available in our [Open Models Setup](https://docs.synth-codeai.ai/quickstart/open-models) guide.

## Architecture

RA.Aid implements a three-stage architecture for handling development and research tasks:

1. **Research Stage**:
   - Gathers information and context
   - Analyzes requirements
   - Identifies key components and dependencies

2. **Planning Stage**:
   - Develops detailed implementation plans
   - Breaks down tasks into manageable steps
   - Identifies potential challenges and solutions

3. **Implementation Stage**:
   - Executes planned tasks
   - Generates code or documentation
   - Performs necessary system operations

### Core Components

- **Console Module** (`console/`): Handles console output formatting and user interaction
- **Processing Module** (`proc/`): Manages interactive processing and workflow control
- **Text Module** (`text/`): Provides text processing and manipulation utilities
- **Tools Module** (`tools/`): Contains various utility tools for file operations, search, and more

## Dependencies

### Core Dependencies
- `langchain-anthropic`: LangChain integration with Anthropic's Claude
- `tavily-python`: Tavily API client for web research
- `langgraph`: Graph-based workflow management
- `rich>=13.0.0`: Terminal formatting and output
- `GitPython==3.1.41`: Git repository management
- `fuzzywuzzy==0.18.0`: Fuzzy string matching
- `python-Levenshtein==0.23.0`: Fast string matching
- `pathspec>=0.11.0`: Path specification utilities

### Development Dependencies
- `pytest>=7.0.0`: Testing framework
- `pytest-timeout>=2.2.0`: Test timeout management

## Development Setup

1. Clone the repository:
```bash
git clone https://github.com/Ifham58741/synth.codeai.git
cd RA.Aid
```

2. Create and activate a virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows use `venv\Scripts\activate`
```

3. Install development dependencies:
```bash
pip install -e ".[dev]"
```

4. Run tests:
```bash
python -m pytest
```


