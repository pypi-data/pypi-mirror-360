from __future__ import annotations

"""doxtract.processor
=====================

Highâ€‘level *preprocess* entryâ€‘point that walks through one or many input
files, extracts structured page metadata, and optionally returns a ğŸ¤—
`datasets.Dataset`.  All behavioural toggles are passed **directly as
parameters** rather than via a dataclass.

If an input PDF looks like a *scanned* document (every page is a near
fullâ€‘page raster image and contains no embedded text), the run is **aborted**
with a warning so the user can run OCR first.
"""

import os, subprocess
from pathlib import Path
from collections import defaultdict
from typing import List, Dict, Union

import fitz  # type: ignore
from tqdm.notebook import tqdm

from .utils.geometry import expand_rect
from .utils.headers import extract_headers_and_footers
from .utils.toc import detect_toc_candidates, filter_page_sequence
from .utils.text import extract_markdown_layout, extract_text_skip_rects
from .utils.images import (
    get_visible_image_xrefs,
    is_full_page_image,
    _detect_diagrams,
)

__all__ = ["preprocess", "process_documents"]  # alias for backward compat

# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
# â”‚ Conversion helper                                                   â”‚
# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

def _convert_to_pdf(file_path: os.PathLike | str, output_dir: os.PathLike | str = "/tmp", *, verbose: bool = False) -> Path:
    """Return a *Path* to a freshly converted PDF using LibreOffice."""
    if verbose:
        print(f"Converting {file_path} â†’ PDFâ€¦")
    ext = Path(file_path).suffix.lower()
    if ext not in {".docx", ".pptx", ".txt"}:
        raise ValueError(f"Unsupported file type '{ext}' for conversion")

    subprocess.run(
        [
            "soffice",
            "--headless",
            "--convert-to",
            "pdf",
            "--outdir",
            str(output_dir),
            str(file_path),
        ],
        check=True,
    )
    return Path(output_dir) / Path(file_path).with_suffix(".pdf").name


# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
# â”‚ OCRâ€‘related guard                                                   â”‚
# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

def _looks_scanned(pdf_path: Path) -> bool:
    """Heuristic: every page has *zero* text and is a near fullâ€‘page image."""
    doc = fitz.open(pdf_path)
    try:
        return all(
            page.get_text().strip() == "" and is_full_page_image(page) for page in doc
        )
    finally:
        doc.close()


# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
# â”‚ Main function                                                       â”‚
# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

def preprocess(
    paths: List[os.PathLike | str],
    *,
    markdown: bool = False,
    extract_vectors: bool = False,
    extract_images: bool = False,
    output_root: os.PathLike | str | None = None,
    strip_headers_footers: bool = True,
    preserve_layout: bool = False,
    as_dataset: bool = False,
    # advanced knobs
    vector_margin: int = 20,
    page_top_pct: float = 0.12,
    page_bottom_pct: float = 0.12,
    min_header_pages: int = 3,
    toc_threshold: int = 3,
    y_tol: int = 2,
    space_thresh: int = 5,
    verbose: bool = False,
) -> Union[Dict[str, List[Dict]], "datasets.Dataset", None]:
    """Preâ€‘process documents into pageâ€‘level metadata.

    Parameters
    ----------
    paths
        List of input file paths (`PDF`, `DOCX`, `PPTX`, `TXT`). Office files
        are autoâ€‘converted to PDF via LibreOffice.
    markdown
        If *True*, `page_content` is returned in GitHubâ€‘flavoured Markdown.
        Otherwise a leftâ€‘indented plainâ€‘text approximation is used.
    extract_vectors
        Save detected vector diagrams to *diagrams/* and include their file
        paths + bounding boxes in metadata.
    extract_images
        Save visible raster images to *images/* and list their paths in
        metadata.
    output_root
        Output directory where one subâ€‘folder per document is created.  When
        *None* (default) this falls back to ``"Doc Data"``.
    strip_headers_footers
        When *True* (default) repeating headers/footers are removed from
        `page_content` but still listed separately in metadata.
    preserve_layout
        If *True*, extracts page text with spacing preserved as-is from the PDF.
        This disables all layout rebuilding and formatting logic (e.g., markdown,
        indentation heuristics). Default is *False*.
    as_dataset
        Return a `datasets.Dataset` instead of a nested ``dict``.
    vector_margin, page_top_pct, page_bottom_pct, min_header_pages,
    toc_threshold, y_tol, space_thresh
        Advanced tuning knobs â€” keep defaults unless you know what youâ€™re
        doing.
            vector_margin - Padding around diagrams (in px)
            page_top_pct - % height for detecting headers
            page_bottom_pct - % height for detecting footers
            min_header_pages - Min pages with similar header/footer to consider valid
            toc_threshold - TOC detection sensitivity 
            y_tol - Line grouping tolerance (vertical)
            space_thresh - Horizontal gap â†’ one space

    Returns
    -------
    dict | datasets.Dataset | None
        *Nested* ``{doc_name â†’ [page_meta, â€¦]}`` or a ğŸ¤— Dataset when
        *as_dataset=True*.  Returns **None** if any input PDF appears to be
        a scanned document requiring OCR (a warning is printed).
    """

    out_root = Path(output_root or "Doc Data")
    out_root.mkdir(parents=True, exist_ok=True)

    # Collate rows (for Dataset) or dict (legacy)
    dataset_rows: List[Dict] = []
    legacy: Dict[str, List[Dict]] = defaultdict(list)

    for p in paths:
        path = Path(p)
        ext = path.suffix.lower()

        # â”€â”€â”€ office â†’ PDF â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if ext in {".docx", ".pptx", ".txt"}:
            path = _convert_to_pdf(path, verbose=verbose)
        elif ext != ".pdf":
            raise ValueError(f"Unsupported file type: {path}")

        # â”€â”€â”€ abort if scanned â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if _looks_scanned(path):
            print(f"âš ï¸  {path.name} looks like a scanned PDF with no text layer.  "
                  "Please run OCR first; aborting.")
            return None

        doc = fitz.open(path)
        doc_outdir = out_root / path.stem
        doc_outdir.mkdir(parents=True, exist_ok=True)

        diag_out = doc_outdir / "diagrams"
        img_out = doc_outdir / "images"
        if extract_vectors:
            diag_out.mkdir(exist_ok=True)
        if extract_images:
            img_out.mkdir(exist_ok=True)

        hf = extract_headers_and_footers(
            doc,
            top_pct=page_top_pct,
            bottom_pct=page_bottom_pct,
            min_pages=min_header_pages,
            verbose=verbose,
        )

        toc_candidates = detect_toc_candidates(doc, threshold=toc_threshold, verbose=verbose)
        toc_pages = filter_page_sequence(toc_candidates)

        for page in tqdm(doc, desc=f"Processing {path.name}", disable=not verbose):
            pg = page.number + 1

            # â€”â€”â€” diagrams â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
            diag_rects = []
            if extract_vectors:
                diag_rects = _detect_diagrams(page)
                for idx, box in enumerate(diag_rects, 1):
                    out_image = diag_out / f"p{pg:03d}_{idx}.png"
                    page.get_pixmap(clip=expand_rect(box, vector_margin), dpi=300).save(out_image)

            # â€”â€”â€” skip zones â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
            hdr_r = hf["page_header_rects"].get(pg, [])
            ftr_r = hf["page_footer_rects"].get(pg, [])
            skip_rects = diag_rects + (hdr_r + ftr_r if strip_headers_footers else [])

            # â€”â€”â€” text extraction â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
            if preserve_layout:
                page_text = page.get_text("text")  # Raw layout-preserved output
            else:
                if markdown:
                    content = "\n".join(
                        extract_markdown_layout(page, skip_rects, y_tol=y_tol, space_thresh=space_thresh)
                    )
                else:
                    content = "\n".join(
                        extract_text_skip_rects(page, skip_rects, y_tol=y_tol, space_thresh=space_thresh)
                    )

            meta = {
                "document_name": path.name,
                "page_number": pg,
                "is_toc_page": pg in toc_pages,
                "page_content": content,
                "headers": [page.get_textbox(r).strip() for r in hdr_r],
                "footers": [page.get_textbox(r).strip() for r in ftr_r],
                "diagrams": [],
                "images_on_this_page": [],
            }

            for idx, box in enumerate(diag_rects, 1):
                meta["diagrams"].append(
                    {
                        "path": str(diag_out / f"p{pg:03d}_{idx}.png"),
                        "bbox": [round(box.x0, 2), round(box.y0, 2), round(box.x1, 2), round(box.y1, 2)],
                    }
                )

            if extract_images:
                for xref in get_visible_image_xrefs(page):
                    pix = fitz.Pixmap(doc, xref)
                    img_path = img_out / f"p{pg}_xref{xref}.png"
                    pix.save(img_path)
                    meta["images_on_this_page"].append(str(img_path))

            dataset_rows.append(meta)
            legacy[path.name].append(meta)

    # â€”â€”â€” return value â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
    if as_dataset:
        try:
            from datasets import Dataset  # type: ignore
        except ImportError as e:
            raise RuntimeError("as_dataset=True but the 'datasets' package is missing.  pip install datasets") from e
        return Dataset.from_list(dataset_rows)
    return dict(legacy)


# Backward compatibility alias
process_documents = preprocess
