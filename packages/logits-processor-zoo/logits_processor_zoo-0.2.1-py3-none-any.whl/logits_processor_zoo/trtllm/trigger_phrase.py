#
# SPDX-FileCopyrightText: Copyright (c) 1993-2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

from typing import List, Optional
import time
from transformers import PreTrainedTokenizer
import torch
from logits_processor_zoo.utils import enforce_tokens, text_to_token
from tensorrt_llm.sampling_params import LogitsProcessor


class TriggerPhraseLogitsProcessor(LogitsProcessor):
    """
    A logits processor which triggers phrases when it encounters a given token.

    Parameters
    ----------
    tokenizer (PreTrainedTokenizer): The tokenizer used by the LLM.
    phrase (str): The phrase to be generated by LLM when it encounters the trigger token.
    trigger_token_phrase (str): (Optional) One token phrase in string to trigger phrases.
    trigger_time (float): (Optional) Time (wall-clock time) in seconds after which the phrase will be triggered.
    trigger_count (int): How many times the phrase will be triggered.
    trigger_after (bool): Whether the phrase is written after the trigger token or instead of the trigger token.
    """

    def __init__(self, tokenizer: PreTrainedTokenizer, phrase: str, trigger_token_phrase: Optional[str] = None,
                 trigger_time: Optional[float] = None, trigger_count: int = 1, trigger_after: bool = False):
        assert (
            trigger_token_phrase is not None or trigger_time is not None
        ), "Either trigger_token_phrase or trigger_time must be provided"
        self.tokenizer = tokenizer
        self.trigger_token = None
        if trigger_token_phrase is not None:
            self.trigger_token = text_to_token(self.tokenizer, trigger_token_phrase, last=False)

        self.trigger_time = trigger_time or float("inf")
        self.phrase_tokens = self.tokenizer.encode(phrase, add_special_tokens=False)
        self.initial_trigger_count = trigger_count
        self.trigger_after = trigger_after
        self.iterators = None
        self.trigger_counts = None
        self.start_time = time.time()

    def _init_before_gen(self, beam_width):
        self.iterators = -torch.ones(beam_width, dtype=torch.int32)
        self.trigger_counts = self.initial_trigger_count * torch.ones(beam_width, dtype=torch.int32)

    def __call__(self, req_id: int, logits: torch.Tensor,
                 token_ids: List[List[int]], stream_ptr: Optional[int],
                 client_id: Optional[int]) -> None:
        beam_width = len(token_ids)
        if self.iterators is None:
            self._init_before_gen(beam_width)

        stream = None if stream_ptr is None else torch.cuda.ExternalStream(stream_ptr)

        with torch.cuda.stream(stream):
            for i in range(beam_width):  # iterate over beams
                if self.trigger_counts[i] <= 0:
                    continue

                current_index = self.iterators[i].item()

                time_over = time.time() - self.start_time > self.trigger_time
                if (logits[0, i].argmax() == self.trigger_token or time_over) and current_index == -1:
                    self.iterators[i] = 0
                    if not self.trigger_after:
                        enforce_tokens(logits[0, i], [self.phrase_tokens[0]])
                        self.iterators[i] += 1
                elif len(self.phrase_tokens) > current_index >= 0:
                    enforce_tokens(logits[0, i], [self.phrase_tokens[current_index]])
                    self.iterators[i] += 1

                if len(self.phrase_tokens) == self.iterators[i].item():  # phrase completed, reset for next trigger
                    self.iterators[i] = -1
                    self.trigger_counts[i] -= 1
