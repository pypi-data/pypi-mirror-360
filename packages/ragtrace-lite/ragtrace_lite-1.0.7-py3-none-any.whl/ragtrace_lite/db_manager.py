"""
RAGTrace Lite Database Manager

SQLite ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬:
- í‰ê°€ ê²°ê³¼ ì €ì¥
- ê¸°ì¡´ RAGTrace ìŠ¤í‚¤ë§ˆ í˜¸í™˜
- í†µê³„ ê³„ì‚° ë° ì¡°íšŒ
- ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
"""

import sqlite3
import pandas as pd
import json
import numpy as np
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from uuid import uuid4

from .config_loader import Config


class DatabaseManager:
    """RAGTrace Lite SQLite ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, config: Optional[Config] = None, db_path: Optional[str] = None):
        """
        ë°ì´í„°ë² ì´ìŠ¤ ë§¤ë‹ˆì € ì´ˆê¸°í™”
        
        Args:
            config: RAGTrace Lite ì„¤ì •
            db_path: ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ê²½ë¡œ (configë³´ë‹¤ ìš°ì„ )
        """
        if db_path:
            self.db_path = Path(db_path)
        elif config:
            self.db_path = Path(config.database.path)
        else:
            self.db_path = Path("db/ragtrace_lite.db")
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
        self.conn = sqlite3.connect(str(self.db_path), check_same_thread=False)
        self.conn.row_factory = sqlite3.Row  # ë”•ì…”ë„ˆë¦¬ ìŠ¤íƒ€ì¼ ì ‘ê·¼
        
        # í…Œì´ë¸” ì´ˆê¸°í™”
        self._create_tables()
        
        print(f"ğŸ“€ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ: {self.db_path}")
    
    def _create_tables(self):
        """í•„ìš”í•œ í…Œì´ë¸”ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        cursor = self.conn.cursor()
        
        # í‰ê°€ ë©”íƒ€ë°ì´í„° í…Œì´ë¸”
        cursor.execute("""
        CREATE TABLE IF NOT EXISTS evaluations (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            run_id TEXT UNIQUE NOT NULL,
            timestamp DATETIME NOT NULL,
            llm_provider TEXT NOT NULL,
            llm_model TEXT,
            dataset_name TEXT,
            total_items INTEGER,
            metrics TEXT,  -- JSON ë°°ì—´
            config_data TEXT,  -- JSON ì„¤ì •
            status TEXT DEFAULT 'running',
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
            completed_at DATETIME
        )
        """)
        
        # ê°œë³„ ê²°ê³¼ í…Œì´ë¸” (ê¸°ì¡´ RAGTrace í˜¸í™˜)
        cursor.execute("""
        CREATE TABLE IF NOT EXISTS evaluation_results (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            run_id TEXT NOT NULL,
            item_index INTEGER NOT NULL,
            question TEXT,
            answer TEXT,
            contexts TEXT,  -- JSON ë°°ì—´
            ground_truth TEXT,
            metric_name TEXT NOT NULL,
            metric_score REAL,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (run_id) REFERENCES evaluations (run_id)
        )
        """)
        
        # ì¸ë±ìŠ¤ ìƒì„±
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_run_id ON evaluation_results (run_id)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_metric ON evaluation_results (metric_name)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_timestamp ON evaluations (timestamp)")
        
        self.conn.commit()
        print("âœ… ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” ìƒì„± ì™„ë£Œ")
    
    def create_evaluation_run(self, 
                            run_id: str,
                            llm_provider: str,
                            llm_model: str,
                            dataset_name: str,
                            total_items: int,
                            metrics: List[str],
                            config_data: Dict[str, Any]) -> str:
        """
        ìƒˆë¡œìš´ í‰ê°€ ì‹¤í–‰ì„ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            run_id: ì‹¤í–‰ ID
            llm_provider: LLM ì œê³µì
            llm_model: LLM ëª¨ë¸ëª…
            dataset_name: ë°ì´í„°ì…‹ ì´ë¦„
            total_items: ì´ ì•„ì´í…œ ìˆ˜
            metrics: í‰ê°€ ë©”íŠ¸ë¦­ ëª©ë¡
            config_data: ì„¤ì • ë°ì´í„°
            
        Returns:
            str: ìƒì„±ëœ run_id
        """
        cursor = self.conn.cursor()
        
        cursor.execute("""
        INSERT INTO evaluations 
        (run_id, timestamp, llm_provider, llm_model, dataset_name, total_items, metrics, config_data, status)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?, 'running')
        """, (
            run_id,
            datetime.now(),
            llm_provider,
            llm_model,
            dataset_name,
            total_items,
            json.dumps(metrics),
            json.dumps(config_data),
        ))
        
        self.conn.commit()
        print(f"ğŸ“Š í‰ê°€ ì‹¤í–‰ ìƒì„±: {run_id}")
        return run_id
    
    def save_evaluation_results(self, 
                              run_id: str, 
                              results_df: pd.DataFrame,
                              dataset) -> None:
        """
        í‰ê°€ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤.
        
        Args:
            run_id: ì‹¤í–‰ ID
            results_df: í‰ê°€ ê²°ê³¼ DataFrame
            dataset: ì›ë³¸ ë°ì´í„°ì…‹ (ì§ˆë¬¸, ë‹µë³€ ë“±)
        """
        cursor = self.conn.cursor()
        
        print(f"ğŸ’¾ í‰ê°€ ê²°ê³¼ ì €ì¥ ì¤‘: {run_id}")
        
        # ì›ë³¸ ë°ì´í„°ì™€ ê²°ê³¼ë¥¼ ê²°í•©
        for i, result_row in results_df.iterrows():
            # ì›ë³¸ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            original_data = dataset[i]
            
            question = original_data.get('question', '')
            answer = original_data.get('answer', '')
            contexts = json.dumps(original_data.get('contexts', []), ensure_ascii=False)
            ground_truth = original_data.get('ground_truth', '') or original_data.get('reference', '')
            
            # ê° ë©”íŠ¸ë¦­ë³„ë¡œ ê²°ê³¼ ì €ì¥
            # ë©”íŠ¸ë¦­ ì»¬ëŸ¼ë§Œ ì²˜ë¦¬ (ë©”íƒ€ë°ì´í„° ì œì™¸)
            metric_columns = ['faithfulness', 'answer_relevancy', 'context_precision', 
                            'context_recall', 'answer_correctness']
            
            for metric_name in metric_columns:
                if metric_name not in results_df.columns:
                    continue  # í•´ë‹¹ ë©”íŠ¸ë¦­ì´ ì—†ìœ¼ë©´ ìŠ¤í‚µ
                
                metric_score = result_row[metric_name]
                
                # NaN ê°’ ì²˜ë¦¬ - ë°°ì—´ì¸ ê²½ìš° ì²« ë²ˆì§¸ ê°’ ì‚¬ìš©
                if isinstance(metric_score, (list, np.ndarray)):
                    metric_score = float(metric_score[0]) if len(metric_score) > 0 else None
                elif pd.isna(metric_score):
                    metric_score = None
                else:
                    metric_score = float(metric_score)
                
                cursor.execute("""
                INSERT INTO evaluation_results 
                (run_id, item_index, question, answer, contexts, ground_truth, metric_name, metric_score)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    run_id,
                    i,
                    question,
                    answer,
                    contexts,
                    ground_truth,
                    metric_name,
                    metric_score
                ))
        
        self.conn.commit()
        print(f"âœ… í‰ê°€ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {len(results_df)} í•­ëª©")
    
    def complete_evaluation_run(self, run_id: str) -> None:
        """í‰ê°€ ì‹¤í–‰ì„ ì™„ë£Œ ìƒíƒœë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
        cursor = self.conn.cursor()
        
        cursor.execute("""
        UPDATE evaluations 
        SET status = 'completed', completed_at = ?
        WHERE run_id = ?
        """, (datetime.now(), run_id))
        
        self.conn.commit()
        print(f"ğŸ‰ í‰ê°€ ì‹¤í–‰ ì™„ë£Œ: {run_id}")
    
    def get_evaluation_summary(self, run_id: str) -> Dict[str, Any]:
        """íŠ¹ì • ì‹¤í–‰ì— ëŒ€í•œ ìš”ì•½ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        cursor = self.conn.cursor()
        
        # ê¸°ë³¸ ì •ë³´
        cursor.execute("""
        SELECT * FROM evaluations WHERE run_id = ?
        """, (run_id,))
        
        eval_info = dict(cursor.fetchone() or {})
        
        if not eval_info:
            raise ValueError(f"í‰ê°€ ì‹¤í–‰ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {run_id}")
        
        # ë©”íŠ¸ë¦­ë³„ í†µê³„
        cursor.execute("""
        SELECT 
            metric_name,
            COUNT(*) as count,
            AVG(metric_score) as average,
            MIN(metric_score) as minimum,
            MAX(metric_score) as maximum,
            SUM(CASE WHEN metric_score IS NULL THEN 1 ELSE 0 END) as null_count
        FROM evaluation_results 
        WHERE run_id = ? AND metric_score IS NOT NULL
        GROUP BY metric_name
        ORDER BY metric_name
        """, (run_id,))
        
        metric_stats = {}
        for row in cursor.fetchall():
            metric_stats[row['metric_name']] = {
                'count': row['count'],
                'average': round(row['average'], 4) if row['average'] else None,
                'minimum': round(row['minimum'], 4) if row['minimum'] else None,
                'maximum': round(row['maximum'], 4) if row['maximum'] else None,
                'null_count': row['null_count']
            }
        
        # ì „ì²´ RAGAS ì ìˆ˜ ê³„ì‚°
        if metric_stats:
            valid_averages = [stats['average'] for stats in metric_stats.values() 
                            if stats['average'] is not None]
            ragas_score = sum(valid_averages) / len(valid_averages) if valid_averages else None
        else:
            ragas_score = None
        
        return {
            'run_info': eval_info,
            'metric_statistics': metric_stats,
            'ragas_score': round(ragas_score, 4) if ragas_score else None,
            'total_metrics': len(metric_stats),
            'successful_evaluations': sum(stats['count'] for stats in metric_stats.values())
        }
    
    def get_evaluation_details(self, run_id: str) -> pd.DataFrame:
        """íŠ¹ì • ì‹¤í–‰ì˜ ìƒì„¸ ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
        query = """
        SELECT 
            item_index,
            question,
            answer,
            contexts,
            ground_truth,
            metric_name,
            metric_score
        FROM evaluation_results 
        WHERE run_id = ?
        ORDER BY item_index, metric_name
        """
        
        return pd.read_sql_query(query, self.conn, params=(run_id,))
    
    def list_evaluations(self, limit: int = 10) -> List[Dict[str, Any]]:
        """ìµœê·¼ í‰ê°€ ì‹¤í–‰ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        cursor = self.conn.cursor()
        
        cursor.execute("""
        SELECT 
            run_id,
            timestamp,
            llm_provider,
            llm_model,
            dataset_name,
            total_items,
            status,
            completed_at
        FROM evaluations 
        ORDER BY timestamp DESC 
        LIMIT ?
        """, (limit,))
        
        return [dict(row) for row in cursor.fetchall()]
    
    def get_performance_trends(self, metric_name: str, limit: int = 10) -> List[Dict[str, Any]]:
        """íŠ¹ì • ë©”íŠ¸ë¦­ì˜ ì„±ëŠ¥ íŠ¸ë Œë“œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        cursor = self.conn.cursor()
        
        cursor.execute("""
        SELECT 
            e.run_id,
            e.timestamp,
            e.llm_provider,
            AVG(er.metric_score) as average_score
        FROM evaluations e
        JOIN evaluation_results er ON e.run_id = er.run_id
        WHERE er.metric_name = ? AND er.metric_score IS NOT NULL
        GROUP BY e.run_id, e.timestamp, e.llm_provider
        ORDER BY e.timestamp DESC
        LIMIT ?
        """, (metric_name, limit))
        
        return [dict(row) for row in cursor.fetchall()]
    
    def cleanup_old_evaluations(self, keep_days: int = 30) -> int:
        """ì˜¤ë˜ëœ í‰ê°€ ê²°ê³¼ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤."""
        cursor = self.conn.cursor()
        
        cutoff_date = datetime.now().replace(day=datetime.now().day - keep_days)
        
        # ê²°ê³¼ ì‚­ì œ
        cursor.execute("""
        DELETE FROM evaluation_results 
        WHERE run_id IN (
            SELECT run_id FROM evaluations 
            WHERE timestamp < ?
        )
        """, (cutoff_date,))
        
        results_deleted = cursor.rowcount
        
        # í‰ê°€ ë©”íƒ€ë°ì´í„° ì‚­ì œ
        cursor.execute("""
        DELETE FROM evaluations 
        WHERE timestamp < ?
        """, (cutoff_date,))
        
        evaluations_deleted = cursor.rowcount
        
        self.conn.commit()
        
        print(f"ğŸ§¹ ì •ë¦¬ ì™„ë£Œ: {evaluations_deleted}ê°œ í‰ê°€, {results_deleted}ê°œ ê²°ê³¼")
        return evaluations_deleted
    
    def export_evaluation_data(self, run_id: str, output_path: str) -> None:
        """í‰ê°€ ë°ì´í„°ë¥¼ CSVë¡œ ë‚´ë³´ëƒ…ë‹ˆë‹¤."""
        details_df = self.get_evaluation_details(run_id)
        
        # í”¼ë²—í•˜ì—¬ ë©”íŠ¸ë¦­ì„ ì»¬ëŸ¼ìœ¼ë¡œ ë³€í™˜
        pivot_df = details_df.pivot_table(
            index=['item_index', 'question', 'answer', 'contexts', 'ground_truth'],
            columns='metric_name',
            values='metric_score',
            aggfunc='first'
        ).reset_index()
        
        # ì»¬ëŸ¼ëª… ì •ë¦¬
        pivot_df.columns.name = None
        
        # CSV ì €ì¥
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)
        pivot_df.to_csv(output_file, index=False, encoding='utf-8-sig')
        
        print(f"ğŸ“„ í‰ê°€ ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {output_file}")
    
    def close(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì„ ë‹«ìŠµë‹ˆë‹¤."""
        if self.conn:
            self.conn.close()
            print("ğŸ“€ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ")


def test_database_manager():
    """ë°ì´í„°ë² ì´ìŠ¤ ë§¤ë‹ˆì € í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("ğŸ§ª DatabaseManager í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    try:
        # í…ŒìŠ¤íŠ¸ìš© DB
        db_manager = DatabaseManager(db_path="test_ragtrace_lite.db")
        
        # í…ŒìŠ¤íŠ¸ í‰ê°€ ì‹¤í–‰ ìƒì„±
        run_id = f"test_{uuid4().hex[:8]}"
        db_manager.create_evaluation_run(
            run_id=run_id,
            llm_provider="hcx",
            llm_model="HCX-005",
            dataset_name="sample.json",
            total_items=3,
            metrics=["faithfulness", "answer_relevancy"],
            config_data={"temperature": 0.5, "max_tokens": 1000}
        )
        
        # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë°ì´í„° ìƒì„±
        test_results = pd.DataFrame({
            'faithfulness': [0.85, 0.92, 0.78],
            'answer_relevancy': [0.90, 0.88, 0.83]
        })
        
        test_dataset = [
            {'question': 'í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ 1', 'answer': 'í…ŒìŠ¤íŠ¸ ë‹µë³€ 1', 'contexts': ['ì»¨í…ìŠ¤íŠ¸ 1'], 'ground_truth': 'ì •ë‹µ 1'},
            {'question': 'í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ 2', 'answer': 'í…ŒìŠ¤íŠ¸ ë‹µë³€ 2', 'contexts': ['ì»¨í…ìŠ¤íŠ¸ 2'], 'ground_truth': 'ì •ë‹µ 2'},
            {'question': 'í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ 3', 'answer': 'í…ŒìŠ¤íŠ¸ ë‹µë³€ 3', 'contexts': ['ì»¨í…ìŠ¤íŠ¸ 3'], 'ground_truth': 'ì •ë‹µ 3'}
        ]
        
        # ê²°ê³¼ ì €ì¥
        db_manager.save_evaluation_results(run_id, test_results, test_dataset)
        db_manager.complete_evaluation_run(run_id)
        
        # ìš”ì•½ í†µê³„ í™•ì¸
        summary = db_manager.get_evaluation_summary(run_id)
        print(f"âœ… ìš”ì•½ í†µê³„:")
        print(f"   - RAGAS Score: {summary['ragas_score']}")
        print(f"   - ì´ ë©”íŠ¸ë¦­: {summary['total_metrics']}ê°œ")
        
        # í‰ê°€ ëª©ë¡ í™•ì¸
        evaluations = db_manager.list_evaluations(limit=5)
        print(f"âœ… ìµœê·¼ í‰ê°€: {len(evaluations)}ê°œ")
        
        # ì •ë¦¬
        db_manager.close()
        
        # í…ŒìŠ¤íŠ¸ DB íŒŒì¼ ì‚­ì œ
        Path("test_ragtrace_lite.db").unlink(missing_ok=True)
        
        print("âœ… DatabaseManager í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
        return True
        
    except Exception as e:
        print(f"âŒ DatabaseManager í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    test_database_manager()