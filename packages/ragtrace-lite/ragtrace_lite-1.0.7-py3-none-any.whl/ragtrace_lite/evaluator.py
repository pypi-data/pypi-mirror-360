"""
RAGTrace Lite Evaluator

RAGAS í‰ê°€ ì—”ì§„:
- 5ê°€ì§€ ë©”íŠ¸ë¦­ ì§€ì› (faithfulness, answer_relevancy, context_precision, context_recall, answer_correctness)
- ë°°ì¹˜ ì²˜ë¦¬ (batch_size í™œìš©)
- ì§„í–‰ë¥  í‘œì‹œ (tqdm)
- ë™ê¸°ì‹ í‰ê°€
"""

import pandas as pd
from typing import Dict, List, Any, Optional
from datasets import Dataset
from tqdm import tqdm
from pathlib import Path

# RAGAS imports with fallback
try:
    from ragas import evaluate
    from ragas.metrics import (
        faithfulness,
        answer_relevancy,
        context_precision,
        context_recall,
        answer_correctness,
    )
    RAGAS_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸  RAGAS import ì˜¤ë¥˜: {e}")
    RAGAS_AVAILABLE = False

from .config_loader import Config
from .llm_factory import create_llm


class RagasEvaluator:
    """RAGTrace Lite RAGAS í‰ê°€ í´ë˜ìŠ¤"""
    
    # ë©”íŠ¸ë¦­ ë§¤í•‘
    METRIC_MAP = {
        "faithfulness": "faithfulness",
        "answer_relevancy": "answer_relevancy", 
        "context_precision": "context_precision",
        "context_recall": "context_recall",
        "answer_correctness": "answer_correctness",
    } if RAGAS_AVAILABLE else {}
    
    def __init__(self, config: Config, llm=None):
        """
        RAGAS í‰ê°€ì ì´ˆê¸°í™”
        
        Args:
            config: RAGTrace Lite ì„¤ì •
            llm: ì‚¬ì „ ìƒì„±ëœ LLM ì¸ìŠ¤í„´ìŠ¤ (ì˜µì…˜)
            
        Raises:
            ImportError: RAGASê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì€ ê²½ìš°
            ValueError: ì„¤ì •ì´ ì˜¬ë°”ë¥´ì§€ ì•Šì€ ê²½ìš°
        """
        if not RAGAS_AVAILABLE:
            raise ImportError("RAGAS ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install ragas' ì‹¤í–‰í•˜ì„¸ìš”.")
            
        self.config = config
        
        # LLM ì¸ìŠ¤í„´ìŠ¤ ì„¤ì •
        if llm:
            print(f"ğŸ¤– ì™¸ë¶€ LLM ì‚¬ìš©: {config.llm.provider}")
            self.llm = llm
        else:
            print(f"ğŸ¤– ìƒˆ LLM ìƒì„±: {config.llm.provider}")
            self.llm = create_llm(config)
        
        # ì„ë² ë”© ëª¨ë¸ ì„¤ì •
        self.embeddings = self._setup_embeddings()
        if self.embeddings:
            print("âœ… ì„ë² ë”© ëª¨ë¸ ì„¤ì • ì™„ë£Œ")
        else:
            print("âš ï¸  ì„ë² ë”© ì„¤ì • ì‹¤íŒ¨, RAGAS ê¸°ë³¸ê°’ ì‚¬ìš©")
        
        # í‰ê°€ ë©”íŠ¸ë¦­ì€ evaluate() í˜¸ì¶œ ì‹œ ë°ì´í„°ì…‹ì„ ê¸°ë°˜ìœ¼ë¡œ ì„¤ì •
        self.metrics = None
        
        print(f"âœ… í‰ê°€ì ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _setup_embeddings(self):
        """ì„ë² ë”© ëª¨ë¸ì„ ì„¤ì •í•©ë‹ˆë‹¤."""
        embedding_provider = self.config.embedding.provider.lower()
        
        print(f"ğŸ”§ ì„ë² ë”© ì„¤ì •: {embedding_provider}")
        
        if embedding_provider == "bge_m3":
            print("ğŸ“ BGE-M3 ì„ë² ë”© ì´ˆê¸°í™”")
            try:
                return self._setup_bge_m3_embeddings()
            except Exception as e:
                print(f"âš ï¸  BGE-M3 ì„ë² ë”© ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                return None
            
        elif embedding_provider == "default":
            # OpenAI ì„ë² ë”© ì‚¬ìš© (RAGAS ê¸°ë³¸ê°’)
            try:
                from langchain_openai.embeddings import OpenAIEmbeddings
                import os
                
                api_key = os.getenv('OPENAI_API_KEY')
                if not api_key or api_key == "your_openai_api_key_here":
                    print("âš ï¸  OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                    return None
                    
                embeddings = OpenAIEmbeddings(
                    model="text-embedding-ada-002",
                    openai_api_key=api_key
                )
                print("âœ… OpenAI ì„ë² ë”© (text-embedding-ada-002) ë¡œë“œ ì™„ë£Œ")
                return embeddings
                
            except ImportError as e:
                print(f"âš ï¸  OpenAI ì„ë² ë”© import ì‹¤íŒ¨: {e}")
                return None
            except Exception as e:
                print(f"âš ï¸  OpenAI ì„ë² ë”© ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                return None
                
        else:
            print(f"âš ï¸  ì§€ì›í•˜ì§€ ì•ŠëŠ” ì„ë² ë”© ì œê³µì: {embedding_provider}")
            return None

    def _setup_bge_m3_embeddings(self):
        """BGE-M3 ì„ë² ë”© ëª¨ë¸ì„ ì„¤ì •í•©ë‹ˆë‹¤."""
        import os
        
        # ëª¨ë¸ ê²½ë¡œ ì„¤ì •
        model_path = Path(os.getenv('BGE_M3_MODEL_PATH', './models/bge-m3'))
        
        # ëª¨ë¸ í´ë” ìƒì„±
        model_path.parent.mkdir(parents=True, exist_ok=True)
        
        # ëª¨ë¸ì´ ë¡œì»¬ì— ìˆëŠ”ì§€ í™•ì¸
        if not model_path.exists() or not any(model_path.iterdir()):
            print(f"ğŸ“¥ BGE-M3 ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤: {model_path}")
            self._download_bge_m3_model(model_path)
        else:
            print(f"âœ… BGE-M3 ëª¨ë¸ ë°œê²¬: {model_path}")
        
        # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        try:
            from sentence_transformers import SentenceTransformer
            
            device = os.getenv('BGE_M3_DEVICE', 'auto')
            if device == 'auto':
                import torch
                if torch.cuda.is_available():
                    device = 'cuda'
                elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                    device = 'mps'
                else:
                    device = 'cpu'
            
            print(f"ğŸ”§ BGE-M3 ëª¨ë¸ ë¡œë”© (device: {device})...")
            model = SentenceTransformer(str(model_path), device=device)
            
            # RAGAS í˜¸í™˜ ì„ë² ë”© ë˜í¼ ìƒì„±
            from ragas.embeddings import LangchainEmbeddingsWrapper
            
            try:
                # ìƒˆë¡œìš´ langchain_huggingface ì‚¬ìš© (ê¶Œì¥)
                from langchain_huggingface import HuggingFaceEmbeddings
            except ImportError:
                # ëŒ€ì²´: langchain_community ì‚¬ìš©
                from langchain_community.embeddings import HuggingFaceEmbeddings
            
            # HuggingFace ì„ë² ë”©ì„ langchainìœ¼ë¡œ ê°ì‹¸ê¸°
            lc_embeddings = HuggingFaceEmbeddings(
                model_name=str(model_path),
                model_kwargs={'device': device}
            )
            embeddings = LangchainEmbeddingsWrapper(lc_embeddings)
            
            print(f"âœ… BGE-M3 ì„ë² ë”© ë¡œë“œ ì™„ë£Œ (device: {device})")
            return embeddings
            
        except ImportError as e:
            raise ImportError(f"BGE-M3 ì˜ì¡´ì„±ì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {e}")
        except Exception as e:
            raise Exception(f"BGE-M3 ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def _download_bge_m3_model(self, model_path: Path):
        """BGE-M3 ëª¨ë¸ì„ Hugging Faceì—ì„œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            from huggingface_hub import snapshot_download
            
            print(f"ğŸ“¦ BGE-M3 ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
            print(f"   ìœ„ì¹˜: {model_path.absolute()}")
            print(f"   í¬ê¸°: ì•½ 2.3GB (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
            
            # ë””ë ‰í† ë¦¬ ìƒì„± (í¬ë¡œìŠ¤ í”Œë«í¼ í˜¸í™˜)
            model_path.mkdir(parents=True, exist_ok=True)
            
            # BGE-M3 ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (í¬ë¡œìŠ¤ í”Œë«í¼ ê²½ë¡œ ì²˜ë¦¬)
            snapshot_download(
                repo_id="BAAI/bge-m3",
                local_dir=str(model_path.absolute()),
                local_dir_use_symlinks=False,  # ì‹¬ë³¼ë¦­ ë§í¬ ëŒ€ì‹  ì‹¤ì œ íŒŒì¼ ë³µì‚¬
                resume_download=True  # ì¤‘ë‹¨ëœ ë‹¤ìš´ë¡œë“œ ì¬ê°œ ì§€ì›
            )
            
            print(f"âœ… BGE-M3 ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {model_path.absolute()}")
            
        except ImportError as e:
            print("âŒ huggingface_hubê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            print("ğŸ’¡ í•´ê²°ë°©ë²•: pip install huggingface_hub")
            raise ImportError(f"huggingface_hubê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {e}")
        except Exception as e:
            raise Exception(f"BGE-M3 ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")

    def _check_ground_truth_availability(self, dataset: Dataset = None) -> bool:
        """Ground truth ë°ì´í„°ì˜ ê°€ìš©ì„±ì„ í™•ì¸í•©ë‹ˆë‹¤."""
        if dataset is None:
            return False
        
        # ground_truths ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
        if 'ground_truths' not in dataset.column_names:
            return False
        
        # ground_truthsê°€ ë¹„ì–´ìˆì§€ ì•Šì€ í•­ëª©ì´ ìˆëŠ”ì§€ í™•ì¸
        for item in dataset:
            ground_truths = item.get('ground_truths', [])
            if ground_truths and len(ground_truths) > 0:
                # ë¹ˆ ë¬¸ìì—´ì´ ì•„ë‹Œ ground truthê°€ ìˆëŠ”ì§€ í™•ì¸
                valid_truths = [gt for gt in ground_truths if isinstance(gt, str) and gt.strip()]
                if valid_truths:
                    return True
        
        return False

    def _setup_metrics(self, dataset: Dataset = None) -> List[Any]:
        """í‰ê°€ ë©”íŠ¸ë¦­ì„ ì„¤ì •í•©ë‹ˆë‹¤."""
        metrics = []
        
        print("ğŸ”§ ë©”íŠ¸ë¦­ ì„¤ì • ì¤‘...")
        
        # Ground truth ë°ì´í„° ê°€ìš©ì„± í™•ì¸
        has_ground_truths = self._check_ground_truth_availability(dataset)
        
        # ë©”íŠ¸ë¦­ ì„ íƒ: ground truthê°€ ìˆìœ¼ë©´ 5ê°œ, ì—†ìœ¼ë©´ 4ê°œ
        if has_ground_truths:
            selected_metrics = ["faithfulness", "answer_relevancy", "context_precision", "context_recall", "answer_correctness"]
            print("ğŸ“Š Ground truth ë°ì´í„° í™•ì¸: 5ê°œ ë©”íŠ¸ë¦­ ì‚¬ìš©")
        else:
            selected_metrics = ["faithfulness", "answer_relevancy", "context_precision", "answer_correctness"]
            print("ğŸ“Š Ground truth ë°ì´í„° ì—†ìŒ: 4ê°œ ë©”íŠ¸ë¦­ ì‚¬ìš© (context_recall ì œì™¸)")
        
        for metric_name in selected_metrics:
            try:
                if metric_name == "faithfulness":
                    metric = faithfulness
                    metric.llm = self.llm
                    metrics.append(metric)
                    print(f"  âœ… {metric_name} (LLM ê¸°ë°˜)")
                    
                elif metric_name == "answer_relevancy":
                    metric = answer_relevancy
                    metric.llm = self.llm
                    if self.embeddings:
                        metric.embeddings = self.embeddings
                    metrics.append(metric)
                    print(f"  âœ… {metric_name} (LLM + ì„ë² ë”© ê¸°ë°˜)")
                    
                elif metric_name == "context_precision":
                    metric = context_precision
                    metric.llm = self.llm
                    metrics.append(metric)
                    print(f"  âœ… {metric_name} (LLM ê¸°ë°˜)")
                    
                elif metric_name == "context_recall":
                    metric = context_recall
                    metric.llm = self.llm
                    metrics.append(metric)
                    print(f"  âœ… {metric_name} (LLM ê¸°ë°˜)")
                    
                elif metric_name == "answer_correctness":
                    metric = answer_correctness
                    metric.llm = self.llm
                    metrics.append(metric)
                    print(f"  âœ… {metric_name} (LLM ê¸°ë°˜)")
                    
                else:
                    print(f"  âš ï¸  ì•Œ ìˆ˜ ì—†ëŠ” ë©”íŠ¸ë¦­: {metric_name}")
                    
            except Exception as e:
                print(f"  âŒ {metric_name} ì„¤ì • ì‹¤íŒ¨: {e}")
        
        if not metrics:
            raise ValueError("ì„¤ì •ëœ ë©”íŠ¸ë¦­ì´ ì—†ìŠµë‹ˆë‹¤")
            
        return metrics
    
    def evaluate(self, dataset: Dataset) -> pd.DataFrame:
        """
        ë°ì´í„°ì…‹ì— ëŒ€í•´ RAGAS í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            dataset: í‰ê°€í•  RAGAS Dataset
            
        Returns:
            pd.DataFrame: í‰ê°€ ê²°ê³¼ (ê° í•­ëª©ë³„ ë©”íŠ¸ë¦­ ì ìˆ˜)
            
        Raises:
            ValueError: ë°ì´í„°ì…‹ì´ ì˜¬ë°”ë¥´ì§€ ì•Šì€ ê²½ìš°
            Exception: í‰ê°€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•œ ê²½ìš°
        """
        print(f"\nğŸš€ RAGAS í‰ê°€ ì‹œì‘")
        print(f"   - ë°ì´í„° ìˆ˜: {len(dataset)}ê°œ")
        print(f"   - LLM: {self.config.llm.provider}")
        print(f"   - ë°°ì¹˜ í¬ê¸°: {self.config.evaluation.batch_size}")
        
        # ë°ì´í„°ì…‹ ê¸°ë°˜ ë©”íŠ¸ë¦­ ì„¤ì •
        if self.metrics is None:
            self.metrics = self._setup_metrics(dataset)
        
        print(f"   - ë©”íŠ¸ë¦­: {len(self.metrics)}ê°œ")
        
        # ë°ì´í„°ì…‹ ê²€ì¦ ë° ìˆ˜ì •
        dataset = self._validate_dataset(dataset)
        
        try:
            # RAGAS evaluate í˜¸ì¶œ
            print("\nğŸ“Š í‰ê°€ ì§„í–‰ ì¤‘...")
            
            # RAGASëŠ” ë‚´ë¶€ì ìœ¼ë¡œ ì§„í–‰ë¥ ì„ í‘œì‹œí•˜ë¯€ë¡œ ë³„ë„ tqdm ë¶ˆí•„ìš”
            result = evaluate(
                dataset=dataset,
                metrics=self.metrics,
                llm=self.llm,
                embeddings=self.embeddings,
                raise_exceptions=False,  # íŒŒì‹± ì˜¤ë¥˜ ì‹œì—ë„ ê³„ì† ì§„í–‰
                show_progress=self.config.evaluation.show_progress,
            )
            
            print("âœ… í‰ê°€ ì™„ë£Œ!")
            
            # ê²°ê³¼ë¥¼ pandas DataFrameìœ¼ë¡œ ë³€í™˜
            try:
                results_df = result.to_pandas()
            except Exception as e:
                print(f"âš ï¸ DataFrame ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {e}")
                # ìˆ˜ë™ìœ¼ë¡œ DataFrame ìƒì„±
                results_dict = {}
                if hasattr(result, 'scores'):
                    for key, value in result.scores.items():
                        results_dict[key] = value
                
                results_df = pd.DataFrame(results_dict)
            
            # ë””ë²„ê¹…: ê²°ê³¼ ë°ì´í„° íƒ€ì… í™•ì¸
            print("\nğŸ“Š ê²°ê³¼ ë°ì´í„° íƒ€ì…:")
            for col in results_df.columns:
                print(f"  - {col}: {results_df[col].dtype}")
                # ë¬¸ìì—´ ì»¬ëŸ¼ í™•ì¸
                if results_df[col].dtype == 'object':
                    print(f"    ìƒ˜í”Œ: {results_df[col].head(2).tolist()}")
            
            # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
            try:
                self._print_evaluation_summary(results_df)
            except Exception as e:
                print(f"âš ï¸ ê²°ê³¼ ìš”ì•½ ì¶œë ¥ ì¤‘ ì˜¤ë¥˜: {e}")
                # ê¸°ë³¸ ì •ë³´ë§Œ ì¶œë ¥
                print("\nğŸ“ˆ í‰ê°€ ê²°ê³¼ (ì›ì‹œ ë°ì´í„°):")
                print(results_df.head())
            
            return results_df
            
        except Exception as e:
            print(f"âŒ í‰ê°€ ì‹¤íŒ¨: {e}")
            raise Exception(f"RAGAS í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    def _validate_dataset(self, dataset: Dataset) -> Dataset:
        """í‰ê°€ìš© ë°ì´í„°ì…‹ì„ ê²€ì¦í•˜ê³  í•„ìš”ì‹œ ìˆ˜ì •í•©ë‹ˆë‹¤."""
        
        # ê¸°ë³¸ ê²€ì¦
        if len(dataset) == 0:
            raise ValueError("ë°ì´í„°ì…‹ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
        
        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        required_columns = ['question', 'answer', 'contexts']
        missing_columns = [col for col in required_columns if col not in dataset.column_names]
        
        if missing_columns:
            raise ValueError(f"í•„ìˆ˜ ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {missing_columns}")
        
        # ground_truths ì»¬ëŸ¼ í™•ì¸ (answer_correctness, context_recallìš©)
        if ('answer_correctness' in self.config.evaluation.metrics or 
            'context_recall' in self.config.evaluation.metrics):
            if 'ground_truths' not in dataset.column_names:
                print("âš ï¸  'ground_truths' ì»¬ëŸ¼ì´ ì—†ì–´ answer_correctness/context_recall í‰ê°€ê°€ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        
        # reference ì»¬ëŸ¼ í™•ì¸ ë° ìë™ ìƒì„±
        if 'reference' not in dataset.column_names and 'ground_truths' in dataset.column_names:
            print("âš ï¸  'reference' ì»¬ëŸ¼ì´ ì—†ì–´ ground_truthsë¥¼ referenceë¡œ ë³€í™˜í•©ë‹ˆë‹¤")
            # Datasetì„ dictionaryë¡œ ë³€í™˜í•˜ì—¬ ìˆ˜ì •
            data_dict = dataset.to_dict()
            # ground_truthsì˜ ì²« ë²ˆì§¸ ìš”ì†Œë¥¼ referenceë¡œ ì‚¬ìš©
            data_dict['reference'] = [
                gt[0] if gt and len(gt) > 0 else '' 
                for gt in data_dict['ground_truths']
            ]
            # ìƒˆë¡œìš´ Dataset ìƒì„±
            dataset = Dataset.from_dict(data_dict)
            print("âœ… reference ì»¬ëŸ¼ ìƒì„± ì™„ë£Œ")
        
        print(f"âœ… ë°ì´í„°ì…‹ ê²€ì¦ ì™„ë£Œ")
        return dataset
    
    def _print_evaluation_summary(self, results_df: pd.DataFrame) -> None:
        """í‰ê°€ ê²°ê³¼ ìš”ì•½ì„ ì¶œë ¥í•©ë‹ˆë‹¤."""
        
        print(f"\nğŸ“ˆ í‰ê°€ ê²°ê³¼ ìš”ì•½:")
        print(f"{'='*50}")
        
        # ì‹¤ì œ í‰ê°€ëœ ë©”íŠ¸ë¦­ í™•ì¸
        evaluated_metrics = [col for col in results_df.columns 
                           if col not in ['question', 'answer', 'contexts', 'ground_truths', 'reference']]
        
        # ê° ë©”íŠ¸ë¦­ë³„ í‰ê·  ì ìˆ˜
        for metric_name in evaluated_metrics:
            try:
                # ìˆ«ìê°€ ì•„ë‹Œ ê°’ ì œì™¸í•˜ê³  numeric íƒ€ì…ìœ¼ë¡œ ë³€í™˜
                scores = pd.to_numeric(results_df[metric_name], errors='coerce').dropna()
                
                if len(scores) > 0:
                    avg_score = scores.mean()
                    min_score = scores.min()
                    max_score = scores.max()
                    
                    # NaN ì²´í¬
                    if pd.isna(avg_score):
                        print(f"{metric_name:20}: ê³„ì‚° ë¶ˆê°€ (ìœ íš¨í•œ ì ìˆ˜ ì—†ìŒ)")
                    else:
                        print(f"{metric_name:20}: {avg_score:.4f} (ë²”ìœ„: {min_score:.4f}-{max_score:.4f})")
                else:
                    print(f"{metric_name:20}: ë°ì´í„° ì—†ìŒ")
            except Exception as e:
                print(f"{metric_name:20}: ì˜¤ë¥˜ ë°œìƒ ({str(e)})")
        
        # ì „ì²´ í‰ê·  (RAGAS Score)
        if evaluated_metrics:
            try:
                # ê° ë©”íŠ¸ë¦­ì„ numericìœ¼ë¡œ ë³€í™˜
                numeric_df = pd.DataFrame()
                valid_metrics = []
                
                for metric in evaluated_metrics:
                    numeric_col = pd.to_numeric(results_df[metric], errors='coerce')
                    if numeric_col.notna().any():  # ìµœì†Œ í•˜ë‚˜ì˜ ìœ íš¨í•œ ê°’ì´ ìˆìœ¼ë©´
                        numeric_df[metric] = numeric_col
                        valid_metrics.append(metric)
                
                if valid_metrics:
                    # ê° í–‰ì˜ í‰ê·  ê³„ì‚° (NaN ì œì™¸)
                    overall_scores = numeric_df[valid_metrics].mean(axis=1, skipna=True)
                    overall_avg = overall_scores.mean(skipna=True)
                    
                    if not pd.isna(overall_avg):
                        print(f"{'='*50}")
                        print(f"{'ì „ì²´ í‰ê·  (RAGAS Score)':20}: {overall_avg:.4f}")
                    else:
                        print(f"{'='*50}")
                        print(f"{'ì „ì²´ í‰ê·  (RAGAS Score)':20}: ê³„ì‚° ë¶ˆê°€")
                else:
                    print(f"{'='*50}")
                    print(f"{'ì „ì²´ í‰ê·  (RAGAS Score)':20}: ìœ íš¨í•œ ë©”íŠ¸ë¦­ ì—†ìŒ")
            except Exception as e:
                print(f"{'='*50}")
                print(f"{'ì „ì²´ í‰ê·  (RAGAS Score)':20}: ì˜¤ë¥˜ ({str(e)})")
        
        print(f"{'='*50}")
    
    def get_detailed_results(self, results_df: pd.DataFrame) -> Dict[str, Any]:
        """ìƒì„¸í•œ í‰ê°€ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        
        detailed_results = {
            'summary': {},
            'by_metric': {},
            'by_item': {},
            'statistics': {}
        }
        
        # ì‹¤ì œ í‰ê°€ëœ ë©”íŠ¸ë¦­ í™•ì¸
        metric_columns = [col for col in results_df.columns 
                         if col not in ['question', 'answer', 'contexts', 'ground_truths', 'reference']]
        
        # ë©”íŠ¸ë¦­ë³„ í†µê³„
        for metric_name in metric_columns:
            scores = results_df[metric_name].dropna()
            if len(scores) > 0:
                detailed_results['by_metric'][metric_name] = {
                    'mean': float(scores.mean()),
                    'std': float(scores.std()),
                    'min': float(scores.min()),
                    'max': float(scores.max()),
                    'count': len(scores)
                }
        
        # ì „ì²´ í†µê³„
        if metric_columns:
            overall_scores = results_df[metric_columns].mean(axis=1)
            detailed_results['summary'] = {
                'ragas_score': float(overall_scores.mean()),
                'total_items': len(results_df),
                'evaluated_metrics': len(metric_columns)
            }
        
        return detailed_results


def test_evaluator():
    """í‰ê°€ì í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("ğŸ§ª RagasEvaluator í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    try:
        # ì„¤ì • ë° ë°ì´í„° ë¡œë“œ
        from .config_loader import load_config
        from .data_processor import DataProcessor
        
        config = load_config()
        processor = DataProcessor()
        dataset = processor.load_and_prepare_data("data/input/sample.json")
        
        print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(dataset)}ê°œ í•­ëª©")
        
        # í‰ê°€ì ìƒì„±
        evaluator = RagasEvaluator(config)
        
        # í‰ê°€ ìˆ˜í–‰ (ì‘ì€ ë°ì´í„°ì…‹ì´ë¯€ë¡œ ë¹ ë¦„)
        results_df = evaluator.evaluate(dataset)
        
        print(f"\nâœ… í‰ê°€ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
        print(f"   - ê²°ê³¼ DataFrame í¬ê¸°: {results_df.shape}")
        print(f"   - ì»¬ëŸ¼: {list(results_df.columns)}")
        
        # ìƒì„¸ ê²°ê³¼
        detailed = evaluator.get_detailed_results(results_df)
        print(f"   - RAGAS Score: {detailed['summary'].get('ragas_score', 'N/A'):.4f}")
        
        return True
        
    except Exception as e:
        print(f"âŒ í‰ê°€ì í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False


if __name__ == "__main__":
    test_evaluator()