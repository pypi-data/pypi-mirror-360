"""
RAGTrace Lite Report Generator

Markdown ë³´ê³ ì„œ ìƒì„±:
- í‰ê°€ ê²°ê³¼ ìš”ì•½
- í…ìŠ¤íŠ¸ ê¸°ë°˜ ì‹œê°í™”
- í†µê³„ ë¶„ì„ ë° ì¸ì‚¬ì´íŠ¸
- ìƒì„¸ ê²°ê³¼ ë¶„ì„
"""

import pandas as pd
import json
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path

from .config_loader import Config


class ReportGenerator:
    """RAGTrace Lite Markdown ë³´ê³ ì„œ ìƒì„± í´ë˜ìŠ¤"""
    
    def __init__(self, config: Optional[Config] = None):
        """
        ë³´ê³ ì„œ ìƒì„±ê¸° ì´ˆê¸°í™”
        
        Args:
            config: RAGTrace Lite ì„¤ì •
        """
        self.config = config
    
    def generate_evaluation_report(self,
                                 run_id: str,
                                 summary_data: Dict[str, Any],
                                 results_df: pd.DataFrame,
                                 dataset: List[Dict[str, Any]]) -> str:
        """
        ì™„ì „í•œ í‰ê°€ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            run_id: ì‹¤í–‰ ID
            summary_data: ìš”ì•½ í†µê³„ ë°ì´í„°
            results_df: í‰ê°€ ê²°ê³¼ DataFrame
            dataset: ì›ë³¸ ë°ì´í„°ì…‹
            
        Returns:
            str: Markdown í˜•ì‹ì˜ ë³´ê³ ì„œ
        """
        report_sections = []
        
        # í—¤ë”
        report_sections.append(self._generate_header(run_id, summary_data))
        
        # ìš”ì•½ í†µê³„
        report_sections.append(self._generate_summary_statistics(summary_data))
        
        # ë©”íŠ¸ë¦­ë³„ ë¶„ì„
        report_sections.append(self._generate_metric_analysis(summary_data, results_df))
        
        # ì„±ëŠ¥ ë¶„ì„
        report_sections.append(self._generate_performance_analysis(results_df, dataset))
        
        # ìƒì„¸ ê²°ê³¼
        report_sections.append(self._generate_detailed_results(results_df, dataset))
        
        # ì¸ì‚¬ì´íŠ¸ ë° ê¶Œì¥ì‚¬í•­
        report_sections.append(self._generate_insights_and_recommendations(summary_data, results_df))
        
        # í‘¸í„°
        report_sections.append(self._generate_footer())
        
        return "\n\n".join(report_sections)
    
    def _generate_header(self, run_id: str, summary_data: Dict[str, Any]) -> str:
        """ë³´ê³ ì„œ í—¤ë” ìƒì„±"""
        run_info = summary_data.get('run_info', {})
        
        header = [
            f"# RAGTrace Lite í‰ê°€ ë³´ê³ ì„œ",
            f"",
            f"**ì‹¤í–‰ ID**: `{run_id}`  ",
            f"**ìƒì„± ì¼ì‹œ**: {datetime.now().strftime('%Yë…„ %mì›” %dì¼ %H:%M:%S')}  ",
            f"**LLM ëª¨ë¸**: {run_info.get('llm_provider', 'Unknown')} - {run_info.get('llm_model', 'Unknown')}  ",
            f"**ë°ì´í„°ì…‹**: {run_info.get('dataset_name', 'Unknown')}  ",
            f"**ì´ í•­ëª© ìˆ˜**: {run_info.get('total_items', 0)}ê°œ  ",
            f"**í‰ê°€ ìƒíƒœ**: {run_info.get('status', 'Unknown')}  ",
            f"",
            "---"
        ]
        
        return "\n".join(header)
    
    def _generate_summary_statistics(self, summary_data: Dict[str, Any]) -> str:
        """ìš”ì•½ í†µê³„ ì„¹ì…˜ ìƒì„±"""
        metric_stats = summary_data.get('metric_statistics', {})
        ragas_score = summary_data.get('ragas_score')
        
        section = [
            "## ğŸ“Š ì¢…í•© ê²°ê³¼",
            ""
        ]
        
        # RAGAS ì „ì²´ ì ìˆ˜
        if ragas_score is not None:
            ragas_score = float(ragas_score)  # ëª…ì‹œì  float ë³€í™˜
            score_bar = self._create_score_bar(ragas_score)
            section.extend([
                f"### ğŸ¯ ì „ì²´ RAGAS ì ìˆ˜",
                f"",
                f"**{ragas_score:.4f}** {score_bar}",
                f"",
                self._interpret_score(ragas_score),
                ""
            ])
        
        # ë©”íŠ¸ë¦­ë³„ ì ìˆ˜ í…Œì´ë¸”
        if metric_stats:
            section.extend([
                "### ğŸ“ˆ ë©”íŠ¸ë¦­ë³„ ì„±ëŠ¥",
                "",
                "| ë©”íŠ¸ë¦­ | í‰ê·  ì ìˆ˜ | ì‹œê°í™” | ìµœì†Œê°’ | ìµœëŒ€ê°’ | í‰ê°€ ìˆ˜ |",
                "|--------|-----------|--------|--------|--------|---------|"
            ])
            
            for metric_name, stats in metric_stats.items():
                avg_score = float(stats.get('average', 0) or 0)
                min_score = float(stats.get('minimum', 0) or 0)
                max_score = float(stats.get('maximum', 0) or 0)
                count = int(stats.get('count', 0) or 0)
                
                score_viz = self._create_score_bar(avg_score, length=15)
                
                section.append(
                    f"| {metric_name} | {avg_score:.4f} | `{score_viz}` | {min_score:.4f} | {max_score:.4f} | {count} |"
                )
        
        return "\n".join(section)
    
    def _generate_metric_analysis(self, summary_data: Dict[str, Any], results_df: pd.DataFrame) -> str:
        """ë©”íŠ¸ë¦­ë³„ ìƒì„¸ ë¶„ì„ ìƒì„±"""
        metric_stats = summary_data.get('metric_statistics', {})
        
        section = [
            "## ğŸ” ë©”íŠ¸ë¦­ë³„ ìƒì„¸ ë¶„ì„",
            ""
        ]
        
        metric_descriptions = {
            'faithfulness': '**ì¶©ì‹¤ë„** - ë‹µë³€ì´ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì— ì–¼ë§ˆë‚˜ ì¶©ì‹¤í•œì§€ ì¸¡ì •',
            'answer_relevancy': '**ë‹µë³€ ê´€ë ¨ì„±** - ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ì´ ìˆëŠ”ì§€ ì¸¡ì •',
            'context_precision': '**ì»¨í…ìŠ¤íŠ¸ ì •í™•ë„** - ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì˜ ì •í™•ë„ ì¸¡ì •',
            'context_recall': '**ì»¨í…ìŠ¤íŠ¸ íšŒìƒë¥ ** - í•„ìš”í•œ ì •ë³´ê°€ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨ëœ ì •ë„ ì¸¡ì •',
            'answer_correctness': '**ë‹µë³€ ì •í™•ì„±** - ë‹µë³€ì´ ì •ë‹µê³¼ ì–¼ë§ˆë‚˜ ì¼ì¹˜í•˜ëŠ”ì§€ ì¸¡ì •'
        }
        
        for metric_name, stats in metric_stats.items():
            description = metric_descriptions.get(metric_name, f'**{metric_name}** - í‰ê°€ ë©”íŠ¸ë¦­')
            # ë” ê°•ë ¥í•œ None ì²˜ë¦¬ - float ë³€í™˜ ì¶”ê°€
            avg_score = float(stats.get('average', 0) or 0)  # None ê°’ ì²˜ë¦¬
            min_score = float(stats.get('minimum', 0) or 0)
            max_score = float(stats.get('maximum', 0) or 0)
            count = int(stats.get('count', 0) or 0)
            
            section.extend([
                f"### {metric_name}",
                f"",
                description,
                f"",
                f"- **í‰ê·  ì ìˆ˜**: {avg_score:.4f}",
                f"- **ì ìˆ˜ ë²”ìœ„**: {min_score:.4f} ~ {max_score:.4f}",
                f"- **í‰ê°€ ì™„ë£Œ**: {count}ê°œ í•­ëª©",
                ""
            ])
            
            # ì„±ëŠ¥ í•´ì„
            interpretation = self._interpret_metric_performance(metric_name, avg_score)
            section.extend([
                f"**ì„±ëŠ¥ í•´ì„**: {interpretation}",
                ""
            ])
        
        return "\n".join(section)
    
    def _generate_performance_analysis(self, results_df: pd.DataFrame, dataset: List[Dict[str, Any]]) -> str:
        """ì„±ëŠ¥ ë¶„ì„ ì„¹ì…˜ ìƒì„±"""
        section = [
            "## ğŸ“ˆ ì„±ëŠ¥ ë¶„ì„",
            ""
        ]
        
        # ìƒìœ„/í•˜ìœ„ ì„±ëŠ¥ í•­ëª© ë¶„ì„
        if not results_df.empty:
            # ì „ì²´ ì ìˆ˜ ê³„ì‚° (ë©”íŠ¸ë¦­ í‰ê· )
            metric_columns = [col for col in results_df.columns 
                            if col not in ['question', 'answer', 'contexts', 'ground_truths', 'reference']]
            
            if metric_columns:
                results_df_copy = results_df.copy()
                # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ ì„ íƒí•˜ê³  í‰ê·  ê³„ì‚°
                numeric_columns = []
                for col in metric_columns:
                    if col in results_df_copy.columns:
                        # ìˆ˜ì¹˜í˜• ë°ì´í„°ë¡œ ë³€í™˜ ê°€ëŠ¥í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
                        try:
                            pd.to_numeric(results_df_copy[col], errors='coerce')
                            numeric_columns.append(col)
                        except:
                            continue
                
                if numeric_columns:
                    numeric_data = results_df_copy[numeric_columns].apply(pd.to_numeric, errors='coerce')
                    results_df_copy['overall_score'] = numeric_data.mean(axis=1, skipna=True)
                else:
                    results_df_copy['overall_score'] = 0
                
                # ìƒìœ„ 3ê°œ
                top_3 = results_df_copy.nlargest(3, 'overall_score')
                section.extend([
                    "### ğŸ† ìµœê³  ì„±ëŠ¥ í•­ëª© (ìƒìœ„ 3ê°œ)",
                    ""
                ])
                
                for i, (idx, row) in enumerate(top_3.iterrows(), 1):
                    question = dataset[idx]['question'] if idx < len(dataset) else "Unknown"
                    question_short = (question[:50] + "...") if len(question) > 50 else question
                    overall_score = float(row['overall_score']) if pd.notna(row['overall_score']) else 0.0
                    section.extend([
                        f"{i}. **ì ìˆ˜: {overall_score:.4f}**",
                        f"   - ì§ˆë¬¸: {question_short}",
                        ""
                    ])
                
                # í•˜ìœ„ 3ê°œ
                bottom_3 = results_df_copy.nsmallest(3, 'overall_score')
                section.extend([
                    "### âš ï¸ ê°œì„  í•„ìš” í•­ëª© (í•˜ìœ„ 3ê°œ)",
                    ""
                ])
                
                for i, (idx, row) in enumerate(bottom_3.iterrows(), 1):
                    question = dataset[idx]['question'] if idx < len(dataset) else "Unknown"
                    question_short = (question[:50] + "...") if len(question) > 50 else question
                    overall_score = float(row['overall_score']) if pd.notna(row['overall_score']) else 0.0
                    section.extend([
                        f"{i}. **ì ìˆ˜: {overall_score:.4f}**",
                        f"   - ì§ˆë¬¸: {question_short}",
                        ""
                    ])
        
        # ì ìˆ˜ ë¶„í¬ ë¶„ì„
        section.extend(self._generate_score_distribution_analysis(results_df))
        
        return "\n".join(section)
    
    def _generate_detailed_results(self, results_df: pd.DataFrame, dataset: List[Dict[str, Any]]) -> str:
        """ìƒì„¸ ê²°ê³¼ ì„¹ì…˜ ìƒì„±"""
        section = [
            "## ğŸ“‹ ìƒì„¸ í‰ê°€ ê²°ê³¼",
            "",
            "ê° ì§ˆë¬¸ë³„ ìƒì„¸ ì ìˆ˜ ë° ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤.",
            ""
        ]
        
        # ë©”íŠ¸ë¦­ ì»¬ëŸ¼ ì‹ë³„
        metric_columns = [col for col in results_df.columns 
                        if col not in ['question', 'answer', 'contexts', 'ground_truths', 'reference']]
        
        if not metric_columns:
            section.append("í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return "\n".join(section)
        
        # í…Œì´ë¸” í—¤ë”
        header_row = "| ë²ˆí˜¸ | ì§ˆë¬¸ | " + " | ".join(metric_columns) + " |"
        separator_row = "|------|------|" + "|".join(["--------" for _ in metric_columns]) + "|"
        
        section.extend([header_row, separator_row])
        
        # ê° í•­ëª©ë³„ ê²°ê³¼
        for i, (idx, row) in enumerate(results_df.iterrows()):
            question = dataset[idx]['question'] if idx < len(dataset) else "Unknown"
            question_short = (question[:30] + "...") if len(question) > 30 else question
            
            # ë©”íŠ¸ë¦­ ì ìˆ˜ë“¤
            metric_scores = []
            for metric in metric_columns:
                score = row[metric]
                if pd.isna(score) or score is None:
                    metric_scores.append("N/A")
                else:
                    try:
                        # ìˆ˜ì¹˜í˜•ìœ¼ë¡œ ë³€í™˜ ì‹œë„
                        numeric_score = float(score)
                        metric_scores.append(f"{numeric_score:.3f}")
                    except (ValueError, TypeError):
                        # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ë¬¸ìì—´ ê·¸ëŒ€ë¡œ í‘œì‹œ
                        metric_scores.append(str(score))
            
            table_row = f"| {i+1} | {question_short} | " + " | ".join(metric_scores) + " |"
            section.append(table_row)
        
        return "\n".join(section)
    
    def _generate_insights_and_recommendations(self, summary_data: Dict[str, Any], results_df: pd.DataFrame) -> str:
        """ì¸ì‚¬ì´íŠ¸ ë° ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        ragas_score = summary_data.get('ragas_score', 0)
        metric_stats = summary_data.get('metric_statistics', {})
        
        section = [
            "## ğŸ’¡ ì¸ì‚¬ì´íŠ¸ ë° ê¶Œì¥ì‚¬í•­",
            ""
        ]
        
        # ì „ì²´ ì„±ëŠ¥ í‰ê°€
        if ragas_score >= 0.8:
            section.extend([
                "### ğŸ‰ ì „ì²´ í‰ê°€",
                "ì „ë°˜ì ìœ¼ë¡œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤. í˜„ì¬ ì‹œìŠ¤í…œì´ íš¨ê³¼ì ìœ¼ë¡œ ì‘ë™í•˜ê³  ìˆìŠµë‹ˆë‹¤.",
                ""
            ])
        elif ragas_score >= 0.6:
            section.extend([
                "### ğŸ“ˆ ì „ì²´ í‰ê°€", 
                "ì–‘í˜¸í•œ ì„±ëŠ¥ì„ ë³´ì´ë‚˜ ì¼ë¶€ ê°œì„ ì˜ ì—¬ì§€ê°€ ìˆìŠµë‹ˆë‹¤.",
                ""
            ])
        else:
            section.extend([
                "### âš ï¸ ì „ì²´ í‰ê°€",
                "ì„±ëŠ¥ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤. ì‹œìŠ¤í…œ ìµœì í™”ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.",
                ""
            ])
        
        # ë©”íŠ¸ë¦­ë³„ ê¶Œì¥ì‚¬í•­
        section.append("### ğŸ”§ ë©”íŠ¸ë¦­ë³„ ê°œì„  ê¶Œì¥ì‚¬í•­")
        section.append("")
        
        for metric_name, stats in metric_stats.items():
            avg_score = stats.get('average', 0)
            recommendation = self._get_improvement_recommendation(metric_name, avg_score)
            section.extend([
                f"**{metric_name}**:",
                f"- {recommendation}",
                ""
            ])
        
        # ì¼ë°˜ì ì¸ ê°œì„  ë°©ì•ˆ
        section.extend([
            "### ğŸš€ ì¼ë°˜ì ì¸ ê°œì„  ë°©ì•ˆ",
            "",
            "1. **ë°ì´í„° í’ˆì§ˆ**: ê³ í’ˆì§ˆì˜ ì»¨í…ìŠ¤íŠ¸ì™€ ì •ë‹µ ë°ì´í„° í™•ë³´",
            "2. **í”„ë¡¬í”„íŠ¸ ìµœì í™”**: LLM í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì„ í†µí•œ ì„±ëŠ¥ ê°œì„ ",
            "3. **ëª¨ë¸ íŠœë‹**: í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì • ë° ëª¨ë¸ ì„ íƒ ìµœì í™”",
            "4. **ì»¨í…ìŠ¤íŠ¸ ê°œì„ **: ê²€ìƒ‰ ì‹œìŠ¤í…œì˜ ì •í™•ë„ ë° ê´€ë ¨ì„± í–¥ìƒ",
            "5. **í‰ê°€ ê¸°ì¤€**: ë„ë©”ì¸ íŠ¹í™” í‰ê°€ ê¸°ì¤€ ê°œë°œ",
            ""
        ])
        
        return "\n".join(section)
    
    def _generate_score_distribution_analysis(self, results_df: pd.DataFrame) -> List[str]:
        """ì ìˆ˜ ë¶„í¬ ë¶„ì„ ìƒì„±"""
        section = [
            "### ğŸ“Š ì ìˆ˜ ë¶„í¬ ë¶„ì„",
            ""
        ]
        
        metric_columns = [col for col in results_df.columns 
                        if col not in ['question', 'answer', 'contexts', 'ground_truths', 'reference']]
        
        for metric in metric_columns:
            scores = results_df[metric].dropna()
            if len(scores) == 0:
                continue
            
            # ìˆ˜ì¹˜í˜•ìœ¼ë¡œ ë³€í™˜ ê°€ëŠ¥í•œ ë°ì´í„°ë§Œ ì²˜ë¦¬
            try:
                numeric_scores = pd.to_numeric(scores, errors='coerce').dropna()
                if len(numeric_scores) == 0:
                    continue
                    
                # ì ìˆ˜ êµ¬ê°„ë³„ ë¶„í¬
                excellent = (numeric_scores >= 0.8).sum()
                good = ((numeric_scores >= 0.6) & (numeric_scores < 0.8)).sum()
                fair = ((numeric_scores >= 0.4) & (numeric_scores < 0.6)).sum()
                poor = (numeric_scores < 0.4).sum()
                scores = numeric_scores  # ìˆ˜ì¹˜í˜• ë°ì´í„°ë¡œ êµì²´
            except:
                continue
            
            section.extend([
                f"**{metric} ë¶„í¬**:",
                f"- ìš°ìˆ˜ (â‰¥0.8): {excellent}ê°œ ({excellent/len(scores)*100:.1f}%)",
                f"- ì–‘í˜¸ (0.6-0.8): {good}ê°œ ({good/len(scores)*100:.1f}%)",
                f"- ë³´í†µ (0.4-0.6): {fair}ê°œ ({fair/len(scores)*100:.1f}%)",
                f"- ë¯¸í¡ (<0.4): {poor}ê°œ ({poor/len(scores)*100:.1f}%)",
                ""
            ])
        
        return section
    
    def _generate_footer(self) -> str:
        """ë³´ê³ ì„œ í‘¸í„° ìƒì„±"""
        footer = [
            "---",
            "",
            "## ğŸ“ ë³´ê³ ì„œ ì •ë³´",
            "",
            f"- **ìƒì„± ë„êµ¬**: RAGTrace Lite v0.1.0",
            f"- **ìƒì„± ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"- **í‰ê°€ í”„ë ˆì„ì›Œí¬**: RAGAS",
            "",
            "ì´ ë³´ê³ ì„œëŠ” RAGTrace Liteì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.",
            ""
        ]
        
        return "\n".join(footer)
    
    def _create_score_bar(self, score: float, max_score: float = 1.0, length: int = 20) -> str:
        """ì ìˆ˜ë¥¼ í…ìŠ¤íŠ¸ ë§‰ëŒ€ê·¸ë˜í”„ë¡œ ë³€í™˜"""
        if score is None or pd.isna(score):
            return "â–‘" * length
        
        filled_length = int((score / max_score) * length)
        filled_length = max(0, min(length, filled_length))
        
        bar = "â–ˆ" * filled_length + "â–‘" * (length - filled_length)
        return bar
    
    def _interpret_score(self, score: float) -> str:
        """ì ìˆ˜ í•´ì„"""
        if score >= 0.9:
            return "ğŸŸ¢ **ë§¤ìš° ìš°ìˆ˜** - íƒì›”í•œ ì„±ëŠ¥"
        elif score >= 0.8:
            return "ğŸŸ¢ **ìš°ìˆ˜** - ë†’ì€ í’ˆì§ˆì˜ ê²°ê³¼"
        elif score >= 0.7:
            return "ğŸŸ¡ **ì–‘í˜¸** - ë§Œì¡±ìŠ¤ëŸ¬ìš´ ì„±ëŠ¥"
        elif score >= 0.6:
            return "ğŸŸ¡ **ë³´í†µ** - ê°œì„  ì—¬ì§€ ìˆìŒ"
        else:
            return "ğŸ”´ **ë¯¸í¡** - ìƒë‹¹í•œ ê°œì„  í•„ìš”"
    
    def _interpret_metric_performance(self, metric_name: str, score: float) -> str:
        """ë©”íŠ¸ë¦­ë³„ ì„±ëŠ¥ í•´ì„"""
        # None ê°’ ì²˜ë¦¬
        if score is None:
            score = 0.0
        base_interpretation = self._interpret_score(score)
        
        specific_advice = {
            'faithfulness': "ë‹µë³€ì´ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì–¼ë§ˆë‚˜ ì¶©ì‹¤íˆ ë°˜ì˜í•˜ëŠ”ì§€ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.",
            'answer_relevancy': "ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ì„±ì´ ë†’ì€ì§€ ë³´ì—¬ì¤ë‹ˆë‹¤.",
            'context_precision': "ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì˜ ì •í™•ë„ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤.",
            'context_recall': "í•„ìš”í•œ ì •ë³´ê°€ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨ëœ ì •ë„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.",
            'answer_correctness': "ë‹µë³€ì˜ ì •í™•ì„±ê³¼ ì™„ì „ì„±ì„ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤."
        }
        
        advice = specific_advice.get(metric_name, "ë©”íŠ¸ë¦­ ì„±ëŠ¥ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.")
        return f"{base_interpretation} {advice}"
    
    def _get_improvement_recommendation(self, metric_name: str, score: float) -> str:
        """ë©”íŠ¸ë¦­ë³„ ê°œì„  ê¶Œì¥ì‚¬í•­"""
        if score >= 0.8:
            return "í˜„ì¬ ì„±ëŠ¥ì´ ìš°ìˆ˜í•©ë‹ˆë‹¤. ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§ì„ ê¶Œì¥í•©ë‹ˆë‹¤."
        
        recommendations = {
            'faithfulness': "ë‹µë³€ ìƒì„± ì‹œ ì»¨í…ìŠ¤íŠ¸ì— ë” ì¶©ì‹¤í•˜ë„ë¡ í”„ë¡¬í”„íŠ¸ë¥¼ ê°œì„ í•˜ì„¸ìš”.",
            'answer_relevancy': "ì§ˆë¬¸-ë‹µë³€ ë§¤ì¹­ì„ ê°œì„ í•˜ê³  ë¶ˆí•„ìš”í•œ ì •ë³´ë¥¼ ì œê±°í•˜ì„¸ìš”.",
            'context_precision': "ê²€ìƒ‰ ì‹œìŠ¤í…œì˜ ì •í™•ë„ë¥¼ ë†’ì´ê³  ë…¸ì´ì¦ˆë¥¼ ì¤„ì´ì„¸ìš”.",
            'context_recall': "ë” í¬ê´„ì ì¸ ì •ë³´ ê²€ìƒ‰ì„ ìœ„í•´ ê²€ìƒ‰ ë²”ìœ„ë¥¼ í™•ëŒ€í•˜ì„¸ìš”.",
            'answer_correctness': "ì •ë‹µ ë°ì´í„°ì˜ í’ˆì§ˆì„ ê°œì„ í•˜ê³  ë‹µë³€ ìƒì„± ë¡œì§ì„ ìµœì í™”í•˜ì„¸ìš”."
        }
        
        return recommendations.get(metric_name, "ì‹œìŠ¤í…œ ì„±ëŠ¥ ìµœì í™”ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.")
    
    def save_report(self, report_content: str, output_path: str, run_id: str) -> str:
        """ë³´ê³ ì„œë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        output_file = Path(output_path) / f"{run_id}_report.md"
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"ğŸ“„ ë³´ê³ ì„œ ì €ì¥ ì™„ë£Œ: {output_file}")
        return str(output_file)


def test_report_generator():
    """ë³´ê³ ì„œ ìƒì„±ê¸° í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ReportGenerator í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    try:
        generator = ReportGenerator()
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°
        run_id = "test_report_001"
        
        summary_data = {
            'run_info': {
                'llm_provider': 'hcx',
                'llm_model': 'HCX-005',
                'dataset_name': 'sample.json',
                'total_items': 3,
                'status': 'completed'
            },
            'metric_statistics': {
                'faithfulness': {'average': 0.85, 'minimum': 0.78, 'maximum': 0.92, 'count': 3},
                'answer_relevancy': {'average': 0.87, 'minimum': 0.83, 'maximum': 0.90, 'count': 3}
            },
            'ragas_score': 0.86,
            'total_metrics': 2
        }
        
        results_df = pd.DataFrame({
            'faithfulness': [0.85, 0.92, 0.78],
            'answer_relevancy': [0.90, 0.88, 0.83]
        })
        
        dataset = [
            {'question': 'í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ 1', 'answer': 'í…ŒìŠ¤íŠ¸ ë‹µë³€ 1'},
            {'question': 'í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ 2', 'answer': 'í…ŒìŠ¤íŠ¸ ë‹µë³€ 2'},
            {'question': 'í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ 3', 'answer': 'í…ŒìŠ¤íŠ¸ ë‹µë³€ 3'}
        ]
        
        # ë³´ê³ ì„œ ìƒì„±
        report = generator.generate_evaluation_report(run_id, summary_data, results_df, dataset)
        
        print("âœ… ë³´ê³ ì„œ ìƒì„± ì„±ê³µ")
        print(f"ë³´ê³ ì„œ ê¸¸ì´: {len(report)} ë¬¸ì")
        
        # íŒŒì¼ë¡œ ì €ì¥ í…ŒìŠ¤íŠ¸
        report_file = generator.save_report(report, "test_reports", run_id)
        
        # í…ŒìŠ¤íŠ¸ íŒŒì¼ ì •ë¦¬
        Path(report_file).unlink(missing_ok=True)
        Path("test_reports").rmdir()
        
        print("âœ… ReportGenerator í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
        return True
        
    except Exception as e:
        print(f"âŒ ReportGenerator í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    test_report_generator()