#!/usr/bin/env python3
"""
RAGTrace Lite - Main Application Class

Original work Copyright 2024 RAGTrace Contributors
Modified work Copyright 2025 RAGTrace Lite Contributors

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

This file has been modified from the original version in the RAGTrace project.
"""

import sys
import time
import traceback
from pathlib import Path
from typing import Optional
from uuid import uuid4

# Use relative imports for package modules
from .config_loader import load_config, Config
from .llm_factory import create_llm, check_llm_connection
from .data_processor import DataProcessor
from .evaluator import RagasEvaluator
from .db_manager import DatabaseManager
from .report_generator import ReportGenerator


class RAGTraceLite:
    """RAGTrace Lite ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ í´ë˜ìŠ¤"""
    
    def __init__(self, config_path: Optional[str] = None):
        """
        RAGTrace Lite ì´ˆê¸°í™”
        
        Args:
            config_path: ì„¤ì • íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: config.yaml)
        """
        self.config = load_config(config_path)
        self.run_id = f"ragtrace_{uuid4().hex[:8]}"
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.llm = None
        self.data_processor = DataProcessor(self.config)
        self.evaluator = None
        self.db_manager = DatabaseManager(self.config)
        self.report_generator = ReportGenerator(self.config)
        
        from . import __version__
        print(f"ğŸš€ RAGTrace Lite v{__version__} ì‹œì‘")
        print(f"ğŸ“Š ì‹¤í–‰ ID: {self.run_id}")
    
    def initialize_llm(self, provider: Optional[str] = None) -> bool:
        """
        LLM ì´ˆê¸°í™” ë° ì—°ê²° í…ŒìŠ¤íŠ¸
        
        Args:
            provider: LLM ì œê³µì ('gemini' ë˜ëŠ” 'hcx')
            
        Returns:
            bool: ì´ˆê¸°í™” ì„±ê³µ ì—¬ë¶€
        """
        try:
            # ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ
            if provider:
                import os
                self.config.llm.provider = provider
                
                # API í‚¤ë„ í•¨ê»˜ ì—…ë°ì´íŠ¸
                if provider.lower() == 'gemini':
                    self.config.llm.api_key = os.getenv('GEMINI_API_KEY', '')
                    self.config.llm.model_name = 'gemini-2.5-flash'
                elif provider.lower() == 'hcx':
                    self.config.llm.api_key = os.getenv('CLOVA_STUDIO_API_KEY', '')
                    self.config.llm.model_name = 'HCX-005'
            
            print(f"ğŸ”§ LLM ì´ˆê¸°í™” ì¤‘: {self.config.llm.provider.upper()}")
            print(f"   ëª¨ë¸: {self.config.llm.model_name}")
            print(f"   API í‚¤: {'ì„¤ì •ë¨' if self.config.llm.api_key else 'ì—†ìŒ'}")
            
            # LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            self.llm = create_llm(self.config)
            
            # ì—°ê²° í…ŒìŠ¤íŠ¸
            if not check_llm_connection(self.llm, self.config.llm.provider):
                return False
            
            # í‰ê°€ê¸° ì´ˆê¸°í™”
            self.evaluator = RagasEvaluator(config=self.config, llm=self.llm)
            
            return True
            
        except Exception as e:
            print(f"âŒ LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def evaluate_dataset(self, 
                        data_path: str, 
                        output_dir: Optional[str] = None,
                        llm_provider: Optional[str] = None) -> bool:
        """
        ë°ì´í„°ì…‹ í‰ê°€ ìˆ˜í–‰
        
        Args:
            data_path: í‰ê°€ ë°ì´í„° íŒŒì¼ ê²½ë¡œ
            output_dir: ê²°ê³¼ ì¶œë ¥ ë””ë ‰í† ë¦¬
            llm_provider: LLM ì œê³µì ì˜¤ë²„ë¼ì´ë“œ
            
        Returns:
            bool: í‰ê°€ ì„±ê³µ ì—¬ë¶€
        """
        try:
            start_time = time.time()
            
            # LLM ì´ˆê¸°í™”
            if not self.initialize_llm(llm_provider):
                return False
            
            # 1. ë°ì´í„° ë¡œë”©
            print(f"\nğŸ“ ë°ì´í„° ë¡œë”©: {data_path}")
            dataset = self.data_processor.load_and_prepare_data(data_path)
            
            if not dataset:
                print("âŒ ë°ì´í„°ì…‹ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
                return False
            
            print(f"âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ: {len(dataset)}ê°œ í•­ëª©")
            
            # 2. ë°ì´í„°ë² ì´ìŠ¤ì— í‰ê°€ ì‹¤í–‰ ê¸°ë¡
            print(f"\nğŸ’¾ í‰ê°€ ì‹¤í–‰ ê¸°ë¡ ìƒì„±...")
            self.db_manager.create_evaluation_run(
                run_id=self.run_id,
                llm_provider=self.config.llm.provider,
                llm_model=self.config.llm.model_name or "default",
                dataset_name=Path(data_path).name,
                total_items=len(dataset),
                metrics=self.config.evaluation.metrics,
                config_data=self.config.model_dump()
            )
            
            # 3. RAGAS í‰ê°€ ìˆ˜í–‰
            print(f"\nğŸ” RAGAS í‰ê°€ ì‹œì‘...")
            print(f"   - LLM: {self.config.llm.provider.upper()}")
            print(f"   - ë©”íŠ¸ë¦­: {', '.join(self.config.evaluation.metrics)}")
            print(f"   - ë°°ì¹˜ í¬ê¸°: {self.config.evaluation.batch_size}")
            
            results_df = self.evaluator.evaluate(dataset)
            
            if results_df is None or results_df.empty:
                print("âŒ í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            print(f"âœ… í‰ê°€ ì™„ë£Œ: {len(results_df)}ê°œ ê²°ê³¼")
            
            # 4. ê²°ê³¼ ì €ì¥
            print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...")
            self.db_manager.save_evaluation_results(
                run_id=self.run_id,
                results_df=results_df,
                dataset=dataset
            )
            self.db_manager.complete_evaluation_run(self.run_id)
            
            # 5. ìš”ì•½ í†µê³„ ê³„ì‚°
            print(f"\nğŸ“Š í†µê³„ ê³„ì‚° ì¤‘...")
            summary_data = self.db_manager.get_evaluation_summary(self.run_id)
            
            # 6. ë³´ê³ ì„œ ìƒì„±
            print(f"\nğŸ“„ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
            report_content = self.report_generator.generate_evaluation_report(
                run_id=self.run_id,
                summary_data=summary_data,
                results_df=results_df,
                dataset=dataset
            )
            
            # 7. íŒŒì¼ ì €ì¥
            output_path = output_dir or "reports"
            report_file = self.report_generator.save_report(
                report_content=report_content,
                output_path=output_path,
                run_id=self.run_id
            )
            
            # 8. ê²°ê³¼ ìš”ì•½ ì¶œë ¥
            elapsed_time = time.time() - start_time
            self._print_evaluation_summary(summary_data, report_file, elapsed_time)
            
            return True
            
        except Exception as e:
            print(f"âŒ í‰ê°€ ìˆ˜í–‰ ì‹¤íŒ¨: {e}")
            traceback.print_exc()
            return False
    
    def _print_evaluation_summary(self, summary_data: dict, report_file: str, elapsed_time: float):
        """í‰ê°€ ê²°ê³¼ ìš”ì•½ì„ ì½˜ì†”ì— ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ‰ RAGTrace Lite í‰ê°€ ì™„ë£Œ!")
        print("="*60)
        
        # ê¸°ë³¸ ì •ë³´
        run_info = summary_data.get('run_info', {})
        print(f"ğŸ“Š ì‹¤í–‰ ID: {self.run_id}")
        print(f"ğŸ¤– LLM: {run_info.get('llm_provider', 'Unknown').upper()}")
        print(f"ğŸ“ ë°ì´í„°ì…‹: {run_info.get('dataset_name', 'Unknown')}")
        print(f"ğŸ“‹ í‰ê°€ í•­ëª©: {run_info.get('total_items', 0)}ê°œ")
        print(f"â±ï¸  ì†Œìš” ì‹œê°„: {elapsed_time:.1f}ì´ˆ")
        
        # RAGAS ì ìˆ˜
        ragas_score = summary_data.get('ragas_score')
        if ragas_score is not None:
            print(f"\nğŸ¯ ì „ì²´ RAGAS ì ìˆ˜: {ragas_score:.4f}")
            
            # ì ìˆ˜ í•´ì„
            if ragas_score >= 0.8:
                print("   ğŸŸ¢ ìš°ìˆ˜í•œ ì„±ëŠ¥!")
            elif ragas_score >= 0.6:
                print("   ğŸŸ¡ ì–‘í˜¸í•œ ì„±ëŠ¥")
            else:
                print("   ğŸ”´ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤")
        
        # ë©”íŠ¸ë¦­ë³„ ì ìˆ˜
        metric_stats = summary_data.get('metric_statistics', {})
        if metric_stats:
            print(f"\nğŸ“ˆ ë©”íŠ¸ë¦­ë³„ ì ìˆ˜:")
            for metric_name, stats in metric_stats.items():
                avg_score = stats.get('average', 0)
                print(f"   {metric_name}: {avg_score:.4f}")
        
        # íŒŒì¼ ê²½ë¡œ
        print(f"\nğŸ“„ ìƒì„¸ ë³´ê³ ì„œ: {report_file}")
        print(f"ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤: {self.db_manager.db_path}")
        print("="*60)
    
    def list_evaluations(self, limit: int = 10):
        """ìµœê·¼ í‰ê°€ ê¸°ë¡ ëª©ë¡ ì¶œë ¥"""
        evaluations = self.db_manager.list_evaluations(limit)
        
        if not evaluations:
            print("í‰ê°€ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"\nğŸ“‹ ìµœê·¼ í‰ê°€ ê¸°ë¡ ({len(evaluations)}ê°œ):")
        print("-" * 80)
        print(f"{'Run ID':<12} {'ë‚ ì§œ':<12} {'LLM':<8} {'ë°ì´í„°ì…‹':<20} {'ìƒíƒœ':<10}")
        print("-" * 80)
        
        for eval_record in evaluations:
            run_id = eval_record['run_id'][:10] + "..."
            timestamp = eval_record['timestamp'][:10]
            provider = eval_record['llm_provider'].upper()
            dataset = eval_record['dataset_name'][:18] + "..." if len(eval_record['dataset_name']) > 18 else eval_record['dataset_name']
            status = eval_record['status']
            
            print(f"{run_id:<12} {timestamp:<12} {provider:<8} {dataset:<20} {status:<10}")
    
    def show_evaluation_details(self, run_id: str):
        """íŠ¹ì • í‰ê°€ì˜ ìƒì„¸ ì •ë³´ ì¶œë ¥"""
        try:
            summary_data = self.db_manager.get_evaluation_summary(run_id)
            
            print(f"\nğŸ“Š í‰ê°€ ìƒì„¸ ì •ë³´: {run_id}")
            print("="*50)
            
            run_info = summary_data['run_info']
            print(f"ë‚ ì§œ: {run_info['timestamp']}")
            print(f"LLM: {run_info['llm_provider']} - {run_info.get('llm_model', 'Unknown')}")
            print(f"ë°ì´í„°ì…‹: {run_info['dataset_name']}")
            print(f"í‰ê°€ í•­ëª©: {run_info['total_items']}ê°œ")
            print(f"ìƒíƒœ: {run_info['status']}")
            
            # RAGAS ì ìˆ˜
            ragas_score = summary_data.get('ragas_score')
            if ragas_score:
                print(f"\nğŸ¯ ì „ì²´ RAGAS ì ìˆ˜: {ragas_score:.4f}")
            
            # ë©”íŠ¸ë¦­ë³„ í†µê³„
            metric_stats = summary_data.get('metric_statistics', {})
            if metric_stats:
                print(f"\nğŸ“ˆ ë©”íŠ¸ë¦­ë³„ í†µê³„:")
                for metric_name, stats in metric_stats.items():
                    print(f"  {metric_name}:")
                    print(f"    í‰ê· : {stats['average']:.4f}")
                    print(f"    ë²”ìœ„: {stats['minimum']:.4f} ~ {stats['maximum']:.4f}")
                    print(f"    ê°œìˆ˜: {stats['count']}")
                    
        except Exception as e:
            print(f"âŒ í‰ê°€ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.db_manager:
            self.db_manager.close()