#!/usr/bin/env python3
"""
RAGTrace Lite CLI entry point

Original work Copyright 2024 RAGTrace Contributors
Modified work Copyright 2025 RAGTrace Lite Contributors

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

This file has been created as part of the RAGTrace Lite project.
"""

import sys
import argparse
import logging
from pathlib import Path
from typing import Optional

from .main import RAGTraceLite
from . import __version__

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def main():
    """Main entry point for ragtrace-lite CLI"""
    parser = argparse.ArgumentParser(
        description='RAGTrace Lite - Lightweight RAG Evaluation Framework'
    )
    
    subparsers = parser.add_subparsers(dest='command', help='Available commands')
    
    # Evaluate command
    evaluate_parser = subparsers.add_parser('evaluate', help='Run RAG evaluation')
    evaluate_parser.add_argument(
        'data_file',
        type=str,
        help='Path to evaluation data file (JSON format)'
    )
    evaluate_parser.add_argument(
        '--llm',
        type=str,
        choices=['hcx', 'gemini'],
        default=None,
        help='LLM to use for evaluation (default: from config)'
    )
    evaluate_parser.add_argument(
        '--output',
        type=str,
        default=None,
        help='Output file path for results'
    )
    evaluate_parser.add_argument(
        '--config',
        type=str,
        default='config.yaml',
        help='Path to configuration file'
    )
    
    # List datasets command
    list_parser = subparsers.add_parser('list-datasets', help='List available datasets')
    list_parser.add_argument(
        '--data-dir',
        type=str,
        default='./data',
        help='Directory containing datasets'
    )
    
    # Version command
    version_parser = subparsers.add_parser('version', help='Show version information')
    
    # Test HCX command
    test_parser = subparsers.add_parser('test-hcx', help='Test HCX-005 & BGE-M3 setup')
    test_parser.add_argument(
        '--quick',
        action='store_true',
        help='Quick test with minimal data'
    )
    test_parser.add_argument(
        '--full',
        action='store_true',
        help='Full pipeline test including DB and report'
    )
    
    # Dashboard command
    dashboard_parser = subparsers.add_parser('dashboard', help='Generate web dashboard')
    dashboard_parser.add_argument(
        '--open',
        action='store_true',
        help='Open dashboard in browser after generation'
    )
    
    args = parser.parse_args()
    
    if args.command == 'evaluate':
        run_evaluation(args)
    elif args.command == 'list-datasets':
        list_datasets(args)
    elif args.command == 'version':
        show_version()
    elif args.command == 'test-hcx':
        test_hcx(args)
    elif args.command == 'dashboard':
        run_dashboard(args)
    else:
        parser.print_help()
        sys.exit(1)


def run_evaluation(args):
    """Run RAG evaluation"""
    try:
        # Create RAGTrace Lite instance
        app = RAGTraceLite(args.config)
        
        # Run evaluation
        success = app.evaluate_dataset(
            data_path=args.data_file,
            output_dir=args.output,
            llm_provider=args.llm
        )
        
        app.cleanup()
        
        if not success:
            logger.error("Evaluation failed")
            sys.exit(1)
        else:
            sys.exit(0)
        
    except Exception as e:
        logger.error(f"Evaluation failed: {str(e)}")
        sys.exit(1)


def list_datasets(args):
    """List available datasets"""
    data_dir = Path(args.data_dir)
    if not data_dir.exists():
        logger.error(f"Data directory not found: {data_dir}")
        sys.exit(1)
    
    json_files = list(data_dir.glob("*.json"))
    xlsx_files = list(data_dir.glob("*.xlsx"))
    all_files = sorted(json_files + xlsx_files)
    
    if not all_files:
        logger.info("No datasets found in data directory")
        return
    
    logger.info(f"Available datasets in {data_dir}:")
    for file in all_files:
        logger.info(f"  - {file.name}")


def show_version():
    """Show version information"""
    print(f"RAGTrace Lite v{__version__}")
    print("Licensed under Apache-2.0")
    print("For more information: https://github.com/ntts9990/ragtrace-lite")


def test_hcx(args):
    """Test HCX-005 & BGE-M3 setup"""
    import os
    from datetime import datetime
    from .config_loader import load_config
    from .llm_factory import create_llm, check_llm_connection
    from .data_processor import DataProcessor
    from .evaluator import RagasEvaluator
    
    print("=" * 70)
    print("ğŸ§ª HCX-005 & BGE-M3 í…ŒìŠ¤íŠ¸")
    print("=" * 70)
    print(f"ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    # 1. í™˜ê²½ í™•ì¸
    print("1ï¸âƒ£ í™˜ê²½ ì„¤ì • í™•ì¸")
    print("-" * 50)
    
    # API í‚¤ í™•ì¸
    hcx_key = os.getenv('CLOVA_STUDIO_API_KEY', '').strip()
    if hcx_key and hcx_key.startswith('nv-'):
        print(f"âœ… HCX API í‚¤: ì„¤ì •ë¨ ({hcx_key[:10]}...)")
    else:
        print("âŒ HCX API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
        print("   export CLOVA_STUDIO_API_KEY='your-key' ì‹¤í–‰ í•„ìš”")
        sys.exit(1)
    
    # ì„¤ì • ë¡œë“œ
    try:
        config = load_config()
        print(f"âœ… ì„¤ì • íŒŒì¼ ë¡œë“œ: config.yaml")
        print(f"   - LLM: {config.llm.provider} ({config.llm.model_name})")
        print(f"   - Embedding: {config.embedding.provider}")
    except Exception as e:
        print(f"âŒ ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")
        sys.exit(1)
    
    # 2. LLM ì—°ê²° í…ŒìŠ¤íŠ¸
    print("\n2ï¸âƒ£ HCX-005 ì—°ê²° í…ŒìŠ¤íŠ¸")
    print("-" * 50)
    
    try:
        llm = create_llm(config)
        print(f"âœ… LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±: {type(llm).__name__}")
        
        if check_llm_connection(llm, config.llm.provider):
            print("âœ… HCX-005 API ì—°ê²° ì„±ê³µ")
        else:
            print("âŒ HCX-005 API ì—°ê²° ì‹¤íŒ¨")
            sys.exit(1)
    except Exception as e:
        print(f"âŒ LLM ìƒì„± ì‹¤íŒ¨: {e}")
        sys.exit(1)
    
    # 3. BGE-M3 ì„ë² ë”© í…ŒìŠ¤íŠ¸
    print("\n3ï¸âƒ£ BGE-M3 ì„ë² ë”© í…ŒìŠ¤íŠ¸")
    print("-" * 50)
    
    if config.embedding.provider == 'bge_m3':
        print("âœ… BGE-M3 ì„¤ì • í™•ì¸")
        model_path = Path('./models/bge-m3')
        if model_path.exists():
            print(f"âœ… BGE-M3 ëª¨ë¸ ì¡´ì¬: {model_path}")
        else:
            print("âš ï¸  BGE-M3 ëª¨ë¸ì´ ì—†ìŒ (ì²« ì‹¤í–‰ ì‹œ ìë™ ë‹¤ìš´ë¡œë“œ)")
    else:
        print(f"âš ï¸  ë‹¤ë¥¸ ì„ë² ë”© ì‚¬ìš© ì¤‘: {config.embedding.provider}")
    
    if args.quick:
        # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
        print("\n4ï¸âƒ£ ë¹ ë¥¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸")
        print("-" * 50)
        
        # ê°„ë‹¨í•œ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
        test_data = {
            'question': ['í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ì…ë‹ˆë‹¤'],
            'answer': ['í…ŒìŠ¤íŠ¸ ë‹µë³€ì…ë‹ˆë‹¤'],
            'contexts': [['í…ŒìŠ¤íŠ¸ ì»¨í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤']],
            'ground_truths': [['í…ŒìŠ¤íŠ¸ ì •ë‹µì…ë‹ˆë‹¤']]
        }
        
        try:
            from datasets import Dataset
            dataset = Dataset.from_dict(test_data)
            print("âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„±")
            
            # í‰ê°€ì ìƒì„±
            evaluator = RagasEvaluator(config, llm=llm)
            print("âœ… í‰ê°€ì ì´ˆê¸°í™” ì„±ê³µ")
            
            print("\nâœ… ëª¨ë“  êµ¬ì„± ìš”ì†Œê°€ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤!")
            
        except Exception as e:
            print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            sys.exit(1)
    
    elif args.full:
        # ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
        print("\n4ï¸âƒ£ ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰")
        print("-" * 50)
        print("ğŸ“Œ íŒŒì´í”„ë¼ì¸: ë°ì´í„° ìƒì„± â†’ í‰ê°€ â†’ DB ì €ì¥ â†’ ë³´ê³ ì„œ ìƒì„±")
        
        # test_full_pipeline.pyì˜ í•¨ìˆ˜ ì¬ì‚¬ìš©
        import subprocess
        import os
        
        # API í‚¤ í™˜ê²½ë³€ìˆ˜ ì„¤ì •
        env = os.environ.copy()
        env['CLOVA_STUDIO_API_KEY'] = os.getenv('CLOVA_STUDIO_API_KEY', '')
        
        # ì „ì²´ íŒŒì´í”„ë¼ì¸ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
        try:
            result = subprocess.run(
                [sys.executable, 'test_full_pipeline.py'],
                env=env,
                capture_output=True,
                text=True
            )
            
            if result.returncode == 0:
                print("\nâœ… ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
            else:
                print(f"\nâŒ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {result.stderr}")
                
        except Exception as e:
            print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
    
    else:
        # ê¸°ë³¸ í…ŒìŠ¤íŠ¸
        print("\nâœ… HCX-005 & BGE-M3 ì„¤ì •ì´ ì˜¬ë°”ë¥´ê²Œ ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        print("\nì‚¬ìš© ê°€ëŠ¥í•œ ì˜µì…˜:")
        print("  --quick : ë¹ ë¥¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸")
        print("  --full  : ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸")
    
    print("\n" + "=" * 70)
    print(f"ì™„ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")


def run_dashboard(args):
    """Generate web dashboard"""
    try:
        from .web_dashboard import generate_web_dashboard
        
        dashboard_path = generate_web_dashboard()
        print(f"âœ… ì›¹ ëŒ€ì‹œë³´ë“œ ìƒì„± ì™„ë£Œ: {dashboard_path}")
        
        if args.open:
            import webbrowser
            import os
            dashboard_url = f"file://{os.path.abspath(dashboard_path)}"
            webbrowser.open(dashboard_url)
            print(f"ğŸŒ ë¸Œë¼ìš°ì €ì—ì„œ ëŒ€ì‹œë³´ë“œ ì—´ê¸°: {dashboard_url}")
        else:
            import os
            dashboard_url = f"file://{os.path.abspath(dashboard_path)}"
            print(f"ğŸŒ ëŒ€ì‹œë³´ë“œ URL: {dashboard_url}")
            
    except Exception as e:
        logger.error(f"Dashboard generation failed: {e}")
        sys.exit(1)


def main_enhanced():
    """Entry point for enhanced CLI (ragtrace-lite-enhanced)"""
    from .main_enhanced import RAGTraceLiteEnhanced
    
    parser = argparse.ArgumentParser(
        description='RAGTrace Lite Enhanced - Advanced RAG Evaluation'
    )
    
    subparsers = parser.add_subparsers(dest='command', help='Available commands')
    
    # Evaluate command
    evaluate_parser = subparsers.add_parser('evaluate', help='Run RAG evaluation')
    evaluate_parser.add_argument(
        'data_file',
        type=str,
        help='Path to evaluation data file (JSON or XLSX format)'
    )
    evaluate_parser.add_argument(
        '--llm',
        type=str,
        choices=['hcx', 'gemini'],
        default=None,
        help='LLM to use for evaluation'
    )
    evaluate_parser.add_argument(
        '--output',
        type=str,
        default=None,
        help='Output directory for results'
    )
    evaluate_parser.add_argument(
        '--config',
        type=str,
        default='config.yaml',
        help='Path to configuration file'
    )
    
    # List evaluations
    list_parser = subparsers.add_parser('list', help='List recent evaluations')
    list_parser.add_argument(
        '--limit',
        type=int,
        default=10,
        help='Number of evaluations to show'
    )
    
    # Show details
    show_parser = subparsers.add_parser('show', help='Show evaluation details')
    show_parser.add_argument(
        'run_id',
        type=str,
        help='Run ID to show details for'
    )
    
    # Export logs
    export_parser = subparsers.add_parser('export-logs', help='Export logs for Elasticsearch')
    export_parser.add_argument(
        'output_path',
        type=str,
        help='Output file path (.ndjson)'
    )
    
    # Version
    version_parser = subparsers.add_parser('version', help='Show version information')
    
    args = parser.parse_args()
    
    if args.command == 'version':
        show_version()
        return
    
    # Create enhanced app instance
    app = RAGTraceLiteEnhanced(getattr(args, 'config', None))
    
    try:
        if args.command == 'evaluate':
            success = app.evaluate_dataset(
                args.data_file,
                output_dir=args.output,
                llm_override=args.llm
            )
            sys.exit(0 if success else 1)
        elif args.command == 'list':
            app.list_evaluations(args.limit)
        elif args.command == 'show':
            app.show_evaluation_details(args.run_id)
        elif args.command == 'export-logs':
            app.export_logs(args.output_path)
        else:
            parser.print_help()
    finally:
        app.cleanup()


if __name__ == "__main__":
    main()