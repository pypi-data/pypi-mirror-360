"""
RAGTrace Lite LLM Factory (ë¹„ë™ê¸° ë²„ì „)

ë©”ì¸ RAGTrace ì–´ëŒ‘í„° ìœ„ì„ íŒ¨í„´ ì‚¬ìš©:
- ë³„ë„ ì–´ëŒ‘í„° í´ë˜ìŠ¤ + LangChain ë˜í¼
- Pydantic í•„ë“œ ë¬¸ì œ íšŒí”¼
- RAGAS ì™„ì „ í˜¸í™˜
"""

import asyncio
import json
import time
import uuid
import aiohttp
import google.generativeai as genai
from typing import List, Dict, Any, Optional
from langchain_core.language_models.llms import LLM
from langchain_core.outputs import Generation, LLMResult
from langchain_core.callbacks import AsyncCallbackManagerForLLMRun, CallbackManagerForLLMRun
from langchain_core.prompt_values import StringPromptValue

from .config_loader import Config


class GeminiAdapter:
    """Gemini API ì–´ëŒ‘í„° (ìˆœìˆ˜ Python í´ë˜ìŠ¤)"""
    
    def __init__(self, api_key: str, model_name: str = "gemini-2.5-flash"):
        # API í‚¤ ê²€ì¦ ë° ì •ë¦¬
        if not api_key:
            raise ValueError("GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ì¤„ë°”ê¿ˆ ë° ê³µë°± ì œê±°
        api_key = api_key.strip()
        
        self.api_key = api_key
        self.model_name = model_name
        
        # Gemini API ì„¤ì •
        genai.configure(api_key=api_key)
        self.gemini_model = genai.GenerativeModel(model_name)
        
        print(f"ğŸ¤– Gemini ì–´ëŒ‘í„° ì´ˆê¸°í™”: {model_name}")
    
    async def agenerate_answer(self, prompt: str, **kwargs) -> str:
        """Gemini API ë¹„ë™ê¸° í˜¸ì¶œ"""
        try:
            # ìƒì„± ì„¤ì •
            generation_config = {
                'temperature': kwargs.get('temperature', 0.1),
                'max_output_tokens': kwargs.get('max_tokens', 8192),  # RAGASë¥¼ ìœ„í•´ ì¦ê°€
                'top_p': kwargs.get('top_p', 0.95),
            }
            
            # ì•ˆì „ ì„¤ì •
            safety_settings = [
                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
            ]
            
            # ë¹„ë™ê¸° ì‹¤í–‰
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: self.gemini_model.generate_content(
                    prompt,
                    generation_config=generation_config,
                    safety_settings=safety_settings
                )
            )
            
            # ì‘ë‹µ ì²˜ë¦¬
            if response.candidates:
                candidate = response.candidates[0]
                # finish_reason í™•ì¸ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ
                if candidate.finish_reason == 1:  # STOP
                    return response.text if response.text else "ì‘ë‹µì´ ìƒì„±ë˜ì—ˆìœ¼ë‚˜ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."
                elif candidate.finish_reason == 2:  # MAX_TOKENS
                    # ë¶€ë¶„ ì‘ë‹µì´ë¼ë„ ìˆìœ¼ë©´ ë°˜í™˜
                    if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                        text_parts = []
                        for part in candidate.content.parts:
                            if hasattr(part, 'text'):
                                text_parts.append(part.text)
                        if text_parts:
                            return ''.join(text_parts) + " [ìµœëŒ€ í† í° ìˆ˜ ë„ë‹¬]"
                    return "ìµœëŒ€ í† í° ìˆ˜ì— ë„ë‹¬í–ˆìœ¼ë‚˜ ì‘ë‹µì´ ì—†ìŠµë‹ˆë‹¤."
                elif candidate.finish_reason == 3:  # SAFETY
                    return "ì•ˆì „ í•„í„°ì— ì˜í•´ ì°¨ë‹¨ëœ ì‘ë‹µì…ë‹ˆë‹¤."
                else:
                    # ê¸°íƒ€ ê²½ìš°ì—ë„ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
                    try:
                        return response.text
                    except:
                        if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                            text_parts = []
                            for part in candidate.content.parts:
                                if hasattr(part, 'text'):
                                    text_parts.append(part.text)
                            if text_parts:
                                return ''.join(text_parts)
                        return f"ì‘ë‹µ ì¶”ì¶œ ì‹¤íŒ¨ (finish_reason: {candidate.finish_reason})"
            else:
                return "ì‘ë‹µ í›„ë³´ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                
        except Exception as e:
            print(f"âŒ Gemini API ì˜¤ë¥˜: {e}")
            return f"Gemini API ì˜¤ë¥˜: {str(e)}"
    
    def generate_answer(self, prompt: str, **kwargs) -> str:
        """ë™ê¸° í˜¸ì¶œ (ë¹„ë™ê¸°ë¥¼ ë™ê¸°ë¡œ ë˜í•‘)"""
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ê°€ ìˆìœ¼ë©´ executor ì‚¬ìš©
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(asyncio.run, self.agenerate_answer(prompt, **kwargs))
                    return future.result()
            else:
                return asyncio.run(self.agenerate_answer(prompt, **kwargs))
        except Exception as e:
            print(f"âŒ Gemini ë™ê¸° í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return f"Error: {str(e)}"


class HcxAdapter:
    """HCX API ì–´ëŒ‘í„° (ìˆœìˆ˜ Python í´ë˜ìŠ¤)"""
    
    # í´ë˜ìŠ¤ ë³€ìˆ˜ë¡œ ë§ˆì§€ë§‰ ìš”ì²­ ì‹œê°„ ì €ì¥
    _last_request_time = 0
    _min_request_interval = 12.0  # ìµœì†Œ 12ì´ˆ ê°„ê²© (HCX API ì œí•œ ëŒ€ì‘, ì—¬ìœ ìˆê²Œ ì„¤ì •)
    
    def __init__(self, api_key: str, model_name: str = "HCX-005"):
        # API í‚¤ ê²€ì¦ ë° ì •ë¦¬
        if not api_key:
            raise ValueError("CLOVA_STUDIO_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ì¤„ë°”ê¿ˆ ë° ê³µë°± ì œê±°
        api_key = api_key.strip()
        
        if not api_key.startswith("nv-"):
            raise ValueError("CLOVA_STUDIO_API_KEYëŠ” 'nv-'ë¡œ ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤.")
            
        self.api_key = api_key
        self.model_name = model_name
        self.api_url = f"https://clovastudio.stream.ntruss.com/testapp/v3/chat-completions/{model_name}"
        
        # ë™ì  ìš”ì²­ ID ìƒì„±
        request_id = str(uuid.uuid4())
        
        self.headers = {
            "Authorization": f"Bearer {api_key}",  # ì¤‘ìš”: Bearer í† í° ì¶”ê°€
            "X-NCP-CLOVASTUDIO-API-KEY": api_key,
            "X-NCP-APIGW-API-KEY": api_key,
            "X-NCP-CLOVASTUDIO-REQUEST-ID": request_id,
            "Content-Type": "application/json",
            "Accept": "application/json"
        }
        
        print(f"ğŸ¤– HCX ì–´ëŒ‘í„° ì´ˆê¸°í™”: {model_name}")
    
    async def agenerate_answer(self, prompt: str, max_retries: int = 3, **kwargs) -> str:
        """HCX API ë¹„ë™ê¸° í˜¸ì¶œ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
        from .hcx_ragas_adapter import HCXRAGASAdapter
        
        # ë©”íŠ¸ë¦­ íƒ€ì… ê°ì§€ ë° í”„ë¡¬í”„íŠ¸ ê°•í™”
        metric_type = HCXRAGASAdapter.detect_metric_type(prompt)
        self._last_metric_type = metric_type  # ë‚˜ì¤‘ì— ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì €ì¥
        
        # í”„ë¡¬í”„íŠ¸ ê°•í™” (JSON ì‘ë‹µ ìœ ë„)
        enhanced_prompt = HCXRAGASAdapter.enhance_prompt_for_hcx(prompt, metric_type)
        
        for attempt in range(max_retries + 1):
            try:
                # Rate limiting - ìš”ì²­ ê°„ ìµœì†Œ ê°„ê²© ìœ ì§€
                current_time = time.time()
                time_since_last = current_time - HcxAdapter._last_request_time
                if time_since_last < HcxAdapter._min_request_interval:
                    wait_time = HcxAdapter._min_request_interval - time_since_last
                    print(f"â±ï¸  HCX Rate limit ëŒ€ê¸°: {wait_time:.1f}ì´ˆ")
                    await asyncio.sleep(wait_time)
                
                HcxAdapter._last_request_time = time.time()
                
                payload = {
                    "messages": [
                        {
                            "role": "user",
                            "content": enhanced_prompt
                        }
                    ],
                    "topP": kwargs.get('top_p', 0.8),
                    "topK": kwargs.get('top_k', 0),
                    "maxTokens": kwargs.get('max_tokens', 1000),
                    "temperature": kwargs.get('temperature', 0.5),
                    "repetitionPenalty": kwargs.get('repetition_penalty', 1.1),
                    "stop": [],
                    "includeAiFilters": True,
                    "seed": 0
                }
                
                # ê° ìš”ì²­ë§ˆë‹¤ ìƒˆë¡œìš´ ìš”ì²­ ID ìƒì„±
                headers = self.headers.copy()
                headers["X-NCP-CLOVASTUDIO-REQUEST-ID"] = str(uuid.uuid4())
                
                # aiohttp ë¹„ë™ê¸° ìš”ì²­
                async with aiohttp.ClientSession() as session:
                    async with session.post(
                        self.api_url,
                        headers=headers,
                        json=payload,
                        timeout=aiohttp.ClientTimeout(total=30)
                    ) as response:
                        
                        if response.status == 200:
                            result = await response.json()
                            
                            if 'result' in result and 'message' in result['result']:
                                content = result['result']['message']['content']
                                if content:
                                    # RAGAS í˜¸í™˜ì„±ì„ ìœ„í•œ ì‘ë‹µ í›„ì²˜ë¦¬
                                    cleaned_content = self._clean_response_for_ragas(content)
                                    
                                    # JSON ì‘ë‹µì´ ì•„ë‹ˆë©´ ë©”íŠ¸ë¦­ì— ë§ê²Œ ë³€í™˜
                                    if hasattr(self, '_last_metric_type') and self._last_metric_type:
                                        if not cleaned_content.strip().startswith('{'):
                                            from .hcx_ragas_adapter import HCXRAGASAdapter
                                            parsed = HCXRAGASAdapter.parse_hcx_response(cleaned_content, self._last_metric_type)
                                            return json.dumps(parsed, ensure_ascii=False)
                                    
                                    return cleaned_content
                                else:
                                    return "HCX APIì—ì„œ ë¹ˆ ì‘ë‹µì„ ë°›ì•˜ìŠµë‹ˆë‹¤."
                            else:
                                return f"HCX API ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜: {result}"
                                
                        elif response.status == 429:  # Rate limit ì˜¤ë¥˜
                            error_text = await response.text()
                            if attempt < max_retries:
                                wait_time = (attempt + 1) * 10  # ì§€ìˆ˜ì  ë°±ì˜¤í”„: 10ì´ˆ, 20ì´ˆ, 30ì´ˆ
                                print(f"âŒ HCX Rate limit (ì‹œë„ {attempt + 1}/{max_retries + 1}): {wait_time}ì´ˆ í›„ ì¬ì‹œë„")
                                await asyncio.sleep(wait_time)
                                continue
                            else:
                                print(f"âŒ HCX Rate limit ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼: {error_text}")
                                return f"HCX API Rate limit ì´ˆê³¼: {error_text}"
                        else:
                            error_text = await response.text()
                            print(f"âŒ HCX API ì˜¤ë¥˜ {response.status}: {error_text}")
                            return f"HCX API ì˜¤ë¥˜ ({response.status}): {error_text}"
                            
            except asyncio.TimeoutError:
                if attempt < max_retries:
                    print(f"âŒ HCX API íƒ€ì„ì•„ì›ƒ (ì‹œë„ {attempt + 1}/{max_retries + 1}): ì¬ì‹œë„")
                    await asyncio.sleep(5)
                    continue
                else:
                    print("âŒ HCX API íƒ€ì„ì•„ì›ƒ ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼")
                    return "HCX API ìš”ì²­ íƒ€ì„ì•„ì›ƒ"
            except Exception as e:
                if attempt < max_retries:
                    print(f"âŒ HCX API ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}/{max_retries + 1}): {e}")
                    await asyncio.sleep(5)
                    continue
                else:
                    print(f"âŒ HCX API ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼: {e}")
                    return f"HCX API ì˜¤ë¥˜: {str(e)}"
        
        return "HCX API ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼"
    
    def _clean_response_for_ragas(self, content: str) -> str:
        """RAGAS í˜¸í™˜ì„±ì„ ìœ„í•œ ì‘ë‹µ í›„ì²˜ë¦¬"""
        import re
        import json
        from .hcx_ragas_adapter import HCXRAGASAdapter
        
        # í”„ë¡¬í”„íŠ¸ì—ì„œ ë©”íŠ¸ë¦­ íƒ€ì… ê°ì§€ (ì €ì¥ëœ ê°’ ì‚¬ìš©)
        metric_type = getattr(self, '_last_metric_type', None)
        
        # HCX RAGAS ì–´ëŒ‘í„°ë¡œ íŒŒì‹±
        try:
            parsed = HCXRAGASAdapter.parse_hcx_response(content, metric_type)
            return json.dumps(parsed, ensure_ascii=False)
        except Exception as e:
            print(f"âš ï¸ HCX ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨, ì›ë³¸ ë°˜í™˜: {e}")
            
        # í´ë°±: ê¸°ì¡´ ë¡œì§
        if content.strip().startswith('{') and content.strip().endswith('}'):
            try:
                data = json.loads(content)
                if isinstance(data, dict):
                    # textë¥¼ statementsë¡œ ë³€í™˜ (faithfulnessìš©)
                    if 'text' in data and metric_type == "faithfulness":
                        text = data['text']
                        statements = [s.strip() for s in re.split(r'[.ã€‚\n]+', text) if s.strip()]
                        return json.dumps({"statements": statements}, ensure_ascii=False)
                    return json.dumps(data, ensure_ascii=False)
            except json.JSONDecodeError:
                pass
        
        # ì¼ë°˜ í…ìŠ¤íŠ¸ ì‘ë‹µ ì •ë¦¬
        cleaned = content.strip()
        
        # ë§ˆí¬ë‹¤ìš´ í¬ë§·íŒ… ì œê±°
        cleaned = re.sub(r'```.*?```', '', cleaned, flags=re.DOTALL)
        cleaned = re.sub(r'`([^`]+)`', r'\1', cleaned)
        cleaned = re.sub(r'\*\*([^*]+)\*\*', r'\1', cleaned)
        cleaned = re.sub(r'\*([^*]+)\*', r'\1', cleaned)
        
        # ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬
        cleaned = re.sub(r'\s+', ' ', cleaned)
        cleaned = cleaned.strip()
        
        return cleaned
    
    def generate_answer(self, prompt: str, **kwargs) -> str:
        """ë™ê¸° í˜¸ì¶œ (ë¹„ë™ê¸°ë¥¼ ë™ê¸°ë¡œ ë˜í•‘)"""
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ê°€ ìˆìœ¼ë©´ executor ì‚¬ìš©
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(asyncio.run, self.agenerate_answer(prompt, **kwargs))
                    return future.result()
            else:
                return asyncio.run(self.agenerate_answer(prompt, **kwargs))
        except Exception as e:
            print(f"âŒ HCX ë™ê¸° í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return f"Error: {str(e)}"


class LLMAdapterWrapper(LLM):
    """LLM ì–´ëŒ‘í„° ë˜í¼ - ë©”ì¸ RAGTrace HcxLangChainCompat êµ¬ì¡° ì°¸ì¡°"""
    
    adapter: Any = None
    model: str = None
    
    def __init__(self, adapter, **kwargs):
        super().__init__(**kwargs)
        self.adapter = adapter
        self.model = adapter.model_name
    
    @property
    def _llm_type(self) -> str:
        return "ragtrace_lite_adapter"
    
    def set_run_config(self, run_config):
        """RAGAS RunConfig ì„¤ì • - ë¬´ì‹œ"""
        # ìì²´ ì„¤ì •ì„ ì‚¬ìš©í•˜ë¯€ë¡œ RunConfigëŠ” ë¬´ì‹œ
        pass
    
    def _call(self, prompt: str, stop: Optional[List[str]] = None,
              run_manager: Optional[CallbackManagerForLLMRun] = None, **kwargs: Any) -> str:
        """ë™ê¸° í˜¸ì¶œ"""
        return self.adapter.generate_answer(prompt, **kwargs)
    
    async def _acall(self, prompt: str, stop: Optional[List[str]] = None,
                     run_manager: Optional[AsyncCallbackManagerForLLMRun] = None, **kwargs: Any) -> str:
        """ë¹„ë™ê¸° í˜¸ì¶œ"""
        return await self.adapter.agenerate_answer(prompt, **kwargs)
    
    async def agenerate(self, prompts: List[str | StringPromptValue], **kwargs: Any):
        """RAGASê°€ ì›í•˜ëŠ” ë¹„ë™ê¸° generate ë©”ì„œë“œ"""
        results = []
        for prompt in prompts:
            prompt_str = prompt.text if hasattr(prompt, 'text') else str(prompt)
            result = await self.adapter.agenerate_answer(prompt_str, **kwargs)
            results.append(LLMResult(generations=[[Generation(text=result)]]))
        return LLMResult(generations=[gen.generations[0] for gen in results])
    
    def generate(self, prompts: List[str | StringPromptValue], **kwargs: Any):
        """RAGAS í˜¸í™˜ generate - ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ê°ì§€"""
        # RAGAS ë²„ê·¸ ìš°íšŒ: RAGASê°€ await llm.generate()ë¥¼ í˜¸ì¶œí•˜ë¯€ë¡œ
        # ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ì—ì„œëŠ” ì½”ë£¨í‹´ì„ ë°˜í™˜í•´ì•¼ í•¨
        try:
            # ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ í™•ì¸
            loop = asyncio.get_running_loop()
            # ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì‹¤í–‰ ì¤‘ì´ë©´ agenerate ë°˜í™˜
            return self.agenerate(prompts, **kwargs)
        except RuntimeError:
            # ë™ê¸° ì»¨í…ìŠ¤íŠ¸ì—ì„œëŠ” ì¼ë°˜ LLMResult ë°˜í™˜
            if not isinstance(prompts, list):
                prompts = [prompts]
            
            generations = []
            for prompt in prompts:
                # í”„ë¡¬í”„íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
                if hasattr(prompt, 'to_string'):
                    prompt_str = prompt.to_string()
                else:
                    prompt_str = str(prompt)
                
                # ì–´ëŒ‘í„° í˜¸ì¶œ
                response = self._call(prompt_str, **kwargs)
                
                # Generation ê°ì²´ ìƒì„±
                generation = Generation(text=response)
                generations.append([generation])
            
            return LLMResult(generations=generations)
    
    def agenerate(self, prompts: List[str | StringPromptValue], **kwargs: Any):
        """ë¹„ë™ê¸° generate - RAGAS í˜¸í™˜ (ì½”ë£¨í‹´ ë°˜í™˜)"""
        async def _agenerate():
            if not isinstance(prompts, list):
                prompts_list = [prompts]
            else:
                prompts_list = prompts
            
            generations = []
            
            # ëª¨ë“  í”„ë¡¬í”„íŠ¸ë¥¼ ë™ì‹œì— ì²˜ë¦¬
            tasks = []
            for prompt in prompts_list:
                if hasattr(prompt, 'to_string'):
                    prompt_str = prompt.to_string()
                else:
                    prompt_str = str(prompt)
                tasks.append(self._acall(prompt_str, **kwargs))
            
            responses = await asyncio.gather(*tasks)
            
            for response in responses:
                generation = Generation(text=response)
                generations.append([generation])
            
            return LLMResult(generations=generations)
        
        # ì½”ë£¨í‹´ ê°ì²´ ë°˜í™˜ (await ê°€ëŠ¥)
        return _agenerate()


def create_llm(config: Config) -> LLM:
    """RAGAS í˜¸í™˜ LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    provider = config.llm.provider.lower()
    
    try:
        if provider == 'gemini':
            if not config.llm.api_key:
                raise ValueError("Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            
            model_name = config.llm.model_name or "gemini-2.5-flash"
            adapter = GeminiAdapter(
                api_key=config.llm.api_key,
                model_name=model_name
            )
            return LLMAdapterWrapper(adapter)
            
        elif provider == 'hcx':
            if not config.llm.api_key:
                raise ValueError("HCX API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            
            model_name = config.llm.model_name or "HCX-005"
            adapter = HcxAdapter(
                api_key=config.llm.api_key,
                model_name=model_name
            )
            
            # HCXëŠ” í”„ë¡ì‹œë¡œ ê°ì‹¸ì„œ ë°˜í™˜
            from .hcx_proxy import HCXRAGASProxy
            base_llm = LLMAdapterWrapper(adapter)
            return HCXRAGASProxy(base_llm)
            
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM ì œê³µì: {provider}")
            
    except Exception as e:
        raise Exception(f"LLM ì´ˆê¸°í™” ì‹¤íŒ¨ ({provider}): {str(e)}")


async def test_llm_connection_async(llm: LLM, provider: str) -> bool:
    """LLM ë¹„ë™ê¸° ì—°ê²° í…ŒìŠ¤íŠ¸"""
    test_prompt = "Hello, this is a test. Please respond with 'OK'."
    
    try:
        print(f"ğŸ”„ {provider.upper()} LLM ë¹„ë™ê¸° ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘...")
        response = await llm._acall(test_prompt)
        print(f"âœ… {provider.upper()} LLM ì—°ê²° ì„±ê³µ")
        print(f"í…ŒìŠ¤íŠ¸ ì‘ë‹µ: {response[:100]}...")
        return True
        
    except Exception as e:
        print(f"âŒ {provider.upper()} LLM ì—°ê²° ì‹¤íŒ¨: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


def check_llm_connection(llm: LLM, provider: str) -> bool:
    """LLM ì—°ê²° í…ŒìŠ¤íŠ¸ (ë™ê¸° ë˜í¼)"""
    try:
        return asyncio.run(test_llm_connection_async(llm, provider))
    except Exception as e:
        print(f"âŒ {provider.upper()} LLM ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        return False


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    import asyncio
    from .config_loader import load_config
    
    async def test_main():
        try:
            config = load_config()
            print(f"ì„¤ì • ë¡œë“œ ì™„ë£Œ: {config.llm.provider}")
            
            llm = create_llm(config)
            print("LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì™„ë£Œ")
            
            success = await test_llm_connection_async(llm, config.llm.provider)
            
            if success:
                print("âœ… LLM Factory ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ ì„±ê³µ")
            else:
                print("âŒ LLM Factory ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
                
        except Exception as e:
            print(f"âŒ LLM Factory í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
    
    asyncio.run(test_main())