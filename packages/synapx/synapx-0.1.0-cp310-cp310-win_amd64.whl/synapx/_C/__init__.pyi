"""
Synapx C++ bindings
"""
from __future__ import annotations
import torch
import numpy
import typing
from . import _nn
__all__ = ['LogLevel', 'NoGradContext', 'Node', 'Tensor', 'TensorDataContainer', 'add', 'addmm', 'clone', 'concat', 'device', 'dims', 'div', 'dropout', 'dtype', 'empty', 'empty_like', 'exp', 'flatten', 'full', 'full_like', 'get_log_level', 'is_detailed_repr_enabled', 'is_grad_enabled', 'log', 'log_softmax', 'matmul', 'max', 'mean', 'min', 'movedim', 'mul', 'neg', 'no_grad', 'ones', 'ones_like', 'pow', 'rand', 'rand_like', 'randn', 'randn_like', 'relu', 'reshape', 'set_detailed_repr', 'set_grad_enabled', 'set_log_level', 'sigmoid', 'softmax', 'sqrt', 'squeeze', 'stack', 'sub', 'sum', 'swapdims', 'tensor', 'transpose', 'unbind', 'unsqueeze', 'zeros', 'zeros_like']
class LogLevel:
    """
    Members:
    
      DEBUG
    
      INFO
    
      WARNING
    
      ERROR
    
      NONE
    """
    DEBUG: typing.ClassVar[LogLevel]  # value = <LogLevel.DEBUG: 0>
    ERROR: typing.ClassVar[LogLevel]  # value = <LogLevel.ERROR: 3>
    INFO: typing.ClassVar[LogLevel]  # value = <LogLevel.INFO: 1>
    NONE: typing.ClassVar[LogLevel]  # value = <LogLevel.NONE: 4>
    WARNING: typing.ClassVar[LogLevel]  # value = <LogLevel.WARNING: 2>
    __members__: typing.ClassVar[dict[str, LogLevel]]  # value = {'DEBUG': <LogLevel.DEBUG: 0>, 'INFO': <LogLevel.INFO: 1>, 'WARNING': <LogLevel.WARNING: 2>, 'ERROR': <LogLevel.ERROR: 3>, 'NONE': <LogLevel.NONE: 4>}
    def __eq__(self, other: typing.Any) -> bool:
        ...
    def __getstate__(self) -> int:
        ...
    def __hash__(self) -> int:
        ...
    def __index__(self) -> int:
        ...
    def __init__(self, value: int) -> None:
        ...
    def __int__(self) -> int:
        ...
    def __ne__(self, other: typing.Any) -> bool:
        ...
    def __repr__(self) -> str:
        ...
    def __setstate__(self, state: int) -> None:
        ...
    def __str__(self) -> str:
        ...
    @property
    def name(self) -> str:
        ...
    @property
    def value(self) -> int:
        ...
class NoGradContext:
    def __enter__(self) -> None:
        ...
    def __exit__(self, arg0: typing.Any, arg1: typing.Any, arg2: typing.Any) -> None:
        ...
    def __init__(self) -> None:
        ...
class Node:
    def name(self) -> str:
        ...
class Tensor:
    __hash__: typing.ClassVar[None] = None
    @typing.overload
    def __add__(self, other: Tensor) -> Tensor:
        ...
    @typing.overload
    def __add__(self, other: float) -> Tensor:
        ...
    @typing.overload
    def __eq__(self, arg0: Tensor) -> Tensor:
        ...
    @typing.overload
    def __eq__(self, arg0: float) -> Tensor:
        ...
    @typing.overload
    def __ge__(self, arg0: Tensor) -> Tensor:
        ...
    @typing.overload
    def __ge__(self, arg0: float) -> Tensor:
        ...
    @typing.overload
    def __getitem__(self, idx: int) -> Tensor:
        """
        Simple indexing
        """
    @typing.overload
    def __getitem__(self, slice_: typing.Any) -> Tensor:
        """
        Index tensor using Python notation
        """
    @typing.overload
    def __gt__(self, arg0: Tensor) -> Tensor:
        ...
    @typing.overload
    def __gt__(self, arg0: float) -> Tensor:
        ...
    @typing.overload
    def __iadd__(self, other: Tensor) -> Tensor:
        ...
    @typing.overload
    def __iadd__(self, other: float) -> Tensor:
        ...
    @typing.overload
    def __imul__(self, other: Tensor) -> Tensor:
        ...
    @typing.overload
    def __imul__(self, other: float) -> Tensor:
        ...
    def __init__(self, arg0: Tensor) -> None:
        ...
    @typing.overload
    def __isub__(self, other: Tensor) -> Tensor:
        ...
    @typing.overload
    def __isub__(self, other: float) -> Tensor:
        ...
    def __iter__(self) -> typing.Iterator:
        ...
    @typing.overload
    def __itruediv__(self, other: Tensor) -> Tensor:
        ...
    @typing.overload
    def __itruediv__(self, other: float) -> Tensor:
        ...
    @typing.overload
    def __le__(self, arg0: Tensor) -> Tensor:
        ...
    @typing.overload
    def __le__(self, arg0: float) -> Tensor:
        ...
    def __len__(self) -> int:
        ...
    @typing.overload
    def __lt__(self, arg0: Tensor) -> Tensor:
        ...
    @typing.overload
    def __lt__(self, arg0: float) -> Tensor:
        ...
    def __matmul__(self, other: Tensor) -> Tensor:
        ...
    @typing.overload
    def __mul__(self, other: Tensor) -> Tensor:
        ...
    @typing.overload
    def __mul__(self, other: float) -> Tensor:
        ...
    @typing.overload
    def __ne__(self, arg0: Tensor) -> Tensor:
        ...
    @typing.overload
    def __ne__(self, arg0: float) -> Tensor:
        ...
    def __neg__(self) -> Tensor:
        ...
    @typing.overload
    def __pow__(self, exponent: Tensor) -> Tensor:
        ...
    @typing.overload
    def __pow__(self, exponent: float) -> Tensor:
        ...
    @typing.overload
    def __radd__(self, other: Tensor) -> Tensor:
        ...
    @typing.overload
    def __radd__(self, other: float) -> Tensor:
        ...
    def __repr__(self) -> str:
        ...
    def __rmatmul__(self, other: Tensor) -> Tensor:
        ...
    @typing.overload
    def __rmul__(self, other: Tensor) -> Tensor:
        ...
    @typing.overload
    def __rmul__(self, other: float) -> Tensor:
        ...
    @typing.overload
    def __rpow__(self, base: Tensor) -> Tensor:
        ...
    @typing.overload
    def __rpow__(self, base: float) -> Tensor:
        ...
    @typing.overload
    def __rsub__(self, other: Tensor) -> Tensor:
        ...
    @typing.overload
    def __rsub__(self, other: float) -> Tensor:
        ...
    @typing.overload
    def __rtruediv__(self, other: Tensor) -> Tensor:
        ...
    @typing.overload
    def __rtruediv__(self, other: float) -> Tensor:
        ...
    @typing.overload
    def __setitem__(self, indices: typing.Any, value: Tensor) -> None:
        ...
    @typing.overload
    def __setitem__(self, indices: typing.Any, value: float) -> None:
        ...
    @typing.overload
    def __sub__(self, other: Tensor) -> Tensor:
        ...
    @typing.overload
    def __sub__(self, other: float) -> Tensor:
        ...
    @typing.overload
    def __truediv__(self, other: Tensor) -> Tensor:
        ...
    @typing.overload
    def __truediv__(self, other: float) -> Tensor:
        ...
    @typing.overload
    def add(self, other: Tensor) -> Tensor:
        ...
    @typing.overload
    def add(self, scalar: float) -> Tensor:
        ...
    @typing.overload
    def add_(self, other: Tensor) -> Tensor:
        ...
    @typing.overload
    def add_(self, other: float) -> Tensor:
        ...
    def argmax(self, dim: int | None = None, keepdim: bool = False) -> Tensor:
        ...
    def argmin(self, dim: int | None = None, keepdim: bool = False) -> Tensor:
        ...
    def backward(self, grad: typing.Any = None) -> None:
        """
        Union[None, synapx.Tensor, torch.Tensor]: Computes the gradient of current tensor w.r.t. graph leaves.
        """
    def clone(self) -> Tensor:
        ...
    def copy_(self, src: Tensor) -> Tensor:
        ...
    def count_nonzero(self) -> Tensor:
        ...
    def cpu(self) -> Tensor:
        ...
    def cuda(self, index: int = 0) -> Tensor:
        ...
    def detach(self) -> Tensor:
        ...
    def dim(self) -> int:
        ...
    @typing.overload
    def div(self, other: Tensor) -> Tensor:
        ...
    @typing.overload
    def div(self, other: float) -> Tensor:
        ...
    @typing.overload
    def div_(self, other: Tensor) -> Tensor:
        ...
    @typing.overload
    def div_(self, other: float) -> Tensor:
        ...
    def exp(self) -> Tensor:
        ...
    def fill_(self, value: float) -> Tensor:
        ...
    def flatten(self, start_dim: int = 0, end_dim: int = -1) -> Tensor:
        ...
    def is_floating_point(self) -> bool:
        ...
    def item(self) -> typing.Any:
        ...
    def log(self) -> Tensor:
        ...
    def log_softmax(self, dim: int) -> Tensor:
        ...
    def matmul(self, other: Tensor) -> Tensor:
        ...
    @typing.overload
    def max(self) -> Tensor:
        ...
    @typing.overload
    def max(self, dim: int, keepdim: bool = False) -> tuple[Tensor, Tensor]:
        ...
    def mean(self, dim: dims = None, keepdim: bool = False) -> Tensor:
        ...
    @typing.overload
    def min(self) -> Tensor:
        ...
    @typing.overload
    def min(self, dim: int, keepdim: bool = False) -> tuple[Tensor, Tensor]:
        ...
    def movedim(self, source: int, destination: int) -> Tensor:
        ...
    @typing.overload
    def mul(self, other: Tensor) -> Tensor:
        ...
    @typing.overload
    def mul(self, other: float) -> Tensor:
        ...
    @typing.overload
    def mul_(self, other: Tensor) -> Tensor:
        ...
    @typing.overload
    def mul_(self, other: float) -> Tensor:
        ...
    def neg(self) -> Tensor:
        ...
    def neg_(self) -> Tensor:
        ...
    def normal_(self, mean: float = 0, std: float = 1) -> Tensor:
        ...
    def numel(self) -> int:
        ...
    def numpy(self) -> numpy.ndarray:
        ...
    @typing.overload
    def pow(self, exponent: Tensor) -> Tensor:
        ...
    @typing.overload
    def pow(self, exponent: float) -> Tensor:
        ...
    @typing.overload
    def pow_(self, other: Tensor) -> Tensor:
        ...
    @typing.overload
    def pow_(self, other: float) -> Tensor:
        ...
    def relu(self) -> Tensor:
        ...
    def requires_grad_(self, arg0: bool) -> Tensor:
        ...
    def reshape(self, shape: tuple[int, ...]) -> Tensor:
        ...
    def retain_grad(self) -> None:
        ...
    def sigmoid(self) -> Tensor:
        ...
    def softmax(self, dim: int) -> Tensor:
        ...
    def sqrt(self) -> Tensor:
        ...
    def squeeze(self, dim: dims = None) -> Tensor:
        ...
    @typing.overload
    def sub(self, other: Tensor) -> Tensor:
        ...
    @typing.overload
    def sub(self, other: float) -> Tensor:
        ...
    @typing.overload
    def sub_(self, other: Tensor) -> Tensor:
        ...
    @typing.overload
    def sub_(self, other: float) -> Tensor:
        ...
    def sum(self, dim: dims = None, keepdim: bool = False) -> Tensor:
        ...
    def swapdims(self, dim0: int, dim1: int) -> Tensor:
        ...
    def t(self) -> Tensor:
        ...
    @typing.overload
    def to(self, device: torch.device) -> Tensor:
        ...
    @typing.overload
    def to(self, dtype: torch.dtype) -> Tensor:
        ...
    @typing.overload
    def to_(self, device: torch.device) -> Tensor:
        ...
    @typing.overload
    def to_(self, dtype: torch.dtype) -> Tensor:
        ...
    def torch(self) -> torch.Tensor:
        ...
    def transpose(self, dim0: int, dim1: int) -> Tensor:
        ...
    def uniform_(self, from_: float = 0, to: float = 1) -> Tensor:
        ...
    def unsqueeze(self, dim: int) -> Tensor:
        ...
    def zero_(self) -> Tensor:
        ...
    @property
    def data(self) -> torch.Tensor:
        ...
    @property
    def device(self) -> torch.device:
        """
        Underlying torch device
        """
    @property
    def dtype(self) -> torch.dtype:
        """
        Underlying torch dtype
        """
    @property
    def grad(self) -> Tensor | None:
        """
        Gradient tensor or None
        """
    @property
    def grad_fn(self) -> Node | None:
        """
        Backward function
        """
    @property
    def is_leaf(self) -> bool:
        ...
    @property
    def requires_grad(self) -> bool:
        ...
    @property
    def retains_grad(self) -> bool:
        ...
    @property
    def shape(self) -> tuple:
        ...
class TensorDataContainer:
    def __init__(self, arg0: typing.Any) -> None:
        ...
    def torch(self) -> torch.Tensor:
        ...
class device:
    @typing.overload
    def __init__(self) -> None:
        ...
    @typing.overload
    def __init__(self, arg0: str) -> None:
        ...
    @typing.overload
    def __init__(self, arg0: torch.device) -> None:
        ...
    @typing.overload
    def __init__(self, arg0: typing.Any) -> None:
        ...
    def __repr__(self) -> str:
        ...
    def __str__(self) -> str:
        ...
    def torch(self) -> torch.device:
        ...
class dims:
    def __init__(self, arg0: typing.Any) -> None:
        ...
    def tolist(self) -> list[int]:
        ...
class dtype:
    @typing.overload
    def __init__(self: torch.dtype) -> None:
        ...
    @typing.overload
    def __init__(self: torch.dtype, arg0: torch.dtype) -> None:
        ...
    @typing.overload
    def __init__(self: torch.dtype, arg0: typing.Any) -> None:
        ...
    def torch(self: torch.dtype) -> torch.dtype:
        ...
@typing.overload
def add(t1: Tensor, t2: Tensor) -> Tensor:
    ...
@typing.overload
def add(tensor: Tensor, scalar: float) -> Tensor:
    ...
def addmm(inp: Tensor, mat1: Tensor, mat2: Tensor) -> Tensor:
    ...
def clone(tensor: Tensor) -> Tensor:
    ...
def concat(tensor: list[Tensor], dim: int = 0) -> Tensor:
    ...
@typing.overload
def div(t1: Tensor, t2: Tensor) -> Tensor:
    ...
@typing.overload
def div(tensor: Tensor, scalar: float) -> Tensor:
    ...
def dropout(tensor: Tensor, p: float, train: bool) -> Tensor:
    ...
def empty(shape: dims, requires_grad: bool = False, device: torch.device = None, dtype: torch.dtype = None) -> Tensor:
    ...
def empty_like(input: Tensor, requires_grad: bool = False, device: torch.device = None, dtype: torch.dtype = None) -> Tensor:
    ...
def exp(tensor: Tensor) -> Tensor:
    ...
def flatten(tensor: Tensor, start_dim: int = 0, end_dim: int = -1) -> Tensor:
    ...
def full(shape: dims, fill_value: float, requires_grad: bool = False, device: torch.device = None, dtype: torch.dtype = None) -> Tensor:
    ...
def full_like(input: Tensor, fill_value: float, requires_grad: bool = False, device: torch.device = None, dtype: torch.dtype = None) -> Tensor:
    ...
def get_log_level() -> LogLevel:
    """
    Get the current logging level
    """
def is_detailed_repr_enabled() -> bool:
    """
    Check if detailed tensor representation is enabled
    """
def is_grad_enabled() -> bool:
    """
    Check if gradients are enabled
    """
def log(tensor: Tensor) -> Tensor:
    ...
def log_softmax(tensor: Tensor, dim: int) -> Tensor:
    ...
def matmul(t1: Tensor, t2: Tensor) -> Tensor:
    ...
@typing.overload
def max(tensor: Tensor) -> Tensor:
    ...
@typing.overload
def max(tensor: Tensor, dim: int, keepdim: bool = False) -> tuple[Tensor, Tensor]:
    ...
def mean(tensor: Tensor, dim: dims = None, keepdim: bool = False) -> Tensor:
    ...
@typing.overload
def min(tensor: Tensor) -> Tensor:
    ...
@typing.overload
def min(tensor: Tensor, dim: int, keepdim: bool = False) -> tuple[Tensor, Tensor]:
    ...
def movedim(tensor: Tensor, source: int, destination: int) -> Tensor:
    ...
@typing.overload
def mul(t1: Tensor, t2: Tensor) -> Tensor:
    ...
@typing.overload
def mul(tensor: Tensor, scalar: float) -> Tensor:
    ...
def neg(arg0: Tensor) -> Tensor:
    ...
def no_grad() -> NoGradContext:
    """
    Create a context manager that disables gradient computation
    """
def ones(shape: dims, requires_grad: bool = False, device: torch.device = None, dtype: torch.dtype = None) -> Tensor:
    ...
def ones_like(input: Tensor, requires_grad: bool = False, device: torch.device = None, dtype: torch.dtype = None) -> Tensor:
    ...
@typing.overload
def pow(tensor: Tensor, exponent: Tensor) -> Tensor:
    ...
@typing.overload
def pow(tensor: Tensor, exponent: float) -> Tensor:
    ...
def rand(shape: dims, requires_grad: bool = False, device: torch.device = None, dtype: torch.dtype = None) -> Tensor:
    ...
def rand_like(input: Tensor, requires_grad: bool = False, device: torch.device = None, dtype: torch.dtype = None) -> Tensor:
    ...
def randn(shape: dims, requires_grad: bool = False, device: torch.device = None, dtype: torch.dtype = None) -> Tensor:
    ...
def randn_like(input: Tensor, requires_grad: bool = False, device: torch.device = None, dtype: torch.dtype = None) -> Tensor:
    ...
def relu(tensor: Tensor) -> Tensor:
    ...
def reshape(tensor: Tensor, shape: tuple[int, ...]) -> Tensor:
    ...
def set_detailed_repr(flag: bool) -> None:
    """
    Enable or disable detailed tensor representation
    """
def set_grad_enabled(arg0: bool) -> None:
    """
    Set gradient enabled state
    """
def set_log_level(level: LogLevel) -> None:
    """
    Set the logging level
    """
def sigmoid(tensor: Tensor) -> Tensor:
    ...
def softmax(tensor: Tensor, dim: int) -> Tensor:
    ...
def sqrt(tensor: Tensor) -> Tensor:
    ...
def squeeze(tensor: Tensor, dim: dims = None) -> Tensor:
    ...
def stack(tensor: list[Tensor], dim: int = 0) -> Tensor:
    ...
@typing.overload
def sub(t1: Tensor, t2: Tensor) -> Tensor:
    ...
@typing.overload
def sub(tensor: Tensor, scalar: float) -> Tensor:
    ...
def sum(tensor: Tensor, dim: dims = None, keepdim: bool = False) -> Tensor:
    ...
def swapdims(tensor: Tensor, dim0: int, dim1: int) -> Tensor:
    ...
def tensor(data: TensorDataContainer, requires_grad: bool = False, device: torch.device = None, dtype: torch.dtype = None) -> Tensor:
    ...
def transpose(tensor: Tensor, dim0: int, dim1: int) -> Tensor:
    ...
def unbind(tensor: Tensor, dim: int = 0) -> list[Tensor]:
    ...
def unsqueeze(tensor: Tensor, dim: int) -> Tensor:
    ...
def zeros(shape: dims, requires_grad: bool = False, device: torch.device = None, dtype: torch.dtype = None) -> Tensor:
    ...
def zeros_like(input: Tensor, requires_grad: bool = False, device: torch.device = None, dtype: torch.dtype = None) -> Tensor:
    ...
