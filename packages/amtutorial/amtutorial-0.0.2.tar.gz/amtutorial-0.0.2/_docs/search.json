[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Associative Memory Tutorial",
    "section": "",
    "text": "Associative Memory Tutorial\n\nA complete introduction to Associative Memories and Hopfield Networks\n\n\n\n\n\n\n\nüöß Under construction\n\n\n\nThis website and it‚Äôs accompanying notebooks are under construction until July 14, 2025, when this tutorial will be presented at ICML 2025.\n\n\nSee the tutorials for a brief introduction to the list of example notebooks.",
    "crumbs": [
      "Associative Memory Tutorial"
    ]
  },
  {
    "objectID": "lib/index.html",
    "href": "lib/index.html",
    "title": "Reusable utils",
    "section": "",
    "text": "To aid in the pedagogy of the tutorial, a lot of logic (e.g., data processing, boilerplate training code, etc.) has been ported to this small library of reusable utilities.\nAll notebooks are designed to be run using CPU only. Requirements can be installed with:\npip install amtutorial",
    "crumbs": [
      "lib"
    ]
  },
  {
    "objectID": "tutorial/index.html",
    "href": "tutorial/index.html",
    "title": "Tutorials",
    "section": "",
    "text": "Dense Storage in binary networks. A whirlwind tour of the Classical Hopfield Network to Dense Associative Memory.\nEnergy Transformer. Rederiving the Transformer as a pure associative memory\nDiffusion as Memory.\nDistributed memory. A way to encode dense storage capacity in constant parameters.\n\n\n\n\n\n\n\nNote\n\n\n\nThese tutorials are under construction until July 14, 2025, when this tutorial will be presented at ICML 2025.",
    "crumbs": [
      "tutorial"
    ]
  },
  {
    "objectID": "tutorial/diffusion_as_memory.html",
    "href": "tutorial/diffusion_as_memory.html",
    "title": "Memory and Diffusion",
    "section": "",
    "text": "üöß Under construction\n\n\n\nThis notebook is under construction. It will be completed by July 14, 2025.\nThis notebook is a simplified, step-by-step walkthrough of the 2D toy example from the paper: ‚ÄúMemorization to Generalization: Emergence of Diffusion Models from Associative Memory‚Äù.\nWe will train a score-based diffusion model on a small dataset of points lying on a circle. Our goal is to understand how the model learns the data distribution and to visualize its learned ‚Äúenergy landscape,‚Äù which reveals how it behaves like an Associative Memory system initially to later transition into a generative model.\nFor more details, please read the paper and the code repository.\nImports and Setup\n# --- Essential Libraries ---\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.notebook import tqdm\nfrom itertools import cycle\nimport os\nfrom IPython.display import display\nimport PIL\nfrom pathlib import Path\n\nfrom copy import deepcopy\n\n# --- SciPy for specific math functions ---\nfrom scipy.special import i0, i1 # Modified Bessel functions for analytical energy\nimport scipy.integrate as integrate # For ODE solving (likelihood calculation)\n\n# --- Scikit-learn for Clustering ---\nfrom sklearn.cluster import AgglomerativeClustering\n\n# Set a nice plot style\nsns.set_theme(style=\"whitegrid\")\n\n# turn off warnings\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom collections import OrderedDict\n\n# For caching trained models\nCACHE_DIR = Path(\"./cache/02_diffusion_as_memory\")\nCACHE_DIR.mkdir(parents=True, exist_ok=True)\nCACHE_MODELS = True",
    "crumbs": [
      "tutorial",
      "Memory and Diffusion"
    ]
  },
  {
    "objectID": "tutorial/diffusion_as_memory.html#synthetic-data",
    "href": "tutorial/diffusion_as_memory.html#synthetic-data",
    "title": "Memory and Diffusion",
    "section": "Synthetic Data",
    "text": "Synthetic Data\nThe paper uses a simple dataset: points sampled from the circumference of a unit circle. This helps us easily visualize how the model learns.\nWe‚Äôll define a function to generate these points and a PyTorch Dataset class to handle them.\n\ndef generate_circle_data(num_samples=50000, radius=1, seed=59):\n    \"\"\"Generates data points that lie on a unit circle.\"\"\"\n    np.random.seed(seed)\n    # Sample angles uniformly from 0 to 2*pi\n    angles = np.random.uniform(0, 2 * np.pi, num_samples)\n\n    # Convert polar coordinates (angles, radius) to Cartesian (x, y)\n    x = radius * np.cos(angles)\n    y = radius * np.sin(angles)\n    return np.stack([x, y], axis=1)\n\nclass CircleDataset(Dataset):\n    \"\"\"A PyTorch Dataset to wrap our circle data.\"\"\"\n    def __init__(self, num_samples=50000, radius=1, seed=9):\n        # Generate and store the data as a torch tensor\n        self.data = torch.from_numpy(generate_circle_data(num_samples, radius, seed)).float()\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n\ndef create_subset(dataset, sample_size, seed=42):\n    \"\"\"Create a subset of the dataset based on the specified sample size. \"\"\"\n    max_size = len(dataset)\n    generator = torch.Generator().manual_seed(seed)\n    if not 1 &lt;= sample_size &lt;= len(dataset):\n        raise ValueError(\"Sample size must be between 1 and the size of the dataset inclusive.\")\n    subset, _ = torch.utils.data.random_split(\n        dataset, [sample_size, max_size - sample_size], generator=generator\n    )\n    return subset\n\ndef prepare_datasets(sample_size, train_size=60000, test_size=10000, seed=9):\n    dataset = CircleDataset(num_samples=train_size, seed=seed)\n    test_dataset = CircleDataset(num_samples=test_size, seed=seed)\n    train_subset = create_subset(dataset, sample_size)\n    test_subset = create_subset(test_dataset, sample_size)\n    return train_subset, test_subset\n\nDiffusion models can learn from very few samples. In the paper, this is referred to as memorizing ‚Äúpatterns‚Äù. Let‚Äôs create a tiny dataset with just 9 data points (patterns) to train on.\n\n# --- Configuration ---\nSAMPLE_SIZE = 9 # The number of data points (patterns) to memorize\nBATCH_SIZE = min(500, SAMPLE_SIZE)  # Use all data points in each batch\nSEED = 9       # For reproducibility\n\n# Create the full dataset\ntrain_subset, _ = prepare_datasets(SAMPLE_SIZE, seed=SEED)\n\n# Create a DataLoader\ntrain_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True)\n\n# Extract the training data points for visualization\npatterns = train_subset.dataset[train_subset.indices]\n\nLet‚Äôs plot our small dataset. These are the specific points we want our model to learn and remember.",
    "crumbs": [
      "tutorial",
      "Memory and Diffusion"
    ]
  },
  {
    "objectID": "tutorial/diffusion_as_memory.html#the-diffusion-architecture",
    "href": "tutorial/diffusion_as_memory.html#the-diffusion-architecture",
    "title": "Memory and Diffusion",
    "section": "The Diffusion Architecture",
    "text": "The Diffusion Architecture\nThe diffusion model is simply a model \\(s_\\theta(\\mathbf{x}_t, t)\\) which approximates the score function: \\[\ns_\\theta(\\mathbf{x}_t, t) ‚âà \\nabla_{\\mathbf{x}_t} \\log p_t (\\mathbf{x}_t)\n\\] over a series of timesteps.\nIn this tutorial, we will be using Variance Exploding (VE) SDE, which defines how data is gradually noised over time ranging from \\(t \\in [\\epsilon, 1]\\): \\[\n  \\mathrm{d} \\mathbf{x}_t = \\sigma  \\mathrm{d} \\mathbf{w}_t\n\\] and the corresponding reverse process: \\[\n  \\mathrm{d} \\mathbf{x}_t = \\big [ -\\sigma^2 \\nabla_{\\mathbf{x}_t} \\log p_t (\\mathbf{x}_t) \\big ] \\mathrm{d}t + \\sigma^2 \\mathrm{d} \\mathbf{w}_t\n\\] where \\(g(t) = \\sigma\\) is the diffusion coefficient and \\(\\mathbf{w}_t\\) is brownian motion.\n\nclass VESDETerms:\n    \"\"\"Defines the terms for the Variance Exploding SDE.\"\"\"\n    def __init__(self, sigma_max, device=None):\n        self.sigma = sigma_max\n        self.device = device\n\n    def marginal_prob_std(self, t):\n        t = torch.as_tensor(t, device=self.device, dtype=torch.float32)\n        return self.sigma * torch.sqrt(t)\n\n    def diffusion_coeff(self, t):\n        t = torch.as_tensor(t, device=self.device, dtype=torch.float32)\n        return self.sigma * torch.ones_like(t)\n\nOur score network is a simple Multi-Layer Perceptron (MLP). It takes a noisy data point x and a time step t as inputs, and returns the estimated score. The conditioning on time step t is performed via the Fourier embedding, a standard method of time conditioning in diffusion models.\n\n@torch.no_grad()\ndef update_ema(ema_model, model, decay=0.9999):\n    \"\"\"\n    Step the EMA model towards the current model.\n    \"\"\"\n    ema_params = OrderedDict(ema_model.named_parameters())\n    model_params = OrderedDict(model.named_parameters())\n\n    for name, param in model_params.items():\n        if param.requires_grad == True:\n            ema_params[name].mul_(decay).add_(param.data, alpha=1. - decay)\n\nclass FourierEmbedding(torch.nn.Module):\n    \"\"\"Embeds time `t` into a high-dimensional feature space.\"\"\"\n    def __init__(self, embed_dim, scale=16):\n        super().__init__()\n        self.register_buffer('freqs', torch.randn(embed_dim // 2) * scale)\n\n\n    def forward(self, x):\n        x = x.ger((2. * torch.pi * self.freqs).to(x.dtype))\n        x = torch.cat([x.cos(), x.sin()], dim=1)\n        return x\n\nclass ScoreNet(nn.Module):\n    \"\"\"The score-based model (a simple MLP).\"\"\"\n    def __init__(\n        self,\n        input_dim=2,\n        num_layers=4,\n        hidden_dim=128,\n        embed_dim=128,\n        marginal_prob_std=None\n    ):\n        super().__init__()\n\n        self.act = nn.SiLU()\n        self.marginal_prob_std = marginal_prob_std\n\n        # Time embedding\n        self.time_embed = nn.Sequential(\n            FourierEmbedding(embed_dim=embed_dim),\n            nn.Linear(embed_dim, embed_dim),\n            nn.SiLU(),\n            nn.Linear(embed_dim, embed_dim),\n            nn.SiLU()\n        )\n\n        # Project combined (x + time-embedding) to hidden dimension\n        self.input_proj = nn.Linear(input_dim + embed_dim, hidden_dim)\n\n        # Hidden MLP layers\n        layers = []\n        for _ in range(num_layers - 1):\n            layers.append(nn.Linear(hidden_dim, hidden_dim))\n\n            if _ == num_layers - 2:\n                layers.append(nn.LayerNorm(hidden_dim))\n                layers.append(nn.SiLU())\n            else:\n                layers.append(nn.SiLU())\n\n        self.hidden = nn.Sequential(*layers)\n\n        # Final output to 2 dimensions\n        self.output = nn.Linear(hidden_dim, 2)\n\n    def forward(self, x, t):\n        # Generate time embedding and concatenate with x\n        t_emb = self.time_embed(t)\n        h = torch.cat([x, t_emb], dim=1)\n\n        # Pass through MLP\n        h = self.input_proj(h)\n        h = self.hidden(h)\n        h = self.output(h)\n\n        # Scale by 1 / marginal_prob_std(t)\n        return h / self.marginal_prob_std(t)[:, None]",
    "crumbs": [
      "tutorial",
      "Memory and Diffusion"
    ]
  },
  {
    "objectID": "tutorial/diffusion_as_memory.html#training",
    "href": "tutorial/diffusion_as_memory.html#training",
    "title": "Memory and Diffusion",
    "section": "Training",
    "text": "Training\nWe use the denoising score matching (DSM) loss. The goal is to train the ScoreNet model so that its output, the score, matches the direction of the noise z that was added to the clean data x at each time step t. \\[\n\\mathcal{L} = \\mathbb{E}_{\\mathbf{x}_0, \\mathbf{x}_t, t} \\, \\bigg  [ \\lambda(t) \\lVert s_\\theta (\\mathbf{x}_t, t) -  \\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x}_t | \\mathbf{x}_0) \\rVert^2 \\bigg ]\n\\] where \\(\\mathbf{x}_0\\) denotes the clean data point and \\(\\mathbf{x}_t\\) is the perturbed data point.\nFor example, assume \\(\\tilde{\\mathbf{x}} \\sim \\mathcal{N} (\\tilde{\\mathbf{x}} | \\mathbf{x}, \\sigma^2 \\mathbf{I})\\) for the simple case of DSM. We have the following: \\[\n\\nabla_{\\tilde{\\mathbf{x}}} \\log p(\\tilde{\\mathbf{x}} | \\mathbf{x}) = \\nabla_\\tilde{\\mathbf{x}} \\bigg ( -\\frac{1}{2\\sigma^2} (\\tilde{\\mathbf{x}} - \\mathbf{x})^2 \\bigg ) = -\\frac{\\tilde{\\mathbf{x}} - \\mathbf{x}}{\\sigma^2} = -\\frac{\\epsilon}{\\sigma}\n\\] as the score function, which we have to learn for a single timestep of denoising. In the case of diffusion, our mean \\(\\mu_\\theta (\\mathbf{x}_t, t)\\) is conditioned by time, where \\(\\mu_\\theta (\\mathbf{x}_0, 0) = \\mathbf{x}\\) at t = 0 when we are back at the data distribution.\n\ndef loss_fn(model, x, marginal_prob_std, eps=1e-5):\n    \"\"\"The denoising score matching loss function.\"\"\"\n    # Sample a random time t\n    random_t = torch.rand(x.shape[0], device=x.device) * (1. - eps) + eps\n\n    # Sample a random noise vector\n    z = torch.randn_like(x)\n    std = marginal_prob_std(random_t)[:, None]\n\n    # Create the noisy data point\n    perturbed_x = x + z * std\n\n    # Get the model's score prediction\n    score = model(perturbed_x, random_t)\n\n    # Calculate the loss\n    #loss = torch.mean(torch.sum((score * std + z)**2, dim=1))\n    loss = torch.mean(torch.square(score * std + z))\n    return loss\n\n\ndef train_loop(train_loader, vesde, iterations=100_000, lr=1e-4, device='cuda', log_freq=10_000):\n    # create our score model\n    score_model = ScoreNet(\n        marginal_prob_std=vesde.marginal_prob_std\n    )\n\n    # create an exponential moving average version of the model\n    ema = deepcopy(score_model).to(device)\n    score_model = score_model.to(device)\n\n    optimizer = torch.optim.Adam(score_model.parameters(), lr=lr)\n\n    # Use an infinite data loader to cycle through our small dataset\n    infinite_loader = iter(cycle(train_loader))\n\n    # --- Training ---\n    score_model.train()\n    running_loss = 0.\n    pbar = tqdm(range(iterations))\n    for iteration in pbar:\n        # Get a batch of data\n        x = next(infinite_loader).to(device)\n\n        # Calculate loss\n        loss = loss_fn(score_model, x, vesde.marginal_prob_std)\n        running_loss += loss.item()\n\n        # Optimize\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        update_ema(ema, score_model) # udpate ema\n\n        # Log progress\n        if iteration % log_freq == 0 and iteration &gt; 0:\n            pbar.set_description(f\"Loss: {running_loss / log_freq:.4f}\")\n            running_loss = 0.\n\n    # return the exponential moving average model\n    ema.eval()\n    return ema\n\nTime to train! The following code takes a few minutes to run, but the results are cached after the first run.\n\n# Train our SDE-based diffusion models for training data sizes: 2, 9, and 1000.\nSEED = 9       # For reproducibility\nLR = 1e-4\nSIGMA_MAX = 1.\nITERATIONS = 50_000\n\n# Instantiate the SDE and the Model\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\" \nvesde = VESDETerms(sigma_max=SIGMA_MAX, device=device)\n\nCACHE_MODELS = True\n\ndef make_cache_name(sample_size, seed, lr, sigma_max, iterations):\n    return f\"ema_model_{sample_size}_{seed}_{lr}_{sigma_max}_{iterations}.pth\"\n\ndef get_ema_model(cache_name):\n    cache_path = CACHE_DIR / cache_name\n    if cache_path.exists():\n        model = ScoreNet(marginal_prob_std=vesde.marginal_prob_std)\n        state_dict = torch.load(cache_path)\n        model.load_state_dict(state_dict)\n        return model\n    else:\n        return None # Will need to train it\n\ndata_sizes = [2, 9, 1000] # Takes ~5 min on an M1 Pro CPU\nema_set, pattern_set = [], [] # store our ema models and training patterns into two separate lists\n\nfor sample_size in data_sizes:\n    ema = None        \n    cache_name = make_cache_name(sample_size, SEED, LR, SIGMA_MAX, ITERATIONS)\n    if CACHE_MODELS:\n        ema = get_ema_model(cache_name)\n    if ema is None or not CACHE_MODELS: \n        # Train the model if no cache exists\n        batch_size = min(500, sample_size)  # Use all data points in each batch\n        train_subset, _ = prepare_datasets(sample_size, seed=SEED)\n        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n\n        # ensure the initialization of our model remains the same\n        torch.manual_seed(SEED)\n        ema = train_loop(train_loader, vesde, ITERATIONS, LR, device=device)\n        torch.save(ema.state_dict(), CACHE_DIR / f\"{cache_name}\")\n        \n        \n    # Extract the training data points for visualization\n    patterns = train_subset.dataset[train_subset.indices]\n    pattern_set.append(patterns)\n    ema_set.append(ema)",
    "crumbs": [
      "tutorial",
      "Memory and Diffusion"
    ]
  },
  {
    "objectID": "tutorial/diffusion_as_memory.html#sampling-from-the-trained-model",
    "href": "tutorial/diffusion_as_memory.html#sampling-from-the-trained-model",
    "title": "Memory and Diffusion",
    "section": "Sampling from the Trained Model",
    "text": "Sampling from the Trained Model\nTo generate new samples, we run the diffusion process in reverse. We start with pure random noise (sampled at t=1) and use our trained score model to guide it back towards the data distribution (towards t=0). This is done using a numerical SDE solver, like the Euler-Maruyama method.\n\ndef Euler_Maruyama_sampler(score_model,\n                           sde,\n                           batch_size=64,\n                           num_steps=1000,\n                           device='cuda',\n                           eps=1e-5):\n    \"\"\"Generate samples from the score-based model using the Euler-Maruyama solver.\"\"\"\n    score_model.eval()\n    t_end = torch.ones(batch_size, device=device)\n\n    # Start with orthogonalized random noise ~ N(0, sigma_max^2 * I)\n    init_x = torch.randn(batch_size, 2, device=device)\n    init_x = init_x  / torch.norm(init_x, dim = (1), keepdim=True)\n    init_x = init_x * sde.marginal_prob_std(t_end)[:, None]\n\n    time_steps = torch.linspace(1., eps, num_steps, device=device)\n    step_size = time_steps[0] - time_steps[1] #dt\n    x = init_x\n\n    with torch.no_grad():\n        for time_step in tqdm(time_steps, desc=\"Sampling\"):\n            batch_time_step = torch.ones(batch_size, device=device) * time_step\n            g = sde.diffusion_coeff(batch_time_step)\n            # This is the reverse SDE update step\n            mean_x = x + (g**2)[:, None] * score_model(x, batch_time_step) * step_size\n            eps = torch.randn_like(x)\n            noise = torch.sqrt(step_size) * g[:, None] * eps\n            x = mean_x + noise\n\n    score_model.train()\n    return mean_x # Return the final denoised sample\n\nWe can now sample from the trained models.\n\ngenerated_set = []\n\nfor ema in ema_set:\n    generated_samples = Euler_Maruyama_sampler(ema, vesde, batch_size=1_000, device=device)\n    generated_samples = generated_samples.detach().cpu().numpy()\n    generated_set.append(generated_samples)\n\n\n\n\n\n\n\n\n\n\n\nCompute the Potential Energy of Generated vs.¬†Data Samples\nRecall the relationship between energy and probability denoted by the Boltzmann distribution: \\[\n    p_\\theta(\\mathbf{x}) = \\frac{\\exp{(-E_\\theta(\\mathbf{x}))} }{Z_\\theta}\n\\] This indicates that our energy (up to a constant) is obtained by computing the negative log-likelihood: \\[\n    -\\log p_\\theta(\\mathbf{x}) = E_\\theta(\\mathbf{x}) + C\n\\]\nSince we are dealing with a non-equilibrium system, that is our diffusion model, we follow the formulations and codes provided in Song et al.¬†(2021) to compute the log-likelihood: \\[\n    \\log p_0(\\mathbf{x}_0;\\mathbf{\\theta}) = \\log p_T(\\mathbf{x}_T; \\theta) + \\int_0^T \\nabla \\cdot \\tilde{\\mathbf{f}}(\\mathbf{x}_t, t)  \\mathrm{d}t\n\\] where \\[\n    \\tilde{\\mathbf{f}}(\\mathbf{x}_t, t) =-\\frac{1}{2}\\sigma^2 \\nabla_{\\mathbf{x}_{t}} \\log p_t(\\mathbf{x}_{t}; \\theta)\n\\] for this setting. Keep in mind, \\(\\nabla \\cdot ()\\) denotes the laplacian operation.\nTo derive the above equation, we start with the Fokker-Planck equation, as did in Chen et al.¬†(2018) and Song et al.¬†(2021), which yields the following general probability flow ODE (derived from the forward process SDE): \\[\n    \\mathrm{d} \\mathbf{x}_t = \\tilde{\\mathbf{f}}(\\mathbf{x}_t, t)\\mathrm{d} t + \\tilde{\\mathbf{g}} (\\mathbf{x}_t, t) \\mathrm{d} \\mathbf{w}_t\n\\] where \\[\\tilde{\\mathbf{f}}(\\mathbf{x}_t, t) = \\mathbf{f} (\\mathbf{x}_t, t) - \\frac{1}{2} \\nabla \\cdot \\big[\\mathbf{g} (\\mathbf{x}_t, t) \\mathbf{g} (\\mathbf{x}_t, t)^\\top \\big] - \\frac{1}{2}  \\big [ \\mathbf{g} (\\mathbf{x}_t, t) \\mathbf{g} (\\mathbf{x}_t, t)^\\top \\big ] \\nabla_{\\mathbf{x}_t} \\log p_t(\\mathbf{x}_t)\\] and \\(\\tilde{\\mathbf{g}} (\\mathbf{x}_t, t) = 0\\).\nHere, \\(\\mathbf{f}(\\mathbf{x}_t, t) = 0\\) denotes the drift term which vanishes in the VE setting while \\(\\mathbf{g} (\\mathbf{x}_t, t) = \\sigma\\) which is constant in this setting. Thus, the probability flow ODE for our setting is simply: \\[\n    \\mathrm{d} \\mathbf{x}_t = \\tilde{\\mathbf{f}}(\\mathbf{x}_t, t)\\mathrm{d} t = -\\frac{1}{2}\\sigma^2 \\nabla_{\\mathbf{x}_{t}} \\log p_t(\\mathbf{x}_{t}) \\, \\mathrm{d}t\n\\]\n\n# Compute Laplacian and Log-Likelihood\ndef compute_laplacian(score_fn, x, t):\n    \"\"\"Compute the Laplacian of the score function.\"\"\"\n    laplacian = torch.zeros(x.size(0), device=x.device)\n    with torch.enable_grad():\n        x.requires_grad_(True)\n        score = score_fn(x, t)\n        for i in range(x.shape[1]):\n            grad_score_i = torch.autograd.grad(score[:, i].sum(), x, create_graph=True)[0][:, i]\n            laplacian += grad_score_i\n    x.requires_grad_(False)\n    return laplacian.detach()\n\n\ndef ode_likelihood_with_laplacian(x, score_model, sde, device='cuda', eps=1e-5):\n    \"\"\"Compute the log-likelihood of x by solving the probability flow ODE.\"\"\"\n    shape = x.shape\n\n    def score_eval_wrapper(sample, time_steps):\n        \"\"\"A wrapper for evaluating the score-based model for the ODE solver.\"\"\"\n        sample = torch.tensor(sample, device=device, dtype=torch.float32).reshape(shape)\n        time_steps = torch.tensor(time_steps, device=device, dtype=torch.float32).reshape((sample.shape[0], ))\n        with torch.no_grad():\n            score = score_model(sample, time_steps)\n        return score.cpu().numpy().reshape((-1, shape[1])).astype(np.float64)\n\n    def laplacian_eval_wrapper(sample, time_steps):\n        \"\"\"A wrapper for evaluating the Laplacian of the score function.\"\"\"\n        sample = torch.tensor(sample, device=device, dtype=torch.float32).reshape(shape)\n        time_steps = torch.tensor(time_steps, device=device, dtype=torch.float32).reshape((sample.shape[0], ))\n        laplacian = compute_laplacian(score_model, sample, time_steps)\n        return laplacian.cpu().numpy().reshape((-1,)).astype(np.float64)\n\n    def ode_func(t, x_and_logp):\n        time_steps = torch.from_numpy(np.ones((shape[0],)) * t).to(device, torch.float32)\n        sample = torch.from_numpy(x_and_logp[:-shape[0]].reshape(shape)).to(device, torch.float32)\n\n        with torch.no_grad():\n            g = sde.diffusion_coeff(time_steps).cpu().numpy()\n\n            score = score_eval_wrapper(sample, time_steps)\n            laplacian = laplacian_eval_wrapper(sample, time_steps)\n\n        drift = -0.5 * g[:, None]**2 * score\n        logp_grad = -0.5 * g**2 * laplacian\n        return np.concatenate([drift.flatten(), logp_grad], axis = 0)\n\n    init = np.concatenate([x.cpu().numpy().flatten(), np.zeros((shape[0],))])\n    res = integrate.solve_ivp(ode_func, (eps, 1.), init, rtol=1e-5, atol=1e-5, method='RK45')\n    zp = torch.tensor(res.y[:, -1], device=device)\n    z = zp[:-shape[0]].reshape(shape)\n    delta_logp = zp[-shape[0]:]\n\n    sigma_max = sde.marginal_prob_std(torch.tensor(1.))\n\n    prior_logp = -shape[1] / 2. * torch.log(2 * np.pi * sigma_max ** 2)\n    prior_logp = prior_logp - torch.sum(z ** 2, dim=-1) / (2 * sigma_max ** 2)\n    return (prior_logp + delta_logp)\n\n\n\nVisualizing the Energy\nSince we are generating quite a lot of synthetic data points, we use hierarchical clustering to get a sense of where the concentrations of these new points are at. To be more informative, we are also displaying the energy profile of these concentrations alongside that of the training data points.\nAt K = 2, we can see that the concentrations of generated points are pretty much surrounding the data points and their energy profile are similar to that of the training data points.\nMeanwhile, at K = 9, we now see local minima of the energy that devitate drastically from the training points. These new local minima of the energy are called spurious patterns.\nFinally, when K = 1000, the energy now very closely matches that of the DenseAM‚Äôs derived exact energy, see below.\n\ndef to_energy(loglikelihood, normalize=True):\n    # nll = E + C\n    energy = -loglikelihood\n    if normalize:\n         # normalize energy by its minimum\n        return energy - energy.min()\n    return energy\n\n\ndef plot_combined_landscape(fig, ax, score_model, sde, patterns, samples, labels, centers, t_eval=1e-5, device='cpu', annotate=True):\n    \"\"\"Visualize the energy landscape with annotated points.\"\"\"\n    score_model.eval()\n\n    # 1. Calculate Energy for Patterns and Centers\n    # Energy for original patterns\n    patterns_tensor = patterns.to(device)\n    logp_patterns = ode_likelihood_with_laplacian(patterns_tensor, score_model, sde, device, eps=t_eval)\n    energy_patterns = to_energy(logp_patterns).cpu().numpy()\n\n    # Energy for found cluster centers\n    centers_tensor = torch.from_numpy(centers).float().to(device)\n    logp_centers = ode_likelihood_with_laplacian(centers_tensor, score_model, sde, device, eps=t_eval)\n    energy_centers = to_energy(logp_centers).cpu().numpy()\n\n    # 2. Create a grid of points\n    bounds=(-1.5, 1.5); resolution=75\n    x_ = torch.linspace(bounds[0], bounds[1], resolution)\n    y_ = torch.linspace(bounds[0], bounds[1], resolution)\n    X, Y = torch.meshgrid(x_, y_, indexing='ij')\n    grid_tensor = torch.stack([X.flatten(), Y.flatten()], dim=1).to(device)\n\n    # 3. Calculate the energy and score for the grid\n    logp_grid = ode_likelihood_with_laplacian(grid_tensor, score_model, sde, device, eps=t_eval)\n    energy_grid = to_energy(logp_grid).cpu().numpy().reshape(resolution, resolution)\n\n    with torch.no_grad():\n        vec_t = torch.ones(grid_tensor.shape[0], device=device) * t_eval\n        scores = score_model(grid_tensor, vec_t).cpu().numpy()\n\n    # 4. Create the comprehensive plot\n    #fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n    ax.set_title(f'K = {len(patterns)}', fontsize=16)\n    ax.set_aspect('equal'); ax.grid(False)\n\n    # Plot energy contour\n    contour = ax.contourf(X.cpu(), Y.cpu(), energy_grid, levels=100, cmap='inferno', zorder=0)\n\n    # Plot the unit circle\n    theta = np.linspace(0, 2 * np.pi, 200)\n    ax.plot(np.cos(theta), np.sin(theta), color='white', linestyle='--', alpha=0.6, zorder=1, label='Unit Circle')\n\n    # Plot score field\n    ax.quiver(grid_tensor[:, 0].cpu(), grid_tensor[:, 1].cpu(),\n              scores[:, 0], scores[:, 1], color='white', alpha=0.5,\n              width=0.003, headwidth=3, zorder=2)\n\n    # Plot original patterns\n    ax.scatter(patterns[:, 0].cpu(), patterns[:, 1].cpu(), marker=\"*\", alpha=0.75,\n               s=400, color=\"deeppink\", label=\"Original Patterns\", edgecolor='black', zorder=5)\n\n    if annotate: # turn off annotation and plotting center since there are too many patterns at this point\n        # Plot generated samples\n        ax.scatter(samples[:, 0], samples[:, 1], c=labels, cmap='viridis',\n                   s=15, alpha=0.2, zorder=3, label='Generated Samples')\n\n        # Plot found cluster centers\n        ax.scatter(centers[:, 0], centers[:, 1], marker='X', s=250,\n                    color='aqua', edgecolor='black', zorder=4, label='Cluster Centers')\n\n        # Add energy annotations for patterns\n        for i, p in enumerate(patterns.cpu().numpy()):\n            ax.annotate(f'{energy_patterns[i]:.2f}', (p[0], p[1]),\n                        xytext=(15, -15), textcoords='offset points', color='deeppink', fontsize=10,\n                        weight='bold', bbox=dict(boxstyle=\"round,pad=0.2\", fc=\"black\", ec=\"lime\", lw=1, alpha=0.6))\n\n        # Add energy annotations for cluster centers\n        for i, c in enumerate(centers):\n            ax.annotate(f'{energy_centers[i]:.2f}', (c[0], c[1]),\n                        xytext=(-20, 15), textcoords='offset points', color='aqua', fontsize=10,\n                        weight='bold', bbox=dict(boxstyle=\"round,pad=0.2\", fc=\"black\", ec=\"yellow\", lw=1, alpha=0.6))\n\n    ax.set_xlabel('x'); ax.set_ylabel('y')\n    ax.set_xlim(bounds); ax.set_ylim(bounds)\n    return contour\n\n\ndef plot_energy_surface_3d(fig, ax, X, Y, energy, view_angle=(60, -60), title=None):\n    \"\"\"\n    Creates a 3D surface plot of the energy landscape.\n\n    Args:\n        X (np.ndarray): Meshgrid for X coordinates.\n        Y (np.ndarray): Meshgrid for Y coordinates.\n        energy (np.ndarray): 2D array of energy values.\n        view_angle (tuple): Tuple of (elevation, azimuth) for the plot's camera angle.\n    \"\"\"\n    # Plot the 3D surface\n    surface = ax.plot_surface(X, Y, energy, cmap='inferno', rstride=1, cstride=1,\n                              linewidth=0, antialiased=True, alpha=0.9)\n\n    # Set labels and title\n    ax.set_xlabel('x', fontsize=12)\n    ax.set_ylabel('y', fontsize=12)\n    ax.set_title(title, fontsize=18)\n\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_zticks([])\n\n    # Set a nice viewing angle\n    ax.view_init(elev=view_angle[0], azim=view_angle[1])\n    return surface\n\n\n# @title Visualization of the Energy Landscape Across K training Sizes in 2D\n# This threshold determines how close points need to be to be considered in the same cluster.\n# You may need to tune this value based on your results.\ndist_thresholds = [3, 2, 0.45]\nannotates = [True, True, False]\nt_evals = [0.15] * len(dist_thresholds)\n\nfig, axs = plt.subplots(1, 3, figsize=(18, 8), constrained_layout=True)\nfor i, (patterns, generated_samples, ema, threshold, t_eval, annotate) in enumerate(zip(pattern_set, generated_set, ema_set, dist_thresholds, t_evals, annotates)):\n    # Perform Agglomerative (Hierarchical) Clustering\n    clustering = AgglomerativeClustering(\n        n_clusters=None,                                      # We let the algorithm find the clusters based on the threshold\n        distance_threshold=threshold\n    ).fit(generated_samples)\n\n    # Find the center of each identified cluster\n    cluster_labels = clustering.labels_\n    n_clusters_found = len(np.unique(cluster_labels))\n    #print(f\"Found {n_clusters_found} clusters!\")\n\n    cluster_centers = np.array([\n        generated_samples[cluster_labels == i].mean(axis=0)\n        for i in range(n_clusters_found)\n    ])\n\n    contour = plot_combined_landscape(\n        fig, axs[i],\n        ema,\n        vesde,\n        patterns,\n        generated_samples,\n        cluster_labels,\n        cluster_centers,\n        device=device,\n        t_eval=t_eval,\n        annotate=annotate,\n    )\n\naxs[0].legend(loc='lower left', fontsize=12)\n# Set the ticks to only be at the min and max\ncbar = fig.colorbar(contour, ax=axs[-1], label='Energy (Lower is Better)', shrink=0.8)\nvmin, vmax = contour.get_clim()\ncbar.set_ticks([vmin, vmax])\ncbar.set_ticklabels(['Low', 'High'], fontsize=14)\nplt.show()\nplt.show()\n\n\n\n\n\n\n\n\n\n# @title Visualization of the Energy Landscape Across K training Sizes in 3D\n# This code is slow to run, so we cache the figure\nCACHE_FIG = True\n\nbounds = (-1.5, 1.5)\nresolution = 100\nX_grid, Y_grid = torch.meshgrid(\n    torch.linspace(bounds[0], bounds[1], resolution),\n    torch.linspace(bounds[0], bounds[1], resolution),\n    indexing='ij'\n)\ngrid_tensor = torch.stack([X_grid.ravel(), Y_grid.ravel()], dim=1).to(device)\n\nfig_fname = CACHE_DIR / \"slow_fig.png\"\nif CACHE_FIG and fig_fname.exists():\n    img = PIL.Image.open(str(fig_fname))\n    display(img)\nelse:\n    # 1. Create a grid of points (same as before)\n\n    # Make Figure\n    fig, axs = plt.subplots(1, 3, figsize=(20, 8), constrained_layout=True, subplot_kw={\"projection\": \"3d\"})\n\n    for i, (patterns, score_model) in enumerate(zip(pattern_set, ema_set)):\n        # 2. Calculate the energy for the grid (this is the slow step)\n        logp_grid = ode_likelihood_with_laplacian(grid_tensor, score_model, vesde, device=device, eps=0.05)\n        energy_grid = to_energy(logp_grid).cpu().numpy()\n        energy_grid = energy_grid.reshape(resolution, resolution)\n\n        # 3. Create the plot\n        contour = plot_energy_surface_3d(fig, axs[i], X_grid.cpu().numpy(), Y_grid.cpu().numpy(), energy_grid, title=f'K = {len(patterns)}')\n\n    # Set the ticks to only be at the min and max\n    cbar = fig.colorbar(contour, ax=axs[-1], label='Energy (Lower is Better)', shrink=0.75, pad=0.25)\n    vmin, vmax = contour.get_clim()\n    cbar.set_ticks([vmin, vmax])\n    cbar.set_ticklabels(['Low', 'High'], fontsize=14)\n    plt.savefig(fig_fname)\nplt.show()",
    "crumbs": [
      "tutorial",
      "Memory and Diffusion"
    ]
  },
  {
    "objectID": "tutorial/dense_storage.html",
    "href": "tutorial/dense_storage.html",
    "title": "Binary Dense Storage",
    "section": "",
    "text": "Notebook Execution Settings\nCACHE_DIR = \"cache/00_dense_storage\"\nCACHE_RECALL = True # If False, regenerate all saved results even if files exist.\nSHOW_FULL_ANIMATIONS = True # If True, render videos instead of gifs. This is slower than gifs and relies on `ffmpeg` to save the animation, but it lets us see the energy descent alongside the frame evolution.",
    "crumbs": [
      "tutorial",
      "Binary Dense Storage"
    ]
  },
  {
    "objectID": "tutorial/dense_storage.html#the-classical-hopfield-network",
    "href": "tutorial/dense_storage.html#the-classical-hopfield-network",
    "title": "Binary Dense Storage",
    "section": "The Classical Hopfield Network",
    "text": "The Classical Hopfield Network\nLet‚Äôs revisit our task to store \\(K\\) binary patterns each of dimension \\(D\\) into an energy function. Let‚Äôs keep things simple and fast for the first part of this notebook and focus on storing and retrieving \\(K=2\\) patterns: an eevee and pichu, where each (48,48) image is rasterized to a vector dimension of \\(D=2304\\).\n\ndesired_names = [\"eevee\", \"pichu\"]\neevee_pichu_idxs = [poke_names.index(name) for name in desired_names]\nXi = data[eevee_pichu_idxs]\n\nfig, ax = show_im(Xi, figsize=(6,3));\nax.set_title(\"Stored patterns\")\nplt.show()\n\nprint(f\"K={Xi.shape[0]}, D={Xi.shape[1]}\")\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 2\n      1 desired_names = [\"eevee\", \"pichu\"]\n----&gt; 2 eevee_pichu_idxs = [poke_names.index(name) for name in desired_names]\n      3 Xi = data[eevee_pichu_idxs]\n      5 fig, ax = show_im(Xi, figsize=(6,3));\n\nCell In[1], line 2, in &lt;listcomp&gt;(.0)\n      1 desired_names = [\"eevee\", \"pichu\"]\n----&gt; 2 eevee_pichu_idxs = [poke_names.index(name) for name in desired_names]\n      3 Xi = data[eevee_pichu_idxs]\n      5 fig, ax = show_im(Xi, figsize=(6,3));\n\nNameError: name 'poke_names' is not defined\n\n\nNameError: name 'poke_names' is not defined\nTraceback (most recent call last):\n\n  File \"/Users/hoo/Projects/amtutorial/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n\n  File \"&lt;ipython-input-1-f2342d7c1682&gt;\", line 2, in &lt;module&gt;\n    eevee_pichu_idxs = [poke_names.index(name) for name in desired_names]\n\n  File \"&lt;ipython-input-1-f2342d7c1682&gt;\", line 2, in &lt;listcomp&gt;\n    eevee_pichu_idxs = [poke_names.index(name) for name in desired_names]\n\nNameError: name 'poke_names' is not defined\n\n\nThe Classical Hopfield Network (CHN) defines an energy function for this collection of patterns, putting the \\(\\mu\\)-th stored pattern \\(\\xi_\\mu\\) at a low value of energy. The CHN energy is a quadratic function described by dot-product correlations:\n\\[\nE_\\text{CHN}(x) = -\\frac{1}{2} \\left(\\sum_{\\mu} \\xi_{\\mu i} x_i\\right)^2 = -\\frac{1}{2} \\sum_{i,j} T_{ij} x_i x_j.\n\\tag{2}\\]\nWe see the familiar equation for CHN energy on the RHS if we expand the quadratic function, where \\(T_{ij} := \\sum_{\\mu=1}^K \\xi_{\\mu i} \\xi_{\\mu_j}\\) is the matrix of symmetric synapses. Learned patterns \\(\\xi_\\mu\\) are stored in \\(T\\) via a simple, Hebbian learning rule.\nThe CHN can be easily implemented in code via\n\nclass CHN(BinaryAM):\n    def energy(\n        self, \n        x: Float[Array, \"D\"] # Possibly noisy query pattern\n        ): \n        \"Quadratic energy function for the CHN\"\n        return -0.5 * jnp.sum((self.Xi @ x)**2, axis=0)\n\nchn = CHN(Xi)\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 9\n      6         \"Quadratic energy function for the CHN\"\n      7         return -0.5 * jnp.sum((self.Xi @ x)**2, axis=0)\n----&gt; 9 chn = CHN(Xi)\n\nNameError: name 'Xi' is not defined\n\n\nNameError: name 'Xi' is not defined\nTraceback (most recent call last):\n\n  File \"/Users/hoo/Projects/amtutorial/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n\n  File \"&lt;ipython-input-1-f560da26cfe1&gt;\", line 9, in &lt;module&gt;\n    chn = CHN(Xi)\n\nNameError: name 'Xi' is not defined\n\n\nThe asynchronous update rule of Equation¬†1 uses the energy difference of a flipped bit to determine whether to keep the flip or not. That update rule is equivalent to the following, arguably more familiar update rule, which describes the next state based on the sign of the total input current to the neuron \\(x_i\\).\n\\[\n\\begin{align*}\nx_i^{(t+1)} &\\leftarrow \\text{sgn}\\left(\\sum_{\\mu} \\xi_{\\mu i} \\sum_{j \\neq i} \\left(\\xi_{\\mu j} x_j^{(t)}\\right) \\right)\\\\\n\\text{sgn}(x) &:= \\begin{cases}\n1 & \\text{if } x \\geq 0 \\\\\n-1 & \\text{if } x &lt; 0\n\\end{cases}\\quad.\n\\end{align*}\n\\]\nThis update rule also ensures the network always moves toward lower energy states. Because the \\(E_\\text{CHN}\\) is bounded from below, the network will eventually converge to a local minimum that (ideally) corresponds to one of the stored patterns.\n\nLet‚Äôs observe the recall process! We‚Äôll start with a noisy version of the first pattern and see if we can recover it.\n\ndef flip_some_bits(key, x, p=0.1):\n    \"Flip `p` fraction of bits in `x`\"\n    prange = np.array([p, 1-p])\n    return x * jr.choice(key, np.array([-1, 1]), p=prange, shape=x.shape)\n\nx_og = Xi[0] \nx_noisy = flip_some_bits(jr.PRNGKey(0), x_og, 0.2)\n\nshow_im(jnp.stack([x_og, x_noisy]), figsize=(6, 3));\n\nNameError: name 'Xi' is not defined\nTraceback (most recent call last):\n\n  File \"/Users/hoo/Projects/amtutorial/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n\n  File \"&lt;ipython-input-1-849edf721eda&gt;\", line 6, in &lt;module&gt;\n    x_og = Xi[0]\n\nNameError: name 'Xi' is not defined\n\n\nFor the pedagogical purpose of this notebook, we‚Äôll cache the recall process and results so we don‚Äôt have to run it every time.\n\n@delegates(BinaryAM.async_recall)\ndef cached_recall(am, cache_name, x_noisy, key=jr.PRNGKey(0), save=True, **kwargs):\n    \"Cache the recall process using key `cache_name`\"\n    npz_fname = Path(CACHE_DIR) / (cache_name + '.npz')\n    if npz_fname.exists() and CACHE_RECALL: \n        npz_data = np.load(npz_fname)\n        x_final, frames, energies = npz_data['x_final'], npz_data['frames'], npz_data['energies']\n        print(\"Loading cached recall data\")\n    else: \n        x_final, (frames, energies) = am.async_recall(x_noisy, key=key, **kwargs)\n        if save: jnp.savez(npz_fname, x_final=x_final, frames=frames, energies=energies)\n    return x_final, frames, energies\n\ncache_name = 'basic_hopfield_recovery'\nx_final, frames, energies = cached_recall(chn, cache_name, x_noisy, nsteps=12000, key=jr.PRNGKey(5))\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 15\n     12     return x_final, frames, energies\n     14 cache_name = 'basic_hopfield_recovery'\n---&gt; 15 x_final, frames, energies = cached_recall(chn, cache_name, x_noisy, nsteps=12000, key=jr.PRNGKey(5))\n\nNameError: name 'chn' is not defined\n\n\nNameError: name 'chn' is not defined\nTraceback (most recent call last):\n\n  File \"/Users/hoo/Projects/amtutorial/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n\n  File \"&lt;ipython-input-1-89a86417e3ee&gt;\", line 15, in &lt;module&gt;\n    x_final, frames, energies = cached_recall(chn, cache_name, x_noisy, nsteps=12000, key=jr.PRNGKey(5))\n\nNameError: name 'chn' is not defined. Did you mean: 'chr'?\n\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 24\n     21         axes[3].set_title(\"Original pattern\")\n     22     return fig, axes\n---&gt; 24 fig, axes = show_recall_output(x_og, x_noisy, x_final, energies, show_original=False)\n     25 plt.show()\n\nNameError: name 'x_og' is not defined\n\n\nNameError: name 'x_og' is not defined\nTraceback (most recent call last):\n\n  File \"/Users/hoo/Projects/amtutorial/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n\n  File \"&lt;ipython-input-1-277b6b65ef33&gt;\", line 24, in &lt;module&gt;\n    fig, axes = show_recall_output(x_og, x_noisy, x_final, energies, show_original=False)\n\nNameError: name 'x_og' is not defined\n\n\nWe can animate the recall process to view the ‚Äúthinking‚Äù process of the CHN.\n\n\n\n\nVideo\n\n\n\n\n\nRetrieving ‚Äúinverted‚Äù images\nIf we initialize a query with too much noise, it‚Äôs possible to retrieve the negative of a stored pattern or an ‚Äúinverted image‚Äù. Because the energy is quadratic, both \\(x\\) and \\(-x\\) produce the same small value of energy. Whether we retrieve the original \\(x\\) or the inverted \\(-x\\) is dependent on whether we initialize our query closer to the original or inverted pattern.\n\\[\nE_\\text{CHN}(-x) = -\\frac{1}{2} \\left(\\sum_{\\mu} \\xi_{\\mu i} (-x_i)\\right)^2 = E_\\text{CHN}(x)\n\\]\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 2\n      1 #| echo: false\n----&gt; 2 x_og = Xi[0] \n      3 x_noisy = flip_some_bits(jr.PRNGKey(1), x_og, 0.6)\n      5 cache_name = 'hopfield_recovery_inverted'\n\nNameError: name 'Xi' is not defined\n\n\nNameError: name 'Xi' is not defined\nTraceback (most recent call last):\n\n  File \"/Users/hoo/Projects/amtutorial/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n\n  File \"&lt;ipython-input-1-72d87cb893c3&gt;\", line 2, in &lt;module&gt;\n    x_og = Xi[0]\n\nNameError: name 'Xi' is not defined\n\n\n\n\nMemory retrieval failure\nUnfortunately, the CHN is terrible at storing and retrieving multiple patterns. If we add even four more patterns into the synaptic memory, our network will fail to retrieve our eevee.\n\nXi = data[eevee_pichu_idxs]\nXi = jnp.concatenate([Xi, jr.choice(jr.PRNGKey(10), data, shape=(4,), replace=False)])\nfig, ax = show_im(Xi, figsize=(6, 4));\nax.set_title(f\"Stored patterns (K={Xi.shape[0]})\")\nplt.show()\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 1\n----&gt; 1 Xi = data[eevee_pichu_idxs]\n      2 Xi = jnp.concatenate([Xi, jr.choice(jr.PRNGKey(10), data, shape=(4,), replace=False)])\n      3 fig, ax = show_im(Xi, figsize=(6, 4));\n\nNameError: name 'data' is not defined\n\n\nNameError: name 'data' is not defined\nTraceback (most recent call last):\n\n  File \"/Users/hoo/Projects/amtutorial/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n\n  File \"&lt;ipython-input-1-f15949de1ded&gt;\", line 1, in &lt;module&gt;\n    Xi = data[eevee_pichu_idxs]\n\nNameError: name 'data' is not defined\n\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 2\n      1 #| echo: false\n----&gt; 2 x_og = Xi[0]\n      3 x_noisy = flip_some_bits(jr.PRNGKey(0), x_og, 0.2)\n      5 chn = CHN(Xi)\n\nNameError: name 'Xi' is not defined\n\n\nNameError: name 'Xi' is not defined\nTraceback (most recent call last):\n\n  File \"/Users/hoo/Projects/amtutorial/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n\n  File \"&lt;ipython-input-1-ab11b499b723&gt;\", line 2, in &lt;module&gt;\n    x_og = Xi[0]\n\nNameError: name 'Xi' is not defined",
    "crumbs": [
      "tutorial",
      "Binary Dense Storage"
    ]
  },
  {
    "objectID": "tutorial/dense_storage.html#dense-associative-memory",
    "href": "tutorial/dense_storage.html#dense-associative-memory",
    "title": "Binary Dense Storage",
    "section": "Dense Associative Memory",
    "text": "Dense Associative Memory\nThe CHN has a quadratic energy, which is a special case of a more general class of models called Dense Associative Memory (DenseAM). If we increase the degree of the polynomial used in the energy function, we strengthen the coupling between neurons and can store more patterns into the same synaptic matrix.\nThe new energy function, written in terms of polynomials of degree \\(n\\) and using the same notation for stored patterns \\(\\xi_{\\mu i}\\), is\n\\[\n\\begin{align*}\nE_\\text{DAM}(x) &= -\\sum_{\\mu=1}^K F_n\\left(\\sum_{i=1}^D \\xi_{\\mu i} x_i\\right),\\\\\n\\text{where}\\;F_n(x) &= \\begin{cases} \\frac{x^n}{n} & \\text{if } x \\geq 0 \\\\ 0 & \\text{if } x &lt; 0 \\end{cases}.\n\\end{align*}\n\\tag{3}\\]\n\n\n\n\n\n\nNote\n\n\n\nWe need \\(F_n\\) to be convex for all \\(n\\), which is why we perform the rectification. We could alternatively limit ourselves to only even values of \\(n\\).\nFun fact, rectified polynomials remove the ‚Äúinverted‚Äù retrieval phenomenon seen in Section¬†1.1.\n\n\nEquation¬†3 admits the following manual update rule for a single neuron \\(i\\):\n\\[\n\\begin{align*}\nx_i^{(t+1)} &\\leftarrow \\text{sgn}\\left( \\sum_{\\mu} \\xi_{\\mu i} f_n\\left( \\sum_{j \\neq i} \\xi_{\\mu j} x_j^{(t)}\\right)\\right)\\\\\n\\end{align*}.\n\\tag{4}\\]\nHere we introduced an activation function \\(f_n(\\cdot) = F_n'(\\cdot)\\) that is the derivative of the rectified polynomial used to define the energy. This update can be viewed as the negative gradient of the energy function, ensuring that the network always moves toward lower energy states. Like before, this energy is bounded from below and we will eventually converge to a local minimum that corresponds to one of the stored patterns.\nLet‚Äôs implement the DenseAM model. The primary difference from the CHN is that now we generalize the quadratic energy to a (possibly rectified) polynomial energy.\n\nclass PolynomialDenseAM(BinaryAM):\n    Xi: jax.Array # (K, D) Memory patterns \n    n: int # Power of polynomial F\n    rectified: bool = True # Whether to rectify inputs to F\n\n    def F_n(self, sims): \n        \"\"\"Rectified polynomial of degree `n` for energy\"\"\"\n        sims = sims.clip(0) if self.rectified else sims\n        return 1 / self.n * sims ** self.n\n\n    def energy(self, x): \n        return -jnp.sum(self.F_n(self.Xi @ x))\n\nA simple change to using a polynomial of degree \\(6\\) instead of the CHN‚Äôs quadratic energy function allows us to store and retrieve our desired eevee even with up to \\(K=100\\) patterns.\n\n# Increase the number of stored patterns!\nXi = data[eevee_pichu_idxs]\nXi = jnp.concatenate([Xi, jr.choice(jr.PRNGKey(10), data, shape=(98,), replace=False)])\nfig1, ax1 = show_im(Xi, figsize=(7,7));\nax1.set_title(\"Stored patterns\")\ndam = PolynomialDenseAM(Xi, n=6, rectified=True)\n\nfname = f'dam_recovery_n_{dam.n}_K_{Xi.shape[0]}'\n\nx_og = Xi[0]\nx_noisy = flip_some_bits(jr.PRNGKey(0), x_og, 0.2)\nx_final, frames, energies = cached_recall(dam, fname, x_noisy, nsteps=20000, key=jr.PRNGKey(5))\n\nfig2, axes2 = show_recall_output(x_og, x_noisy, x_final, energies, show_original=False)\nfig2.suptitle(f\"DenseAM(n={dam.n}, K={Xi.shape[0]})\")\nplt.subplots_adjust(top=0.75)\nplt.show()\n\nvideo, video_fname = show_cached_recall_animation(fname, steps_per_sample=32)\nMarkdown(f\"![]({video_fname})\")\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 2\n      1 # Increase the number of stored patterns!\n----&gt; 2 Xi = data[eevee_pichu_idxs]\n      3 Xi = jnp.concatenate([Xi, jr.choice(jr.PRNGKey(10), data, shape=(98,), replace=False)])\n      4 fig1, ax1 = show_im(Xi, figsize=(7,7));\n\nNameError: name 'data' is not defined\n\n\nNameError: name 'data' is not defined\nTraceback (most recent call last):\n\n  File \"/Users/hoo/Projects/amtutorial/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n\n  File \"&lt;ipython-input-1-205bf6cd7c9d&gt;\", line 2, in &lt;module&gt;\n    Xi = data[eevee_pichu_idxs]\n\nNameError: name 'data' is not defined\n\n\nA higher degree polynomial gives us more storage capacity, which means that it is easier to retrieve the patterns we have stored in the network. Note that the higher the degree \\(n\\), the narrower the basins of attraction, which makes it easier to pack more patterns into the energy landscape.",
    "crumbs": [
      "tutorial",
      "Binary Dense Storage"
    ]
  },
  {
    "objectID": "tutorial/dense_storage.html#sec-gotta-catch-em-all",
    "href": "tutorial/dense_storage.html#sec-gotta-catch-em-all",
    "title": "Binary Dense Storage",
    "section": "Gotta catch ‚Äôem all!",
    "text": "Gotta catch ‚Äôem all!\nLet‚Äôs try to store and retrieve all 1024 pokemon patterns into our network (though we will only show retrieval for a subset of them for computational reasons). To do this, we‚Äôll need very large values of \\(n\\), which is bad for numeric overflow (computers don‚Äôt like working in really really large numbers i.e., inf energy regimes).\nWe‚Äôll implement an exponential version of the DenseAM (Demircigil et al. 2017) using the numerically stable logsumexp function. This form of the DenseAM energy was introduced by (Ramsauer et al. 2021) and is also referred to as the ‚ÄúModern Hopfield Network‚Äù (MHN).\n\\[\n\\begin{align*}\nE_\\text{MHN}(x) &= -\\log \\sum_{\\mu=1}^K \\exp \\left(\\beta \\sum_{i=1}^D \\xi_{\\mu i} x_i\\right)\n\\end{align*}\n\\tag{5}\\]\nwhere increasing the inverse temperature \\(\\beta\\) has a similar effect to increasing \\(n\\) in the DenseAM polynomial energy function. Because the log is a monotonically increasing function, the energy minima of the original energy function are preserved, while simultaneously making the energy function more numerically stable.\n\nclass ExponentialDenseAM(BinaryAM):\n    Xi: jax.Array # (K, D) Memory patterns \n    beta: float = 1.0 # Temperature parameter\n\n    def energy(self, x):\n        return -jax.nn.logsumexp(self.beta * self.Xi @ x, axis=-1)\n\n\n# Show larger batch retrieval\nXi = data[:1024]\nNshow = 255\nXi_show = jnp.concatenate([data[eevee_pichu_idxs], jr.choice(jr.PRNGKey(10), Xi, shape=(Nshow - len(eevee_pichu_idxs),), replace=False)])\nfig1, ax1 = show_im(Xi_show, figsize=(8,8));\nax1.set_title(f\"Random sample of {Nshow} stored patterns\")\nprint(f\"Storing {Xi.shape[0]} patterns\")\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 2\n      1 # Show larger batch retrieval\n----&gt; 2 Xi = data[:1024]\n      3 Nshow = 255\n      4 Xi_show = jnp.concatenate([data[eevee_pichu_idxs], jr.choice(jr.PRNGKey(10), Xi, shape=(Nshow - len(eevee_pichu_idxs),), replace=False)])\n\nNameError: name 'data' is not defined\n\n\nNameError: name 'data' is not defined\nTraceback (most recent call last):\n\n  File \"/Users/hoo/Projects/amtutorial/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n\n  File \"&lt;ipython-input-1-69d49195a2cf&gt;\", line 2, in &lt;module&gt;\n    Xi = data[:1024]\n\nNameError: name 'data' is not defined\n\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 6\n      3 nh = nw = 10\n      4 N = nh * nw # Sample N patterns to show in grid\n      5 x_og = jnp.concatenate([\n----&gt; 6     data[eevee_pichu_idxs], \n      7     jr.choice(jr.PRNGKey(10), data, shape=(N - len(eevee_pichu_idxs),), replace=False)])\n      8 x_noisy = flip_some_bits(key2, x_og, 0.25)\n     10 mhn = ExponentialDenseAM(Xi, beta=50.)\n\nNameError: name 'data' is not defined\n\n\nNameError: name 'data' is not defined\nTraceback (most recent call last):\n\n  File \"/Users/hoo/Projects/amtutorial/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n\n  File \"&lt;ipython-input-1-37b6e0bb5c60&gt;\", line 6, in &lt;module&gt;\n    data[eevee_pichu_idxs],\n\nNameError: name 'data' is not defined\n\n\nAnd of course, what‚Äôs the fun if we can‚Äôt animate the retrieval process?\n\nNameError Traceback (most recent call last) Cell In[1], line 48 46 gif = Image(filename=gif_fname, width=400) 47 else: ‚Äî&gt; 48 gif, gif_fname = show_batched_recall_animation(frames, energies, cache_name) 50 display(gif) 51 Markdown(f‚Äù‚Äú)\nNameError: name ‚Äòframes‚Äô is not defined\nNameError: name 'frames' is not defined\nTraceback (most recent call last):\n\n  File \"/Users/hoo/Projects/amtutorial/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n\n  File \"&lt;ipython-input-1-0865c94fa7a5&gt;\", line 48, in &lt;module&gt;\n    gif, gif_fname = show_batched_recall_animation(frames, energies, cache_name)\n\nNameError: name 'frames' is not defined",
    "crumbs": [
      "tutorial",
      "Binary Dense Storage"
    ]
  },
  {
    "objectID": "tutorial/energy_transformer.html",
    "href": "tutorial/energy_transformer.html",
    "title": "Energy Transformer",
    "section": "",
    "text": "Squint, and the Transformer looks like a dynamical system.\n\n\n\n\n\n\n\nüöß Under construction\n\n\n\nThis notebook is under construction. It will be completed by July 14, 2025.\n\n\nAt its core, the transformer is a stack of \\(L\\) transformer blocks that takes a length \\(N\\) sequence of input tokens \\(\\{\\mathbf{x}^{(0)}_1, \\ldots, \\mathbf{x}^{(0)}_N\\}\\) and outputs a length \\(N\\) sequence of output tokens \\(\\{\\mathbf{x}^{(L)}_1, \\ldots, \\mathbf{x}^{(L)}_N\\}\\). Each token \\(\\mathbf{x}^{(l)}_i \\in \\mathbb{R}^D\\) is a vector of dimension \\(D\\).\nWhen blocks are stacked, the residual connections form a ‚Äúresidual highway‚Äù that consists entirely of normalizations and additions from Attention and MLP operations.\n\n\n\n\n\n\nFigure¬†1: A vanilla Transformer Block consisting of 4 main operations: (multi-headed) attention, MLP, (pre-)layernorms, and residual connections. The Transformer is a stack of these blocks, which we show depicted as a ‚Äúresidual highway‚Äù design. The residual highway showcases how each block ‚Äúperturbs‚Äù its input, and the mathematical operation looks like a dynamical system. If the system can be described such that the operation of each block is a gradient descent, the system becomes an energy-based model.\n\n\n\nAssociative Memory (AM) requires a global energy function, where each computation minimizes the total energy of the system. Our goal is to derive an energy function whose gradient looks as much like the Transformer block as possible.",
    "crumbs": [
      "tutorial",
      "Energy Transformer"
    ]
  },
  {
    "objectID": "tutorial/energy_transformer.html#transformers-look-like-dynamical-systems",
    "href": "tutorial/energy_transformer.html#transformers-look-like-dynamical-systems",
    "title": "Energy Transformer",
    "section": "",
    "text": "Squint, and the Transformer looks like a dynamical system.\n\n\n\n\n\n\n\nüöß Under construction\n\n\n\nThis notebook is under construction. It will be completed by July 14, 2025.\n\n\nAt its core, the transformer is a stack of \\(L\\) transformer blocks that takes a length \\(N\\) sequence of input tokens \\(\\{\\mathbf{x}^{(0)}_1, \\ldots, \\mathbf{x}^{(0)}_N\\}\\) and outputs a length \\(N\\) sequence of output tokens \\(\\{\\mathbf{x}^{(L)}_1, \\ldots, \\mathbf{x}^{(L)}_N\\}\\). Each token \\(\\mathbf{x}^{(l)}_i \\in \\mathbb{R}^D\\) is a vector of dimension \\(D\\).\nWhen blocks are stacked, the residual connections form a ‚Äúresidual highway‚Äù that consists entirely of normalizations and additions from Attention and MLP operations.\n\n\n\n\n\n\nFigure¬†1: A vanilla Transformer Block consisting of 4 main operations: (multi-headed) attention, MLP, (pre-)layernorms, and residual connections. The Transformer is a stack of these blocks, which we show depicted as a ‚Äúresidual highway‚Äù design. The residual highway showcases how each block ‚Äúperturbs‚Äù its input, and the mathematical operation looks like a dynamical system. If the system can be described such that the operation of each block is a gradient descent, the system becomes an energy-based model.\n\n\n\nAssociative Memory (AM) requires a global energy function, where each computation minimizes the total energy of the system. Our goal is to derive an energy function whose gradient looks as much like the Transformer block as possible.",
    "crumbs": [
      "tutorial",
      "Energy Transformer"
    ]
  },
  {
    "objectID": "tutorial/energy_transformer.html#energy-transformer",
    "href": "tutorial/energy_transformer.html#energy-transformer",
    "title": "Energy Transformer",
    "section": "Energy Transformer",
    "text": "Energy Transformer\nWe will now build a kind of associative memory called the ‚ÄúEnergy Transformer‚Äù (Hoover et al. 2024) that turns the familiar transformer operation into an energy minimization. Energy Transformer (ET) defines a single energy on an \\(\\mathbf{x} \\in \\mathbb{R}^{N \\times D}\\) collection of tokens, where we can think of each token \\(\\mathbf{x}_B\\) as a ‚Äúparticle‚Äù that knows some information about itself and needs to figure out what it should become. Some particles (unmasked tokens) already know their identity, while others (masked tokens) only know their position and must discover their identity by interacting with their neighbors.\nMinimizing the energy of the Energy Transformer (ET) is a recurrent process. The entire transformer consists of a single Transformer block, and each ‚Äúlayer‚Äù of the transformer becomes a gradient descent step down the energy. This gradient descent step looks remarkably like a standard transformer block, complete with attention, MLP-like operations, layer normalizations, and residual connections.\nThe global energy combines two intuitive ideas: (1) attention energy that encourages masked tokens to align with relevant unmasked tokens, and (2) memory energy that ensures all tokens look like realistic patterns the model has learned. The gradient of each of these energies look like a self-attention and MLP, respectively, with some shared weight constraints.\nThis is one of those situations where the code ends up being significantly simpler than the equations. We write the equations for completeness, but feel free to skip to Section¬†2.3 for succinct code.\n\nAttention Energy\nWe describe the energy of a multi-headed attention with \\(H\\) heads, where the \\(h\\)-th head of attention is parameterized by \\(\\mathbf{W}_h^Q, \\mathbf{W}_h^K \\in \\mathbb{R}^{D \\times Y}\\), where \\(Y\\) is the ‚Äúhead dimension‚Äù. The input to the attention is the normalized token vectors \\(\\hat{\\mathbf{x}} \\in \\mathbb{R}^{N \\times D}\\). In the math that follows, we index the heads by \\(h=1\\ldots H\\), the head dimension by \\(\\alpha=1\\ldots Y\\), tokens by \\(A,B,C=1 \\ldots N\\), and each token vector by \\(i,j=1\\ldots D\\).\n\n\n\n\n\n\nEinstein notation\n\n\n\nWe find it convenient to use Einstein notation for the math, since it maps 1:1 to the einops operations we‚Äôll use in the code. If you aren‚Äôt familiar with the notation, check out this awesome tutorial. But fair warning, the equations at first look pretty complicated with all the indices.\nOne tip for reading equations with lots of indices: you don‚Äôt need to remember the shape or order of tensors, just remember the meaning of the indices. The number of subscripts is the number of dimensions of the tensor, and the meaning of each dimension is captured in the index name. For example, let \\(B=1\\ldots N\\) index the token position in a sequence, and let \\(i=1\\ldots D\\) index into each token vector. \\(x_{Bi}\\) is an element of a 2-dimensional tensor capturing the sequence length \\(N\\) and token dimension \\(D\\). Transposes don‚Äôt have meaning since things are named, so \\(x_{Bi} = x_{iB}\\). So long as you know the index semantics, you can read always read the equation. Everything is just scalar multiplication and addition.\n\n\nThe familiar queries and keys are computed as normal linear transformations:\n\\[\n   \\begin{split}\n        K_{h \\alpha B} &= \\sum\\limits_j W^K_{h \\alpha j}\\; \\hat{x}_{Bj}, \\qquad \\mathbf{K} \\in \\mathbb{R}^{H \\times Y \\times N} \\\\\n        Q_{h \\alpha C} &= \\sum\\limits_j W^Q_{h \\alpha j}\\; \\hat{x}_{Cj}, \\qquad \\mathbf{Q} \\in \\mathbb{R}^{H \\times Y \\times N}\n    \\end{split}\n\\]\nOur familiar ‚Äúraw attention scores‚Äù (pre-softmax) are still the dot-product correlations between each query and key:\n\\[\nA_{hBC} = \\sum_{\\alpha} K_{h\\alpha B} Q_{h\\alpha C}\n\\]\nNow for the different part: we describe the energy of the attention as the negative log-sum-exp of the attention scores. We will use the \\(\\beta\\) as an inverse-temperature hyperparameter to scale the attention scores.\n\\[\nE^\\text{ATT} = -\\frac{1}{\\beta} \\sum_{h=1}^H \\sum_{C=1}^N \\log \\left( \\sum_{B \\neq C} \\exp(\\beta A_{hBC}) \\right)\n\\tag{1}\\]\nAs we saw in a previous notebook, the negative log-sum-exp is an exponential variation of the Dense Associative Memory. The cool thing is that the gradient of the negative log-sum-exp is the softmax, which is what we‚Äôd like to see in the attention update rule.\n\n\n\n\n\n\nWhere are our values?\n\n\n\nYou may recall that traditional attention also has a value matrix. When we take the gradient of Equation¬†1, we lose the flexibility to include an independently parameterized values: the values must be a function of the queries and the keys.\n\n\n\n\nMemory Energy\nIn traditional transformers, the MLP (without biases) can be written as a two-layer feedforward network with a ReLU on the hidden activations. The MLP is parameterized by two weight matrices \\(\\mathbf{V}, \\mathbf{W} \\in \\mathbb{R}^{M \\times D}\\) where \\(M\\) is the size of the hidden layer (\\(M=4D\\) is often viewed as the default expansion factor atop token dimension \\(D\\)). Let‚Äôs again use Einstein notation, where \\(\\mu=1\\ldots M\\) indexes the hidden units, \\(i,j=1\\ldots D\\) index the token dimensions, and \\(B=1\\ldots N\\) indexes each token.\n\\[\n\\text{MLP}(\\hat{\\mathbf{x}})_{Bi} = \\sum_\\mu W_{\\mu i} \\; \\text{ReLU}\\left(\\sum_j V_{\\mu j} \\hat{\\mathbf{x}}_{Bj}\\right)\n\\tag{2}\\]\nIf we assume weight sharing between \\(\\mathbf{V} = \\mathbf{W} = \\boldsymbol{\\xi}\\), this is a gradient descent step down the energy of a Hopfield Network\n\\[\nE^{\\text{HN}}(\\hat{\\mathbf{x}}) = - \\sum_{B, \\mu} F\\left(\\sum_j \\xi_{\\mu j} \\hat{\\mathbf{x}}_{Bj}\\right)\n\\]\nwith rectified quadratic energy \\(F(\\cdot) := \\frac12 \\text{ReLU}(\\cdot)^2\\). If we say \\(f(\\cdot) := F'(\\cdot) = \\text{ReLU}(\\cdot)\\), the negative gradient of the energy is\n\\[\n-\\frac{\\partial E^{\\text{HN}}(\\mathbf{\\hat{x}})}{\\partial \\hat{x}_{Bi}}\n= \\sum_\\mu \\xi_{\\mu i} \\; f\\left(\\sum_j \\xi_{\\mu j} \\hat{\\mathbf{x}}_{Bj}\\right),\n\\]\nwhich is identical to the MLP operation in Equation¬†2 with a weight sharing constraint.\n\n\n\n\n\n\nNote\n\n\n\nIt is perfectly reasonable to consider other convex functions \\(F\\) for use in the energy. Polynomials of higher degree \\(n\\) or exponential functions are both valid and will yield Dense Associative Memory. However, because traditional Transformers use a ReLU activation, we use a rectified quadratic energy.\n\n\n\n\nET in code\nLet‚Äôs implement the attention energy in code. We will use jax and equinox for our code.\n\n\nNecessary imports\nimport jax, jax.numpy as jnp, jax.random as jr, jax.tree_util as jtu, jax.lax as lax\nimport equinox as eqx\nfrom dataclasses import dataclass\nfrom typing import *\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom fastcore.basics import *\nfrom fastcore.meta import *\nimport matplotlib.pyplot as plt\nfrom jaxtyping import Float, Array\nimport functools as ft\nfrom nbdev import show_doc\nfrom einops import rearrange\n\n\nThe EnergyTransformer class captures all the token processing in the entire transformer. There are maybe 7 lines of code that perform the actual energy computation. This single energy function, when paired with a layer-norm, is analogous to the full computation across all layers of a traditional transformer. The only things missing are some some token and position embedding matrices to make it work on real data, but we will do that in the following section.\n\nclass ETConfig(eqx.Module):\n  D: int = 768 # token dimension\n  H: int = 12 # number of heads\n  Y: int = 64 # head dimension\n  M: int = 3072 # MLP size\n  beta: Optional[float] = None # Inverse temperature for attention, defaults to 1/sqrt(Y)\n  def get_beta(self): return self.beta or 1/jnp.sqrt(self.Y)\n\nsmallETConfig = ETConfig(D=12, H=2, Y=6, M=24)\nmediumETConfig = ETConfig(D=128, H=4, Y=32, M=256)\nfullETConfig = ETConfig(D=768, H=12, Y=64, M=3072, beta=1/jnp.sqrt(64))\n\nclass EnergyTransformer(eqx.Module):\n  config: ETConfig\n  Wq: Float[Array, \"H D Y\"] # Query projection\n  Wk: Float[Array, \"H D Y\"] # Key projection\n  Xi: Float[Array, \"M D\"]\n\n  def __init__(self, key, config: ETConfig=fullETConfig):\n    self.config = config\n    key1, key2, key3 = jr.split(key, 3)\n    self.Wq = jr.normal(key1, (config.H, config.D, config.Y)) / config.Y\n    self.Wk = jr.normal(key2, (config.H, config.D, config.Y)) / config.Y\n    self.Xi = jr.normal(key3, (config.M, config.D))\n\n  def attn_energy(self, xhat: Float[Array, \"N D\"]):\n    beta = self.config.get_beta()\n    K = jnp.einsum(\"kd,hdy-&gt;khy\", xhat, self.Wk)\n    Q = jnp.einsum(\"qd,hdy-&gt;qhy\", xhat, self.Wq)\n    A = jax.nn.logsumexp(beta * jnp.einsum(\"khy,qhy-&gt;hqk\", Q, K), -1)\n    return -1/beta * A.sum()\n  \n  def hn_energy(self, xhat: Float[Array, \"N D\"]):\n    \"ReLU energy of a Hopfield Network\"\n    hid = jnp.einsum(\"nd,md-&gt;nm\", xhat, self.Xi)\n    return -0.5 * (hid.clip(0) ** 2).sum()\n  \n  def energy(self, xhat: Float[Array, \"N D\"]):\n    \"Total energy of the Energy Transformer\"\n    return self.attn_energy(xhat) + self.hn_energy(xhat)\n\nNote that the xhat inputs above are all layer-normalized tokens. However, like other AMs, we restrict ourselves to using non-linearties that are gradients of a convex Lagrangian function. We will just show this in code, but our ‚Äúspecial layernorm‚Äù is the same as the standard layer normalization except that we need our learnable scale parameter to be a scalar instead of a vector of shape D.\n\nclass EnergyLayerNorm(eqx.Module):\n  \"\"\"Define our primary activation function (modified LayerNorm) as a lagrangian with energy\"\"\"\n  gamma: Float[Array, \"\"]  # Scaling scalar\n  delta: Float[Array, \"D\"] # Bias per token\n  use_bias: bool = False\n  eps: float = 1e-5\n    \n  def lagrangian(self, x):\n    \"\"\"Integral of the standard LayerNorm\"\"\"\n    D = x.shape[-1]\n    xmeaned = x - x.mean(-1, keepdims=True)\n    t1 = D * self.gamma * jnp.sqrt((1 / D * xmeaned**2).sum() + self.eps)\n    if not self.use_bias: return t1\n    t2 = (self.delta * x).sum()\n    return t1 + t2\n\n  def __call__(self, x):\n    \"\"\"LayerNorm. The derivative of the Lagrangian\"\"\"\n    xmeaned = x - x.mean(-1, keepdims=True)\n    v = self.gamma * (xmeaned) / jnp.sqrt((xmeaned**2).mean(-1, keepdims=True)+ self.eps)\n    if self.use_bias: return v + self.delta\n    return v\n    \n  def energy(self, x):\n    \"\"\"Compute the energy of this Lagrangian through the Legendre Transform\"\"\"\n    return (self(x) * x).sum() - self.lagrangian(x)\n\nThat‚Äôs it! We rely on autograd to do the energy minimization, or the ‚Äúinference‚Äù pass through the entire transformer.\nLet‚Äôs check that the energies of both attention and memory monotonically decreases and is bounded from below.\n\nkey = jr.PRNGKey(11)\net = EnergyTransformer(key, config=smallETConfig)\nlnorm = EnergyLayerNorm(gamma=1., delta=jnp.zeros(et.config.D))\n\ndef energy_recall(Efn, x_init, nsteps, step_size):\n  \"Simple gradient descent to recall a memory\"\n  @jax.jit\n  def gd_step(x, i):\n      energy, grad = jax.value_and_grad(Efn)(lnorm(x))\n      x_next = x - step_size * grad\n      return x_next, energy\n\n  xhat_init = lnorm(x_init)\n  final_x, energy_history = jax.lax.scan(\n      gd_step,\n      xhat_init,\n      jnp.arange(nsteps)\n  )\n  return final_x, energy_history\n\nx_init = jr.normal(key, (100, et.config.D)) # Layer normalized tokens\nfinal_x, energy_history = energy_recall(et.energy, x_init, nsteps=3000, step_size=0.5)\n\n\n\n\n\n\n\n\n\nFigure¬†2: Energy descent for the Energy Transformer.",
    "crumbs": [
      "tutorial",
      "Energy Transformer"
    ]
  },
  {
    "objectID": "tutorial/energy_transformer.html#inference-with-an-energy-transformer",
    "href": "tutorial/energy_transformer.html#inference-with-an-energy-transformer",
    "title": "Energy Transformer",
    "section": "Inference with an Energy Transformer",
    "text": "Inference with an Energy Transformer\nTo make the Energy Transformer described above work on real data, we need to add some necessary addendums to work with image data: the token and position embedding matrices, and some data processing code.\n\nLoading data\nEnergy Transformer was originally trained on ImageNet. We will load some validation images of the same expected shape of ImageNet to test the performance of ET.\n\n\nPatching images\nWe build a Patcher class to patchify and unpatchify images, which is mostly a simple wrapper around the rearrange function from einops.\n\n\nPatcher class\nclass Patcher(eqx.Module):\n  \"Patchify and unpatchify an image.\"\n  image_shape: Iterable[int] # (C, H, W) Image shape\n  patch_size: int # Square patch size\n  kh: int # Number of patches in the height direction\n  kw: int # Number of patches in the width direction\n\n  @property\n  def patch_shape(self): return (self.image_shape[0], self.patch_size, self.patch_size)\n\n  @property\n  def num_patch_elements(self): return ft.reduce(lambda a, b=1: a * b, self.patch_shape)\n\n  @property\n  def num_patches(self): return self.kh * self.kw\n\n  def patchify(self, img):\n    \"Turn an image (possibly batched) into a collection of patches.\"\n    return rearrange(\n      img,\n      \"... c (kh h) (kw w)-&gt; ... (kh kw) c h w\",\n      h=self.patch_size,\n      w=self.patch_size,\n    )\n\n  def unpatchify(self, patches):\n    \"Turn a collection of patches (possibly batched) back into an image.\"\n    return rearrange(\n      patches, \"... (kh kw) c h w -&gt; ... c (kh h) (kw w)\", kh=self.kh, kw=self.kw\n    )\n\n  def patchified_shape(self):\n    \"The expected shape of a patchified image\"\n    return (self.num_patches, *self.patch_shape)\n\n  @classmethod\n  def from_img(cls, img, patch_size):\n    \"Create a Patcher from an example image.\"\n    return cls.from_img_shape(img.shape, patch_size)\n\n  @classmethod\n  def from_img_shape(cls, img_shape, patch_size):\n    \"Create a patcher from a specified image shape.\"\n    height, width = img_shape[-2:]\n    assert (height % patch_size) == 0\n    assert (width % patch_size) == 0\n    kh = int(height / patch_size)\n    kw = int(width / patch_size)\n    return cls(img_shape, patch_size, kh, kw)\n\n\nIt lets us do things like:\n\n# show_doc(Patcher.patchify)\n\n\n# show_doc(Patcher.unpatchify)\n\n\n\nImage-compatible ET\nLet‚Äôs create a full ET, complete with embeddings, model that can be used for masked-image inpainting. We say that each image has \\(N\\) total patches/tokens, where each patch as \\(Z = c \\times h \\times w\\) pixels when rasterized. We will use linear embeddings (with biases) to embed and unembed rasterized image patches to tokens.\nFor interoperability with the original ViT (Dosovitskiy et al. 2020), we will add a CLS token and a MASK token as parameters to the model. Again, most of this code is initializing parameters.\n\nclass ImageETConfig(eqx.Module):\n  image_shape: Tuple[int, int, int] = (3, 224, 224) # (C, H, W) Image shape\n  patch_size: int = 16 # Square patch size\n  et_conf: ETConfig = fullETConfig\n\nclass ImageEnergyTransformer(eqx.Module):\n  patcher: Patcher\n  W_emb: Float[Array, \"Z D\"]\n  b_emb: Float[Array, \"D\"]\n  W_unemb: Float[Array, \"D Z\"]\n  b_unemb: Float[Array, \"Z\"]\n\n  pos_embed: Float[Array, \"(N+1) D\"] # Don't forget the CLS token!\n  cls_token: jax.Array\n  mask_token: jax.Array\n  et: EnergyTransformer\n  lnorm: EnergyLayerNorm\n\n  config: ImageETConfig\n\n  def encode(self, x):\n    \"Turn x from img patches to tokens\"\n    x = self.patcher.patchify(x) # (..., N, Z)\n    return x @ self.W_emb + self.b_emb # (..., N, D)\n\n  def decode(self, x):\n    \"Turn x from tokens to img patches\"\n    x = self.lnorm(x) # (..., N, D)\n    x = x @ self.W_unemb + self.b_unemb # (..., N, Z)\n    return self.patcher.unpatchify(x) # (..., C, H, W)\n\n  def corrupt_tokens(self, x: jax.Array, mask: jax.Array, n_masked: int=100):\n    \"\"\"Corrupt tokens with MASK tokens wherever `mask` is 1.\n\n    `n_masked` needs to be known in advance for JAX JIT to work properly\n    \"\"\"\n    maskmask = jnp.nonzero(mask == 1, size=n_masked, fill_value=0)\n    return x.at[maskmask].set(self.mask_token) # (..., N, D)\n\n  def prep_tokens(self, x, mask):\n    \"Add CLS+MASK tokens and POS embeddings\"\n    x = self.corrupt_tokens(x, mask)\n    x = jnp.concatenate([self.cls_token[None], x]) # (..., N+1, D)\n    return x + self.pos_embed # (..., N+1, D)\n\n  @delegates(energy_recall)\n  def __call__(self, x: jnp.ndarray, mask: jnp.ndarray, **kwargs):\n    \"\"\"A complete pipeline for masked image modeling in ET using `energy_recall`\n    \n    `kwargs` are passed to `energy_recall`\n    \"\"\"\n    x = self.patcher.patchify(x)\n    x = self.encode(x)\n    x = self.prep_tokens(x, mask)  # (..., N+1, D)\n\n    final_x, energy_history = energy_recall(self.et.energy, x, **kwargs)\n\n    x = x[1:]  # Discard CLS token for masked inpainting\n    xhat = self.lnorm(x)\n    x = self.decode(xhat)\n    return self.patcher.unpatchify(x) # (..., C, H, W)\n\n  @classmethod\n  def rand_init(cls, key, config=ImageETConfig()):\n    key1, key2, key3, key4, key5, key6, key7, key8 = jr.split(key, 8)\n    patcher = Patcher.from_img_shape(config.image_shape, config.patch_size)\n    W_emb = jr.normal(key1, (patcher.num_patch_elements, config.et_conf.D)) / config.et_conf.D\n    b_emb = jr.normal(key2, (config.et_conf.D,))\n    W_unemb = jr.normal(key3, (config.et_conf.D, patcher.num_patch_elements)) / patcher.num_patch_elements\n    b_unemb = jr.normal(key4, (patcher.num_patch_elements,))\n    pos_embed = jr.normal(key5, (patcher.num_patches, config.et_conf.D)) / config.et_conf.D\n    cls_token = 0.002 * jr.normal(key6, (config.et_conf.D,))\n    mask_token = 0.002 * jr.normal(key7, (config.et_conf.D,))\n    pos_embed = 0.002 * jr.normal(key8, (1 + patcher.num_patches, config.et_conf.D)) / config.et_conf.D\n\n    return cls(\n      patcher=patcher,\n      W_emb=W_emb,\n      b_emb=b_emb,\n      W_unemb=W_unemb,\n      b_unemb=b_unemb,\n      pos_embed=pos_embed,\n      cls_token=cls_token,\n      mask_token=mask_token,\n      et=EnergyTransformer(key7, config.et_conf),\n      lnorm=EnergyLayerNorm(gamma=1., delta=jnp.zeros(config.et_conf.D)),\n      config=config\n    )\n\n\nimageET = ImageEnergyTransformer.rand_init(key, ImageETConfig())\n\nET has publicly available pretrained weights that can be used for masked-image inpainting. Let‚Äôs load those weights into our model.",
    "crumbs": [
      "tutorial",
      "Energy Transformer"
    ]
  },
  {
    "objectID": "tutorial/energy_transformer.html#training-an-energy-transformer",
    "href": "tutorial/energy_transformer.html#training-an-energy-transformer",
    "title": "Energy Transformer",
    "section": "Training an Energy Transformer",
    "text": "Training an Energy Transformer\nWe train ET on a simple dataset, should work on CPU.\nThe Transformer is a very flexible computing paradigm that can be used for the two major approaches of modern language modeling: masked token prediction (e.g., BERT and diffusion-style transformers) where you predict the fraction of input tokens that are MASKed using information from the unmasked tokens, and autoregressive language modeling (e.g., GPT-style models), where each token in the input sequence is transformed into the next prediction token.",
    "crumbs": [
      "tutorial",
      "Energy Transformer"
    ]
  },
  {
    "objectID": "tutorial/distributed_memory.html",
    "href": "tutorial/distributed_memory.html",
    "title": "Distributed Memory",
    "section": "",
    "text": "Under Construction\n\n\n\n\n\n\n\ndef sin_cos_phi(x: Float[Array, \"... D\"], RF:Float[Array, \"Y D\"], beta: float) -&gt; Float[Array, \"... 2Y\"]:\n    \"\"\"The reigning champion, a basis function that uses sin and cos to encode the input into the feature space. \"\"\"\n    Y = RF.shape[0]\n    h = jnp.sqrt(beta) * (x @ RF.T)\n    return 1 / jnp.sqrt(Y) * jnp.concatenate( [ jnp.cos(h), jnp.sin(h)], axis=-1)\n\n\nclass DrDAM:\n    \"\"\"Defines the interface and basic methods for all KernelizableDAMs\"\"\"\n    def __init__(self, key, D, Y, beta):\n        self.RF = jr.normal(key, (Y, D))\n        self.beta = beta\n        self.Y = Y\n        self.Tdim = 2*Y\n        self.D = D\n\n    def phi(self, x: Float[Array, \"... D\"]) -&gt; Float[Array, \"... 2Y\"]:\n        \"\"\"Compute the basis function \"\"\"\n        return sin_cos_phi(x, self.RF, self.beta)\n\n    def sim(self, x: Float[Array, \"D\"], y: Float[Array, \"D\"]) -&gt; Float[Array, \"\"]:\n        \"\"\"Compute the standard L2 similarity between two vectors.\"\"\"\n        return jnp.exp(-self.beta / 2 * ((x - y) ** 2).sum())\n\n    def energy(\n        self, x: Float[Array, \"D\"], memories: Float[Array, \"M D\"]\n    ) -&gt; Float[Array, \"\"]:\n        \"\"\"Compute the standard L2 energy\"\"\"\n        return -(1 / self.beta) * jax.nn.logsumexp(\n            -self.beta / 2 * ((x - memories) ** 2).sum(-1), axis=0\n        )\n\n    def kernel_energy(\n        self, x: Float[Array, \"D\"], T: Float[Array, \"2Y\"], eps=1e-10\n    ) -&gt; Float[Array, \"\"]:\n        \"\"\"Compute the approximate kernelized energy\"\"\"\n        h = self.phi(x) @ T \n        h = jnp.clip(h,  a_min=eps)\n        return -(1 / self.beta) * jnp.log(h)\n    \n    def kernel_sim(\n        self, x: Float[Array, \"D\"], y: Float[Array, \"D\"]\n    ) -&gt; Float[Array, \"\"]:\n        \"\"\"Compute the approximate kernel similarity of `x` and `y`\"\"\"\n        return self.phi(x) @ self.phi(y)\n\n    def kernelize_memories(self, memories: Float[Array, \"M D\"], **kwargs) -&gt; Float[Array, \"2Y\"]:\n        \"\"\"\n        Naive implementation that BLOWS UP with many memories `n`,\n        since it creates the entire memory matrix from scratch\n        \"\"\"\n        return self.phi(memories).sum(0)\n\n    def recall( \n        self, q: Float[Array, \"D\"], memories: Float[Array, \"M D\"], \n        depth: int=1000, alpha: float = 0.1, return_grads=False, \n        clamp_idxs: Optional[Bool[Array, \"D\"]]=None\n    ) -&gt; Float[Array, \"D\"]: \n        \"\"\"Using the normal similarity function, run energy dynamics\"\"\"\n        dEdxf = jax.jit(jax.value_and_grad(self.energy))\n        logs = {}\n        def step(x, i):\n            E, dEdx = dEdxf(x, memories)\n            if clamp_idxs is not None:\n                dEdx = jnp.where(clamp_idxs, 0, dEdx)\n            x = x - alpha * dEdx\n            aux = (E, dEdx) if return_grads else (E,)\n            return x, aux\n        x, aux = jax.lax.scan(step, q, jnp.arange(depth))\n        logs['energies'] = aux[0]\n        if return_grads:\n            logs['grads'] = aux[1]\n        return x, logs\n\n    def kernel_recall(\n        self, q: Float[Array, \"D\"], T: Float[Array, \"2Y\"], \n        depth: int=1000, alpha: float = 0.1, return_grads=False,\n        clamp_idxs: Optional[Bool[Array, \"D\"]]=None\n    ) -&gt; Float[Array, \"D\"]: \n        \"\"\"Using the kernelized similarity function, run energy dynamics\"\"\"\n        dEdxf = jax.jit(jax.value_and_grad(self.kernel_energy))\n        logs = {}\n        @jax.jit\n        def step(x, i):\n            E, dEdx = dEdxf(x, T)\n            if clamp_idxs is not None:\n                dEdx = jnp.where(clamp_idxs, 0, dEdx)\n            x = x - alpha * dEdx\n            aux = (E, dEdx) if return_grads else (E,)\n            return x, aux\n        x, aux = jax.lax.scan(step, q, jnp.arange(depth))\n        logs['energies'] = aux[0]\n        if return_grads:\n            logs['grads'] = aux[1]\n        return x, logs\n\n\nrng = jr.PRNGKey(0)\nk1, k2, k3, k4, k5, rng = jr.split(rng, 6)\nD = 10\nY = 100\nn_memories = 20\nn_queries = 100\nbeta = 10\nkdam = DrDAM(k1, D=D, Y=Y, beta=beta)\n\n\nx = (jr.uniform(k2, (D,)) &gt; 0.5) / jnp.sqrt(D)\ny = (jr.uniform(k3, (D,)) &gt; 0.5) / jnp.sqrt(D)\nprint(x.shape)\nprint(y.shape)\n\n(10,)\n(10,)\n\n\n\nprint(kdam.sim(x, y))\nprint(kdam.kernel_sim(x, y))\n\n0.011108992\n0.006203072\n\n\n\nmemories = (jr.uniform(k4, (n_memories, D)) &gt; 0.5) / jnp.sqrt(D)\nqueries = (jr.uniform(k5, (n_queries, D)) &gt; 0.5) / jnp.sqrt(D)\nprint(queries.shape, memories.shape)\nT = kdam.kernelize_memories(memories)\nprint(T.shape)\n\n(100, 10) (20, 10)\n(200,)\n\n\n\nprint(kdam.energy(x, memories))\nprint(kdam.kernel_energy(x, T))\n\n-0.07309916\n-0.08676291\n\n\n\nexact_energies = jnp.array([kdam.energy(q, memories).item() for q in queries])\nkernelized_energies = kdam.kernel_energy(queries, T)\nplt.scatter(exact_energies, kernelized_energies)\nplt.xlabel(\"Exact\")\nplt.ylabel(\"Kernel\")\nplt.axis('square')\n# plt.gca().set_aspect('equal', 'box')\nplt.show()\n\n\n\n\n\n\n\n\n\nexact_out, _ = kdam.recall(queries[0], memories, depth=100, alpha=1)\nkernel_out, _ = kdam.kernel_recall(queries[0], T, depth=100, alpha=1)\nprint(jnp.abs(exact_out - kernel_out).sum())\n\n0.13190839",
    "crumbs": [
      "tutorial",
      "Distributed Memory"
    ]
  },
  {
    "objectID": "readme.html",
    "href": "readme.html",
    "title": "Getting started",
    "section": "",
    "text": "This website serves as a living companion to the tutorial manuscript (coming soon!) to be presented at ICML 2025. It dreams of being a one-stop shop for learning all things about Associative Memory. It‚Äôs definitely not there yet.\nSee the tutorials for a brief introduction to the list of example notebooks.",
    "crumbs": [
      "Getting started"
    ]
  },
  {
    "objectID": "readme.html#installation",
    "href": "readme.html#installation",
    "title": "Getting started",
    "section": "Installation",
    "text": "Installation\nWe have tried to streamline the installation of the repo as much as possible.\n\nPre-requisites\n\nInstall uv using curl -LsSf https://astral.sh/uv/install.sh | sh\nInstall quarto\nWe use conda (or better yet, mamba) for managing the ffmpeg dependency, which only matters if ffmpeg is not already installed on your system.\n\n\n\nSetting up the environment\nFrom the root of the repo:\nuv sync\nsource .venv/bin/activate\nuv run ipython kernel install --user --env VIRTUAL_ENV $(pwd)/.venv --name=amtutorial # Expose venv to ipython\n\n# OPTIONAL: For rendering videos in notebooks\nconda install conda-forge::ffmpeg conda-forge::openh264 \n\n# OPTIONAL: For developing the interactive frontend\nconda install conda-forge::nodejs\nnpm install --prefix javascript && npm run build --prefix javascript \nYou can view a local version of the website with\nuv run nbdev_preview\n\n\nWebsite structure\n.ipynb versions of the tutorial notebooks are located in tutorial_ipynbs. Setup the uv environment above to play with them locally, or run them in Google Colab.\n\n\n\n\n\n\nNote\n\n\n\nThe first time you run the notebooks will be slow. We cache some of the long-running code after the first time, but this will not persist across Colab sessions\n\n\n\nBinary Dense Storage Notebook\nEnergy Transformer Notebook\nDiffusion as Memory Notebook\nDistributed Associative Memory Notebook\n\nThe website () is built using an in-house fork of nbdev to allow developing everything (i.e., the tutorials, corresponding pip package, and documentation) using plain text representations of jupyter notebooks in .qmd files. The website preserves the folder-based routing in the nbs/ folder.\nWith the right extensions and hotkeys, .qmd files are pleasant to develop inside VSCode and interop seamlessly with both git and AI tooling.",
    "crumbs": [
      "Getting started"
    ]
  },
  {
    "objectID": "readme.html#deploying",
    "href": "readme.html#deploying",
    "title": "Getting started",
    "section": "Deploying",
    "text": "Deploying\nDeploy to tutorial.amemory.net by pushing commits to the main branch after building the site locally.\nuv run nbdev_export && uv run nbdev_docs && git add . && git commit -m \"Update site\" && git push",
    "crumbs": [
      "Getting started"
    ]
  },
  {
    "objectID": "lib/data_utils.html",
    "href": "lib/data_utils.html",
    "title": "data_utils",
    "section": "",
    "text": "We will use a cache directory to store the downloaded, processed data. This defaults to ~/.cache/amtutorial, but can be overridden by setting the AMTUTORIAL_CACHE_DIR environment variable.\n\n\n\n\n get_cache_dir (subfolder:Optional[str]=None)\n\nGet a cross-platform cache directory that works locally and in Google Colab.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsubfolder\nOptional\nNone\nSubdir of cache dir\n\n\nReturns\nPath\n\n\n\n\n\n\nprint(get_cache_dir())\n\n/Users/hoo/.cache/amtutorial\n\n\nLet‚Äôs make a data directory for the tutorial.\n\n\nExported source\nDATA_DIR = get_cache_dir(\"data\")\nMNIST_DIR = get_cache_dir(\"data/mnist\")\nPOKEMON_DIR = get_cache_dir(\"data/pokemon\")\nMODEL_DIR = get_cache_dir(\"models\")",
    "crumbs": [
      "lib",
      "data_utils"
    ]
  },
  {
    "objectID": "lib/data_utils.html#cache-directory",
    "href": "lib/data_utils.html#cache-directory",
    "title": "data_utils",
    "section": "",
    "text": "We will use a cache directory to store the downloaded, processed data. This defaults to ~/.cache/amtutorial, but can be overridden by setting the AMTUTORIAL_CACHE_DIR environment variable.\n\n\n\n\n get_cache_dir (subfolder:Optional[str]=None)\n\nGet a cross-platform cache directory that works locally and in Google Colab.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsubfolder\nOptional\nNone\nSubdir of cache dir\n\n\nReturns\nPath\n\n\n\n\n\n\nprint(get_cache_dir())\n\n/Users/hoo/.cache/amtutorial\n\n\nLet‚Äôs make a data directory for the tutorial.\n\n\nExported source\nDATA_DIR = get_cache_dir(\"data\")\nMNIST_DIR = get_cache_dir(\"data/mnist\")\nPOKEMON_DIR = get_cache_dir(\"data/pokemon\")\nMODEL_DIR = get_cache_dir(\"models\")",
    "crumbs": [
      "lib",
      "data_utils"
    ]
  },
  {
    "objectID": "lib/data_utils.html#preparing-bipolar-mnist-data",
    "href": "lib/data_utils.html#preparing-bipolar-mnist-data",
    "title": "data_utils",
    "section": "Preparing bipolar MNIST data",
    "text": "Preparing bipolar MNIST data\n\n\nprepare_bipolar_mnist\n\n prepare_bipolar_mnist (force=False)\n\nDownload and prepare the MNIST dataset\n\n\n\nload_bipolar_mnist\n\n load_bipolar_mnist ()\n\nLoad the MNIST dataset\n\nXtrain, Xtest, Ytrain, Ytest = load_bipolar_mnist()\n\nassert Xtest.shape[-2:] == (28, 28)\nassert Xtest.min() == -1 and Xtest.max() == 1\nassert set(np.unique(Xtest)) == set([-1., 1.])",
    "crumbs": [
      "lib",
      "data_utils"
    ]
  },
  {
    "objectID": "lib/data_utils.html#downloading-pokemon-sprites",
    "href": "lib/data_utils.html#downloading-pokemon-sprites",
    "title": "data_utils",
    "section": "Downloading Pokemon Sprites",
    "text": "Downloading Pokemon Sprites\nFor cutesies and funsies, let‚Äôs build our own black and white pokemon dataset from the Pokemon Database.\n\n\ndownload_pokemon_sprites\n\n download_pokemon_sprites (delay:float=0.1, verbose:bool=True)\n\nDownload Pokemon sprite images from pokemondb.net\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndelay\nfloat\n0.1\nDelay between downloads (seconds) to be nice to server\n\n\nverbose\nbool\nTrue\nPrint progress messages",
    "crumbs": [
      "lib",
      "data_utils"
    ]
  },
  {
    "objectID": "lib/data_utils.html#processing-pokemon-sprites",
    "href": "lib/data_utils.html#processing-pokemon-sprites",
    "title": "data_utils",
    "section": "Processing Pokemon Sprites",
    "text": "Processing Pokemon Sprites\nWith a little help from AI, we can process our Pokemon sprites to be binary.\n\n\nscale_to_fit\n\n scale_to_fit (img, target_size)\n\nScale image to fit within target_size while maintaining aspect ratio\n\n\n\nget_sprite_bbox\n\n get_sprite_bbox (img)\n\nGet bounding box of non-transparent pixels\n\n\n\nfloyd_steinberg_dither\n\n floyd_steinberg_dither (gray, strength=1.0)\n\nApply Floyd-Steinberg dithering with adjustable strength\n\n\n\nbinarize_image\n\n binarize_image (img, method='adaptive', invert=False,\n                 dither_strength=1.0)\n\nApply binarization to convert image to black and white\n\n\n\nprocess_pokemon_sprites\n\n process_pokemon_sprites (ds_name:str, canvas_size:int=48,\n                          sprite_percentage:float=0.95,\n                          binarize_method:str='adaptive',\n                          dither_strength:float=1.0,\n                          invert_colors:bool=False, verbose:bool=True,\n                          force:bool=False)\n\n*Process Pokemon sprites: upscale non-transparent pixels, convert to white bg, resize to specified dimensions\nThis script processes Pokemon sprite images by: 1. Cropping excess transparency around sprites 2. Scaling sprites to occupy a specified percentage of the canvas 3. Converting transparent backgrounds to white 4. Optionally converting to black and white using adaptive thresholding or dithering 5. Optionally inverting the binary result (black background, white sprites) 6. Centering sprites on a square canvas*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nds_name\nstr\n\nDataset name\n\n\ncanvas_size\nint\n48\nFinal image size\n\n\nsprite_percentage\nfloat\n0.95\nPct of canvas the sprite should occupy (0.0-1.0)\n\n\nbinarize_method\nstr\nadaptive\nBinarization method: ‚Äòadaptive‚Äô or ‚Äòdithered‚Äô\n\n\ndither_strength\nfloat\n1.0\nDithering intensity (0.0=none, 1.0=full Floyd-Steinberg)\n\n\ninvert_colors\nbool\nFalse\nInvert binary colors (only works with binarize=True)\n\n\nverbose\nbool\nTrue\nPrint progress messages\n\n\nforce\nbool\nFalse\nForce re-processing\n\n\n\n\n\n\ndatafy_pokemon_sprites\n\n datafy_pokemon_sprites (ds_name:str)\n\nConvert Pokemon sprites to numpy arrays and save filenames\n\ndownload_pokemon_sprites()\nprocess_pokemon_sprites(\"processed_adaptive\", binarize_method='adaptive') # Best\npoke_pixels_path, poke_names_path = datafy_pokemon_sprites(\"processed_adaptive\")\n\nLet‚Äôs load the data and take a look at it.\n\n\n\nload_bipolar_pokemon_sprites\n\n load_bipolar_pokemon_sprites (ds_name:str='processed_adaptive',\n                               binarize_method:str='adaptive')\n\nLoad Pokemon sprites from numpy arrays\n\n\nFetching sprite page from: https://pokemondb.net/sprites\nFound 1025 Pokemon sprites to download\nUsing 0.1s delay between downloads\nSkipping 1/1025: bulbasaur.png (already exists)\nSkipping 2/1025: ivysaur.png (already exists)\nSkipping 3/1025: venusaur.png (already exists)\nSkipping 4/1025: charmander.png (already exists)\nSkipping 5/1025: charmeleon.png (already exists)\nSkipping 6/1025: charizard.png (already exists)\nSkipping 7/1025: squirtle.png (already exists)\nSkipping 8/1025: wartortle.png (already exists)\nSkipping 9/1025: blastoise.png (already exists)\nSkipping 10/1025: caterpie.png (already exists)\nSkipping 11/1025: metapod.png (already exists)\nSkipping 12/1025: butterfree.png (already exists)\nSkipping 13/1025: weedle.png (already exists)\nSkipping 14/1025: kakuna.png (already exists)\nSkipping 15/1025: beedrill.png (already exists)\nSkipping 16/1025: pidgey.png (already exists)\nSkipping 17/1025: pidgeotto.png (already exists)\nSkipping 18/1025: pidgeot.png (already exists)\nSkipping 19/1025: rattata.png (already exists)\nSkipping 20/1025: raticate.png (already exists)\nSkipping 21/1025: spearow.png (already exists)\nSkipping 22/1025: fearow.png (already exists)\nSkipping 23/1025: ekans.png (already exists)\nSkipping 24/1025: arbok.png (already exists)\nSkipping 25/1025: pikachu.png (already exists)\nSkipping 26/1025: raichu.png (already exists)\nSkipping 27/1025: sandshrew.png (already exists)\nSkipping 28/1025: sandslash.png (already exists)\nSkipping 29/1025: nidoran-f.png (already exists)\nSkipping 30/1025: nidorina.png (already exists)\nSkipping 31/1025: nidoqueen.png (already exists)\nSkipping 32/1025: nidoran-m.png (already exists)\nSkipping 33/1025: nidorino.png (already exists)\nSkipping 34/1025: nidoking.png (already exists)\nSkipping 35/1025: clefairy.png (already exists)\nSkipping 36/1025: clefable.png (already exists)\nSkipping 37/1025: vulpix.png (already exists)\nSkipping 38/1025: ninetales.png (already exists)\nSkipping 39/1025: jigglypuff.png (already exists)\nSkipping 40/1025: wigglytuff.png (already exists)\nSkipping 41/1025: zubat.png (already exists)\nSkipping 42/1025: golbat.png (already exists)\nSkipping 43/1025: oddish.png (already exists)\nSkipping 44/1025: gloom.png (already exists)\nSkipping 45/1025: vileplume.png (already exists)\nSkipping 46/1025: paras.png (already exists)\nSkipping 47/1025: parasect.png (already exists)\nSkipping 48/1025: venonat.png (already exists)\nSkipping 49/1025: venomoth.png (already exists)\nSkipping 50/1025: diglett.png (already exists)\nSkipping 51/1025: dugtrio.png (already exists)\nSkipping 52/1025: meowth.png (already exists)\nSkipping 53/1025: persian.png (already exists)\nSkipping 54/1025: psyduck.png (already exists)\nSkipping 55/1025: golduck.png (already exists)\nSkipping 56/1025: mankey.png (already exists)\nSkipping 57/1025: primeape.png (already exists)\nSkipping 58/1025: growlithe.png (already exists)\nSkipping 59/1025: arcanine.png (already exists)\nSkipping 60/1025: poliwag.png (already exists)\nSkipping 61/1025: poliwhirl.png (already exists)\nSkipping 62/1025: poliwrath.png (already exists)\nSkipping 63/1025: abra.png (already exists)\nSkipping 64/1025: kadabra.png (already exists)\nSkipping 65/1025: alakazam.png (already exists)\nSkipping 66/1025: machop.png (already exists)\nSkipping 67/1025: machoke.png (already exists)\nSkipping 68/1025: machamp.png (already exists)\nSkipping 69/1025: bellsprout.png (already exists)\nSkipping 70/1025: weepinbell.png (already exists)\nSkipping 71/1025: victreebel.png (already exists)\nSkipping 72/1025: tentacool.png (already exists)\nSkipping 73/1025: tentacruel.png (already exists)\nSkipping 74/1025: geodude.png (already exists)\nSkipping 75/1025: graveler.png (already exists)\nSkipping 76/1025: golem.png (already exists)\nSkipping 77/1025: ponyta.png (already exists)\nSkipping 78/1025: rapidash.png (already exists)\nSkipping 79/1025: slowpoke.png (already exists)\nSkipping 80/1025: slowbro.png (already exists)\nSkipping 81/1025: magnemite.png (already exists)\nSkipping 82/1025: magneton.png (already exists)\nSkipping 83/1025: farfetchd.png (already exists)\nSkipping 84/1025: doduo.png (already exists)\nSkipping 85/1025: dodrio.png (already exists)\nSkipping 86/1025: seel.png (already exists)\nSkipping 87/1025: dewgong.png (already exists)\nSkipping 88/1025: grimer.png (already exists)\nSkipping 89/1025: muk.png (already exists)\nSkipping 90/1025: shellder.png (already exists)\nSkipping 91/1025: cloyster.png (already exists)\nSkipping 92/1025: gastly.png (already exists)\nSkipping 93/1025: haunter.png (already exists)\nSkipping 94/1025: gengar.png (already exists)\nSkipping 95/1025: onix.png (already exists)\nSkipping 96/1025: drowzee.png (already exists)\nSkipping 97/1025: hypno.png (already exists)\nSkipping 98/1025: krabby.png (already exists)\nSkipping 99/1025: kingler.png (already exists)\nSkipping 100/1025: voltorb.png (already exists)\nSkipping 101/1025: electrode.png (already exists)\nSkipping 102/1025: exeggcute.png (already exists)\nSkipping 103/1025: exeggutor.png (already exists)\nSkipping 104/1025: cubone.png (already exists)\nSkipping 105/1025: marowak.png (already exists)\nSkipping 106/1025: hitmonlee.png (already exists)\nSkipping 107/1025: hitmonchan.png (already exists)\nSkipping 108/1025: lickitung.png (already exists)\nSkipping 109/1025: koffing.png (already exists)\nSkipping 110/1025: weezing.png (already exists)\nSkipping 111/1025: rhyhorn.png (already exists)\nSkipping 112/1025: rhydon.png (already exists)\nSkipping 113/1025: chansey.png (already exists)\nSkipping 114/1025: tangela.png (already exists)\nSkipping 115/1025: kangaskhan.png (already exists)\nSkipping 116/1025: horsea.png (already exists)\nSkipping 117/1025: seadra.png (already exists)\nSkipping 118/1025: goldeen.png (already exists)\nSkipping 119/1025: seaking.png (already exists)\nSkipping 120/1025: staryu.png (already exists)\nSkipping 121/1025: starmie.png (already exists)\nSkipping 122/1025: mr-mime.png (already exists)\nSkipping 123/1025: scyther.png (already exists)\nSkipping 124/1025: jynx.png (already exists)\nSkipping 125/1025: electabuzz.png (already exists)\nSkipping 126/1025: magmar.png (already exists)\nSkipping 127/1025: pinsir.png (already exists)\nSkipping 128/1025: tauros.png (already exists)\nSkipping 129/1025: magikarp.png (already exists)\nSkipping 130/1025: gyarados.png (already exists)\nSkipping 131/1025: lapras.png (already exists)\nSkipping 132/1025: ditto.png (already exists)\nSkipping 133/1025: eevee.png (already exists)\nSkipping 134/1025: vaporeon.png (already exists)\nSkipping 135/1025: jolteon.png (already exists)\nSkipping 136/1025: flareon.png (already exists)\nSkipping 137/1025: porygon.png (already exists)\nSkipping 138/1025: omanyte.png (already exists)\nSkipping 139/1025: omastar.png (already exists)\nSkipping 140/1025: kabuto.png (already exists)\nSkipping 141/1025: kabutops.png (already exists)\nSkipping 142/1025: aerodactyl.png (already exists)\nSkipping 143/1025: snorlax.png (already exists)\nSkipping 144/1025: articuno.png (already exists)\nSkipping 145/1025: zapdos.png (already exists)\nSkipping 146/1025: moltres.png (already exists)\nSkipping 147/1025: dratini.png (already exists)\nSkipping 148/1025: dragonair.png (already exists)\nSkipping 149/1025: dragonite.png (already exists)\nSkipping 150/1025: mewtwo.png (already exists)\nSkipping 151/1025: mew.png (already exists)\nSkipping 152/1025: chikorita.png (already exists)\nSkipping 153/1025: bayleef.png (already exists)\nSkipping 154/1025: meganium.png (already exists)\nSkipping 155/1025: cyndaquil.png (already exists)\nSkipping 156/1025: quilava.png (already exists)\nSkipping 157/1025: typhlosion.png (already exists)\nSkipping 158/1025: totodile.png (already exists)\nSkipping 159/1025: croconaw.png (already exists)\nSkipping 160/1025: feraligatr.png (already exists)\nSkipping 161/1025: sentret.png (already exists)\nSkipping 162/1025: furret.png (already exists)\nSkipping 163/1025: hoothoot.png (already exists)\nSkipping 164/1025: noctowl.png (already exists)\nSkipping 165/1025: ledyba.png (already exists)\nSkipping 166/1025: ledian.png (already exists)\nSkipping 167/1025: spinarak.png (already exists)\nSkipping 168/1025: ariados.png (already exists)\nSkipping 169/1025: crobat.png (already exists)\nSkipping 170/1025: chinchou.png (already exists)\nSkipping 171/1025: lanturn.png (already exists)\nSkipping 172/1025: pichu.png (already exists)\nSkipping 173/1025: cleffa.png (already exists)\nSkipping 174/1025: igglybuff.png (already exists)\nSkipping 175/1025: togepi.png (already exists)\nSkipping 176/1025: togetic.png (already exists)\nSkipping 177/1025: natu.png (already exists)\nSkipping 178/1025: xatu.png (already exists)\nSkipping 179/1025: mareep.png (already exists)\nSkipping 180/1025: flaaffy.png (already exists)\nSkipping 181/1025: ampharos.png (already exists)\nSkipping 182/1025: bellossom.png (already exists)\nSkipping 183/1025: marill.png (already exists)\nSkipping 184/1025: azumarill.png (already exists)\nSkipping 185/1025: sudowoodo.png (already exists)\nSkipping 186/1025: politoed.png (already exists)\nSkipping 187/1025: hoppip.png (already exists)\nSkipping 188/1025: skiploom.png (already exists)\nSkipping 189/1025: jumpluff.png (already exists)\nSkipping 190/1025: aipom.png (already exists)\nSkipping 191/1025: sunkern.png (already exists)\nSkipping 192/1025: sunflora.png (already exists)\nSkipping 193/1025: yanma.png (already exists)\nSkipping 194/1025: wooper.png (already exists)\nSkipping 195/1025: quagsire.png (already exists)\nSkipping 196/1025: espeon.png (already exists)\nSkipping 197/1025: umbreon.png (already exists)\nSkipping 198/1025: murkrow.png (already exists)\nSkipping 199/1025: slowking.png (already exists)\nSkipping 200/1025: misdreavus.png (already exists)\nSkipping 201/1025: unown.png (already exists)\nSkipping 202/1025: wobbuffet.png (already exists)\nSkipping 203/1025: girafarig.png (already exists)\nSkipping 204/1025: pineco.png (already exists)\nSkipping 205/1025: forretress.png (already exists)\nSkipping 206/1025: dunsparce.png (already exists)\nSkipping 207/1025: gligar.png (already exists)\nSkipping 208/1025: steelix.png (already exists)\nSkipping 209/1025: snubbull.png (already exists)\nSkipping 210/1025: granbull.png (already exists)\nSkipping 211/1025: qwilfish.png (already exists)\nSkipping 212/1025: scizor.png (already exists)\nSkipping 213/1025: shuckle.png (already exists)\nSkipping 214/1025: heracross.png (already exists)\nSkipping 215/1025: sneasel.png (already exists)\nSkipping 216/1025: teddiursa.png (already exists)\nSkipping 217/1025: ursaring.png (already exists)\nSkipping 218/1025: slugma.png (already exists)\nSkipping 219/1025: magcargo.png (already exists)\nSkipping 220/1025: swinub.png (already exists)\nSkipping 221/1025: piloswine.png (already exists)\nSkipping 222/1025: corsola.png (already exists)\nSkipping 223/1025: remoraid.png (already exists)\nSkipping 224/1025: octillery.png (already exists)\nSkipping 225/1025: delibird.png (already exists)\nSkipping 226/1025: mantine.png (already exists)\nSkipping 227/1025: skarmory.png (already exists)\nSkipping 228/1025: houndour.png (already exists)\nSkipping 229/1025: houndoom.png (already exists)\nSkipping 230/1025: kingdra.png (already exists)\nSkipping 231/1025: phanpy.png (already exists)\nSkipping 232/1025: donphan.png (already exists)\nSkipping 233/1025: porygon2.png (already exists)\nSkipping 234/1025: stantler.png (already exists)\nSkipping 235/1025: smeargle.png (already exists)\nSkipping 236/1025: tyrogue.png (already exists)\nSkipping 237/1025: hitmontop.png (already exists)\nSkipping 238/1025: smoochum.png (already exists)\nSkipping 239/1025: elekid.png (already exists)\nSkipping 240/1025: magby.png (already exists)\nSkipping 241/1025: miltank.png (already exists)\nSkipping 242/1025: blissey.png (already exists)\nSkipping 243/1025: raikou.png (already exists)\nSkipping 244/1025: entei.png (already exists)\nSkipping 245/1025: suicune.png (already exists)\nSkipping 246/1025: larvitar.png (already exists)\nSkipping 247/1025: pupitar.png (already exists)\nSkipping 248/1025: tyranitar.png (already exists)\nSkipping 249/1025: lugia.png (already exists)\nSkipping 250/1025: ho-oh.png (already exists)\nSkipping 251/1025: celebi.png (already exists)\nSkipping 252/1025: treecko.png (already exists)\nSkipping 253/1025: grovyle.png (already exists)\nSkipping 254/1025: sceptile.png (already exists)\nSkipping 255/1025: torchic.png (already exists)\nSkipping 256/1025: combusken.png (already exists)\nSkipping 257/1025: blaziken.png (already exists)\nSkipping 258/1025: mudkip.png (already exists)\nSkipping 259/1025: marshtomp.png (already exists)\nSkipping 260/1025: swampert.png (already exists)\nSkipping 261/1025: poochyena.png (already exists)\nSkipping 262/1025: mightyena.png (already exists)\nSkipping 263/1025: zigzagoon.png (already exists)\nSkipping 264/1025: linoone.png (already exists)\nSkipping 265/1025: wurmple.png (already exists)\nSkipping 266/1025: silcoon.png (already exists)\nSkipping 267/1025: beautifly.png (already exists)\nSkipping 268/1025: cascoon.png (already exists)\nSkipping 269/1025: dustox.png (already exists)\nSkipping 270/1025: lotad.png (already exists)\nSkipping 271/1025: lombre.png (already exists)\nSkipping 272/1025: ludicolo.png (already exists)\nSkipping 273/1025: seedot.png (already exists)\nSkipping 274/1025: nuzleaf.png (already exists)\nSkipping 275/1025: shiftry.png (already exists)\nSkipping 276/1025: taillow.png (already exists)\nSkipping 277/1025: swellow.png (already exists)\nSkipping 278/1025: wingull.png (already exists)\nSkipping 279/1025: pelipper.png (already exists)\nSkipping 280/1025: ralts.png (already exists)\nSkipping 281/1025: kirlia.png (already exists)\nSkipping 282/1025: gardevoir.png (already exists)\nSkipping 283/1025: surskit.png (already exists)\nSkipping 284/1025: masquerain.png (already exists)\nSkipping 285/1025: shroomish.png (already exists)\nSkipping 286/1025: breloom.png (already exists)\nSkipping 287/1025: slakoth.png (already exists)\nSkipping 288/1025: vigoroth.png (already exists)\nSkipping 289/1025: slaking.png (already exists)\nSkipping 290/1025: nincada.png (already exists)\nSkipping 291/1025: ninjask.png (already exists)\nSkipping 292/1025: shedinja.png (already exists)\nSkipping 293/1025: whismur.png (already exists)\nSkipping 294/1025: loudred.png (already exists)\nSkipping 295/1025: exploud.png (already exists)\nSkipping 296/1025: makuhita.png (already exists)\nSkipping 297/1025: hariyama.png (already exists)\nSkipping 298/1025: azurill.png (already exists)\nSkipping 299/1025: nosepass.png (already exists)\nSkipping 300/1025: skitty.png (already exists)\nSkipping 301/1025: delcatty.png (already exists)\nSkipping 302/1025: sableye.png (already exists)\nSkipping 303/1025: mawile.png (already exists)\nSkipping 304/1025: aron.png (already exists)\nSkipping 305/1025: lairon.png (already exists)\nSkipping 306/1025: aggron.png (already exists)\nSkipping 307/1025: meditite.png (already exists)\nSkipping 308/1025: medicham.png (already exists)\nSkipping 309/1025: electrike.png (already exists)\nSkipping 310/1025: manectric.png (already exists)\nSkipping 311/1025: plusle.png (already exists)\nSkipping 312/1025: minun.png (already exists)\nSkipping 313/1025: volbeat.png (already exists)\nSkipping 314/1025: illumise.png (already exists)\nSkipping 315/1025: roselia.png (already exists)\nSkipping 316/1025: gulpin.png (already exists)\nSkipping 317/1025: swalot.png (already exists)\nSkipping 318/1025: carvanha.png (already exists)\nSkipping 319/1025: sharpedo.png (already exists)\nSkipping 320/1025: wailmer.png (already exists)\nSkipping 321/1025: wailord.png (already exists)\nSkipping 322/1025: numel.png (already exists)\nSkipping 323/1025: camerupt.png (already exists)\nSkipping 324/1025: torkoal.png (already exists)\nSkipping 325/1025: spoink.png (already exists)\nSkipping 326/1025: grumpig.png (already exists)\nSkipping 327/1025: spinda.png (already exists)\nSkipping 328/1025: trapinch.png (already exists)\nSkipping 329/1025: vibrava.png (already exists)\nSkipping 330/1025: flygon.png (already exists)\nSkipping 331/1025: cacnea.png (already exists)\nSkipping 332/1025: cacturne.png (already exists)\nSkipping 333/1025: swablu.png (already exists)\nSkipping 334/1025: altaria.png (already exists)\nSkipping 335/1025: zangoose.png (already exists)\nSkipping 336/1025: seviper.png (already exists)\nSkipping 337/1025: lunatone.png (already exists)\nSkipping 338/1025: solrock.png (already exists)\nSkipping 339/1025: barboach.png (already exists)\nSkipping 340/1025: whiscash.png (already exists)\nSkipping 341/1025: corphish.png (already exists)\nSkipping 342/1025: crawdaunt.png (already exists)\nSkipping 343/1025: baltoy.png (already exists)\nSkipping 344/1025: claydol.png (already exists)\nSkipping 345/1025: lileep.png (already exists)\nSkipping 346/1025: cradily.png (already exists)\nSkipping 347/1025: anorith.png (already exists)\nSkipping 348/1025: armaldo.png (already exists)\nSkipping 349/1025: feebas.png (already exists)\nSkipping 350/1025: milotic.png (already exists)\nSkipping 351/1025: castform.png (already exists)\nSkipping 352/1025: kecleon.png (already exists)\nSkipping 353/1025: shuppet.png (already exists)\nSkipping 354/1025: banette.png (already exists)\nSkipping 355/1025: duskull.png (already exists)\nSkipping 356/1025: dusclops.png (already exists)\nSkipping 357/1025: tropius.png (already exists)\nSkipping 358/1025: chimecho.png (already exists)\nSkipping 359/1025: absol.png (already exists)\nSkipping 360/1025: wynaut.png (already exists)\nSkipping 361/1025: snorunt.png (already exists)\nSkipping 362/1025: glalie.png (already exists)\nSkipping 363/1025: spheal.png (already exists)\nSkipping 364/1025: sealeo.png (already exists)\nSkipping 365/1025: walrein.png (already exists)\nSkipping 366/1025: clamperl.png (already exists)\nSkipping 367/1025: huntail.png (already exists)\nSkipping 368/1025: gorebyss.png (already exists)\nSkipping 369/1025: relicanth.png (already exists)\nSkipping 370/1025: luvdisc.png (already exists)\nSkipping 371/1025: bagon.png (already exists)\nSkipping 372/1025: shelgon.png (already exists)\nSkipping 373/1025: salamence.png (already exists)\nSkipping 374/1025: beldum.png (already exists)\nSkipping 375/1025: metang.png (already exists)\nSkipping 376/1025: metagross.png (already exists)\nSkipping 377/1025: regirock.png (already exists)\nSkipping 378/1025: regice.png (already exists)\nSkipping 379/1025: registeel.png (already exists)\nSkipping 380/1025: latias.png (already exists)\nSkipping 381/1025: latios.png (already exists)\nSkipping 382/1025: kyogre.png (already exists)\nSkipping 383/1025: groudon.png (already exists)\nSkipping 384/1025: rayquaza.png (already exists)\nSkipping 385/1025: jirachi.png (already exists)\nSkipping 386/1025: deoxys.png (already exists)\nSkipping 387/1025: turtwig.png (already exists)\nSkipping 388/1025: grotle.png (already exists)\nSkipping 389/1025: torterra.png (already exists)\nSkipping 390/1025: chimchar.png (already exists)\nSkipping 391/1025: monferno.png (already exists)\nSkipping 392/1025: infernape.png (already exists)\nSkipping 393/1025: piplup.png (already exists)\nSkipping 394/1025: prinplup.png (already exists)\nSkipping 395/1025: empoleon.png (already exists)\nSkipping 396/1025: starly.png (already exists)\nSkipping 397/1025: staravia.png (already exists)\nSkipping 398/1025: staraptor.png (already exists)\nSkipping 399/1025: bidoof.png (already exists)\nSkipping 400/1025: bibarel.png (already exists)\nSkipping 401/1025: kricketot.png (already exists)\nSkipping 402/1025: kricketune.png (already exists)\nSkipping 403/1025: shinx.png (already exists)\nSkipping 404/1025: luxio.png (already exists)\nSkipping 405/1025: luxray.png (already exists)\nSkipping 406/1025: budew.png (already exists)\nSkipping 407/1025: roserade.png (already exists)\nSkipping 408/1025: cranidos.png (already exists)\nSkipping 409/1025: rampardos.png (already exists)\nSkipping 410/1025: shieldon.png (already exists)\nSkipping 411/1025: bastiodon.png (already exists)\nSkipping 412/1025: burmy.png (already exists)\nSkipping 413/1025: wormadam.png (already exists)\nSkipping 414/1025: mothim.png (already exists)\nSkipping 415/1025: combee.png (already exists)\nSkipping 416/1025: vespiquen.png (already exists)\nSkipping 417/1025: pachirisu.png (already exists)\nSkipping 418/1025: buizel.png (already exists)\nSkipping 419/1025: floatzel.png (already exists)\nSkipping 420/1025: cherubi.png (already exists)\nSkipping 421/1025: cherrim.png (already exists)\nSkipping 422/1025: shellos.png (already exists)\nSkipping 423/1025: gastrodon.png (already exists)\nSkipping 424/1025: ambipom.png (already exists)\nSkipping 425/1025: drifloon.png (already exists)\nSkipping 426/1025: drifblim.png (already exists)\nSkipping 427/1025: buneary.png (already exists)\nSkipping 428/1025: lopunny.png (already exists)\nSkipping 429/1025: mismagius.png (already exists)\nSkipping 430/1025: honchkrow.png (already exists)\nSkipping 431/1025: glameow.png (already exists)\nSkipping 432/1025: purugly.png (already exists)\nSkipping 433/1025: chingling.png (already exists)\nSkipping 434/1025: stunky.png (already exists)\nSkipping 435/1025: skuntank.png (already exists)\nSkipping 436/1025: bronzor.png (already exists)\nSkipping 437/1025: bronzong.png (already exists)\nSkipping 438/1025: bonsly.png (already exists)\nSkipping 439/1025: mime-jr.png (already exists)\nSkipping 440/1025: happiny.png (already exists)\nSkipping 441/1025: chatot.png (already exists)\nSkipping 442/1025: spiritomb.png (already exists)\nSkipping 443/1025: gible.png (already exists)\nSkipping 444/1025: gabite.png (already exists)\nSkipping 445/1025: garchomp.png (already exists)\nSkipping 446/1025: munchlax.png (already exists)\nSkipping 447/1025: riolu.png (already exists)\nSkipping 448/1025: lucario.png (already exists)\nSkipping 449/1025: hippopotas.png (already exists)\nSkipping 450/1025: hippowdon.png (already exists)\nSkipping 451/1025: skorupi.png (already exists)\nSkipping 452/1025: drapion.png (already exists)\nSkipping 453/1025: croagunk.png (already exists)\nSkipping 454/1025: toxicroak.png (already exists)\nSkipping 455/1025: carnivine.png (already exists)\nSkipping 456/1025: finneon.png (already exists)\nSkipping 457/1025: lumineon.png (already exists)\nSkipping 458/1025: mantyke.png (already exists)\nSkipping 459/1025: snover.png (already exists)\nSkipping 460/1025: abomasnow.png (already exists)\nSkipping 461/1025: weavile.png (already exists)\nSkipping 462/1025: magnezone.png (already exists)\nSkipping 463/1025: lickilicky.png (already exists)\nSkipping 464/1025: rhyperior.png (already exists)\nSkipping 465/1025: tangrowth.png (already exists)\nSkipping 466/1025: electivire.png (already exists)\nSkipping 467/1025: magmortar.png (already exists)\nSkipping 468/1025: togekiss.png (already exists)\nSkipping 469/1025: yanmega.png (already exists)\nSkipping 470/1025: leafeon.png (already exists)\nSkipping 471/1025: glaceon.png (already exists)\nSkipping 472/1025: gliscor.png (already exists)\nSkipping 473/1025: mamoswine.png (already exists)\nSkipping 474/1025: porygon-z.png (already exists)\nSkipping 475/1025: gallade.png (already exists)\nSkipping 476/1025: probopass.png (already exists)\nSkipping 477/1025: dusknoir.png (already exists)\nSkipping 478/1025: froslass.png (already exists)\nSkipping 479/1025: rotom.png (already exists)\nSkipping 480/1025: uxie.png (already exists)\nSkipping 481/1025: mesprit.png (already exists)\nSkipping 482/1025: azelf.png (already exists)\nSkipping 483/1025: dialga.png (already exists)\nSkipping 484/1025: palkia.png (already exists)\nSkipping 485/1025: heatran.png (already exists)\nSkipping 486/1025: regigigas.png (already exists)\nSkipping 487/1025: giratina.png (already exists)\nSkipping 488/1025: cresselia.png (already exists)\nSkipping 489/1025: phione.png (already exists)\nSkipping 490/1025: manaphy.png (already exists)\nSkipping 491/1025: darkrai.png (already exists)\nSkipping 492/1025: shaymin.png (already exists)\nSkipping 493/1025: arceus.png (already exists)\nSkipping 494/1025: victini.png (already exists)\nSkipping 495/1025: snivy.png (already exists)\nSkipping 496/1025: servine.png (already exists)\nSkipping 497/1025: serperior.png (already exists)\nSkipping 498/1025: tepig.png (already exists)\nSkipping 499/1025: pignite.png (already exists)\nSkipping 500/1025: emboar.png (already exists)\nSkipping 501/1025: oshawott.png (already exists)\nSkipping 502/1025: dewott.png (already exists)\nSkipping 503/1025: samurott.png (already exists)\nSkipping 504/1025: patrat.png (already exists)\nSkipping 505/1025: watchog.png (already exists)\nSkipping 506/1025: lillipup.png (already exists)\nSkipping 507/1025: herdier.png (already exists)\nSkipping 508/1025: stoutland.png (already exists)\nSkipping 509/1025: purrloin.png (already exists)\nSkipping 510/1025: liepard.png (already exists)\nSkipping 511/1025: pansage.png (already exists)\nSkipping 512/1025: simisage.png (already exists)\nSkipping 513/1025: pansear.png (already exists)\nSkipping 514/1025: simisear.png (already exists)\nSkipping 515/1025: panpour.png (already exists)\nSkipping 516/1025: simipour.png (already exists)\nSkipping 517/1025: munna.png (already exists)\nSkipping 518/1025: musharna.png (already exists)\nSkipping 519/1025: pidove.png (already exists)\nSkipping 520/1025: tranquill.png (already exists)\nSkipping 521/1025: unfezant.png (already exists)\nSkipping 522/1025: blitzle.png (already exists)\nSkipping 523/1025: zebstrika.png (already exists)\nSkipping 524/1025: roggenrola.png (already exists)\nSkipping 525/1025: boldore.png (already exists)\nSkipping 526/1025: gigalith.png (already exists)\nSkipping 527/1025: woobat.png (already exists)\nSkipping 528/1025: swoobat.png (already exists)\nSkipping 529/1025: drilbur.png (already exists)\nSkipping 530/1025: excadrill.png (already exists)\nSkipping 531/1025: audino.png (already exists)\nSkipping 532/1025: timburr.png (already exists)\nSkipping 533/1025: gurdurr.png (already exists)\nSkipping 534/1025: conkeldurr.png (already exists)\nSkipping 535/1025: tympole.png (already exists)\nSkipping 536/1025: palpitoad.png (already exists)\nSkipping 537/1025: seismitoad.png (already exists)\nSkipping 538/1025: throh.png (already exists)\nSkipping 539/1025: sawk.png (already exists)\nSkipping 540/1025: sewaddle.png (already exists)\nSkipping 541/1025: swadloon.png (already exists)\nSkipping 542/1025: leavanny.png (already exists)\nSkipping 543/1025: venipede.png (already exists)\nSkipping 544/1025: whirlipede.png (already exists)\nSkipping 545/1025: scolipede.png (already exists)\nSkipping 546/1025: cottonee.png (already exists)\nSkipping 547/1025: whimsicott.png (already exists)\nSkipping 548/1025: petilil.png (already exists)\nSkipping 549/1025: lilligant.png (already exists)\nSkipping 550/1025: basculin.png (already exists)\nSkipping 551/1025: sandile.png (already exists)\nSkipping 552/1025: krokorok.png (already exists)\nSkipping 553/1025: krookodile.png (already exists)\nSkipping 554/1025: darumaka.png (already exists)\nSkipping 555/1025: darmanitan.png (already exists)\nSkipping 556/1025: maractus.png (already exists)\nSkipping 557/1025: dwebble.png (already exists)\nSkipping 558/1025: crustle.png (already exists)\nSkipping 559/1025: scraggy.png (already exists)\nSkipping 560/1025: scrafty.png (already exists)\nSkipping 561/1025: sigilyph.png (already exists)\nSkipping 562/1025: yamask.png (already exists)\nSkipping 563/1025: cofagrigus.png (already exists)\nSkipping 564/1025: tirtouga.png (already exists)\nSkipping 565/1025: carracosta.png (already exists)\nSkipping 566/1025: archen.png (already exists)\nSkipping 567/1025: archeops.png (already exists)\nSkipping 568/1025: trubbish.png (already exists)\nSkipping 569/1025: garbodor.png (already exists)\nSkipping 570/1025: zorua.png (already exists)\nSkipping 571/1025: zoroark.png (already exists)\nSkipping 572/1025: minccino.png (already exists)\nSkipping 573/1025: cinccino.png (already exists)\nSkipping 574/1025: gothita.png (already exists)\nSkipping 575/1025: gothorita.png (already exists)\nSkipping 576/1025: gothitelle.png (already exists)\nSkipping 577/1025: solosis.png (already exists)\nSkipping 578/1025: duosion.png (already exists)\nSkipping 579/1025: reuniclus.png (already exists)\nSkipping 580/1025: ducklett.png (already exists)\nSkipping 581/1025: swanna.png (already exists)\nSkipping 582/1025: vanillite.png (already exists)\nSkipping 583/1025: vanillish.png (already exists)\nSkipping 584/1025: vanilluxe.png (already exists)\nSkipping 585/1025: deerling.png (already exists)\nSkipping 586/1025: sawsbuck.png (already exists)\nSkipping 587/1025: emolga.png (already exists)\nSkipping 588/1025: karrablast.png (already exists)\nSkipping 589/1025: escavalier.png (already exists)\nSkipping 590/1025: foongus.png (already exists)\nSkipping 591/1025: amoonguss.png (already exists)\nSkipping 592/1025: frillish.png (already exists)\nSkipping 593/1025: jellicent.png (already exists)\nSkipping 594/1025: alomomola.png (already exists)\nSkipping 595/1025: joltik.png (already exists)\nSkipping 596/1025: galvantula.png (already exists)\nSkipping 597/1025: ferroseed.png (already exists)\nSkipping 598/1025: ferrothorn.png (already exists)\nSkipping 599/1025: klink.png (already exists)\nSkipping 600/1025: klang.png (already exists)\nSkipping 601/1025: klinklang.png (already exists)\nSkipping 602/1025: tynamo.png (already exists)\nSkipping 603/1025: eelektrik.png (already exists)\nSkipping 604/1025: eelektross.png (already exists)\nSkipping 605/1025: elgyem.png (already exists)\nSkipping 606/1025: beheeyem.png (already exists)\nSkipping 607/1025: litwick.png (already exists)\nSkipping 608/1025: lampent.png (already exists)\nSkipping 609/1025: chandelure.png (already exists)\nSkipping 610/1025: axew.png (already exists)\nSkipping 611/1025: fraxure.png (already exists)\nSkipping 612/1025: haxorus.png (already exists)\nSkipping 613/1025: cubchoo.png (already exists)\nSkipping 614/1025: beartic.png (already exists)\nSkipping 615/1025: cryogonal.png (already exists)\nSkipping 616/1025: shelmet.png (already exists)\nSkipping 617/1025: accelgor.png (already exists)\nSkipping 618/1025: stunfisk.png (already exists)\nSkipping 619/1025: mienfoo.png (already exists)\nSkipping 620/1025: mienshao.png (already exists)\nSkipping 621/1025: druddigon.png (already exists)\nSkipping 622/1025: golett.png (already exists)\nSkipping 623/1025: golurk.png (already exists)\nSkipping 624/1025: pawniard.png (already exists)\nSkipping 625/1025: bisharp.png (already exists)\nSkipping 626/1025: bouffalant.png (already exists)\nSkipping 627/1025: rufflet.png (already exists)\nSkipping 628/1025: braviary.png (already exists)\nSkipping 629/1025: vullaby.png (already exists)\nSkipping 630/1025: mandibuzz.png (already exists)\nSkipping 631/1025: heatmor.png (already exists)\nSkipping 632/1025: durant.png (already exists)\nSkipping 633/1025: deino.png (already exists)\nSkipping 634/1025: zweilous.png (already exists)\nSkipping 635/1025: hydreigon.png (already exists)\nSkipping 636/1025: larvesta.png (already exists)\nSkipping 637/1025: volcarona.png (already exists)\nSkipping 638/1025: cobalion.png (already exists)\nSkipping 639/1025: terrakion.png (already exists)\nSkipping 640/1025: virizion.png (already exists)\nSkipping 641/1025: tornadus.png (already exists)\nSkipping 642/1025: thundurus.png (already exists)\nSkipping 643/1025: reshiram.png (already exists)\nSkipping 644/1025: zekrom.png (already exists)\nSkipping 645/1025: landorus.png (already exists)\nSkipping 646/1025: kyurem.png (already exists)\nSkipping 647/1025: keldeo.png (already exists)\nSkipping 648/1025: meloetta.png (already exists)\nSkipping 649/1025: genesect.png (already exists)\nSkipping 650/1025: chespin.png (already exists)\nSkipping 651/1025: quilladin.png (already exists)\nSkipping 652/1025: chesnaught.png (already exists)\nSkipping 653/1025: fennekin.png (already exists)\nSkipping 654/1025: braixen.png (already exists)\nSkipping 655/1025: delphox.png (already exists)\nSkipping 656/1025: froakie.png (already exists)\nSkipping 657/1025: frogadier.png (already exists)\nSkipping 658/1025: greninja.png (already exists)\nSkipping 659/1025: bunnelby.png (already exists)\nSkipping 660/1025: diggersby.png (already exists)\nSkipping 661/1025: fletchling.png (already exists)\nSkipping 662/1025: fletchinder.png (already exists)\nSkipping 663/1025: talonflame.png (already exists)\nSkipping 664/1025: scatterbug.png (already exists)\nSkipping 665/1025: spewpa.png (already exists)\nSkipping 666/1025: vivillon.png (already exists)\nSkipping 667/1025: litleo.png (already exists)\nSkipping 668/1025: pyroar.png (already exists)\nSkipping 669/1025: flabebe.png (already exists)\nSkipping 670/1025: floette.png (already exists)\nSkipping 671/1025: florges.png (already exists)\nSkipping 672/1025: skiddo.png (already exists)\nSkipping 673/1025: gogoat.png (already exists)\nSkipping 674/1025: pancham.png (already exists)\nSkipping 675/1025: pangoro.png (already exists)\nSkipping 676/1025: furfrou.png (already exists)\nSkipping 677/1025: espurr.png (already exists)\nSkipping 678/1025: meowstic.png (already exists)\nSkipping 679/1025: honedge.png (already exists)\nSkipping 680/1025: doublade.png (already exists)\nSkipping 681/1025: aegislash.png (already exists)\nSkipping 682/1025: spritzee.png (already exists)\nSkipping 683/1025: aromatisse.png (already exists)\nSkipping 684/1025: swirlix.png (already exists)\nSkipping 685/1025: slurpuff.png (already exists)\nSkipping 686/1025: inkay.png (already exists)\nSkipping 687/1025: malamar.png (already exists)\nSkipping 688/1025: binacle.png (already exists)\nSkipping 689/1025: barbaracle.png (already exists)\nSkipping 690/1025: skrelp.png (already exists)\nSkipping 691/1025: dragalge.png (already exists)\nSkipping 692/1025: clauncher.png (already exists)\nSkipping 693/1025: clawitzer.png (already exists)\nSkipping 694/1025: helioptile.png (already exists)\nSkipping 695/1025: heliolisk.png (already exists)\nSkipping 696/1025: tyrunt.png (already exists)\nSkipping 697/1025: tyrantrum.png (already exists)\nSkipping 698/1025: amaura.png (already exists)\nSkipping 699/1025: aurorus.png (already exists)\nSkipping 700/1025: sylveon.png (already exists)\nSkipping 701/1025: hawlucha.png (already exists)\nSkipping 702/1025: dedenne.png (already exists)\nSkipping 703/1025: carbink.png (already exists)\nSkipping 704/1025: goomy.png (already exists)\nSkipping 705/1025: sliggoo.png (already exists)\nSkipping 706/1025: goodra.png (already exists)\nSkipping 707/1025: klefki.png (already exists)\nSkipping 708/1025: phantump.png (already exists)\nSkipping 709/1025: trevenant.png (already exists)\nSkipping 710/1025: pumpkaboo.png (already exists)\nSkipping 711/1025: gourgeist.png (already exists)\nSkipping 712/1025: bergmite.png (already exists)\nSkipping 713/1025: avalugg.png (already exists)\nSkipping 714/1025: noibat.png (already exists)\nSkipping 715/1025: noivern.png (already exists)\nSkipping 716/1025: xerneas.png (already exists)\nSkipping 717/1025: yveltal.png (already exists)\nSkipping 718/1025: zygarde.png (already exists)\nSkipping 719/1025: diancie.png (already exists)\nSkipping 720/1025: hoopa.png (already exists)\nSkipping 721/1025: volcanion.png (already exists)\nSkipping 722/1025: rowlet.png (already exists)\nSkipping 723/1025: dartrix.png (already exists)\nSkipping 724/1025: decidueye.png (already exists)\nSkipping 725/1025: litten.png (already exists)\nSkipping 726/1025: torracat.png (already exists)\nSkipping 727/1025: incineroar.png (already exists)\nSkipping 728/1025: popplio.png (already exists)\nSkipping 729/1025: brionne.png (already exists)\nSkipping 730/1025: primarina.png (already exists)\nSkipping 731/1025: pikipek.png (already exists)\nSkipping 732/1025: trumbeak.png (already exists)\nSkipping 733/1025: toucannon.png (already exists)\nSkipping 734/1025: yungoos.png (already exists)\nSkipping 735/1025: gumshoos.png (already exists)\nSkipping 736/1025: grubbin.png (already exists)\nSkipping 737/1025: charjabug.png (already exists)\nSkipping 738/1025: vikavolt.png (already exists)\nSkipping 739/1025: crabrawler.png (already exists)\nSkipping 740/1025: crabominable.png (already exists)\nSkipping 741/1025: oricorio.png (already exists)\nSkipping 742/1025: cutiefly.png (already exists)\nSkipping 743/1025: ribombee.png (already exists)\nSkipping 744/1025: rockruff.png (already exists)\nSkipping 745/1025: lycanroc.png (already exists)\nSkipping 746/1025: wishiwashi.png (already exists)\nSkipping 747/1025: mareanie.png (already exists)\nSkipping 748/1025: toxapex.png (already exists)\nSkipping 749/1025: mudbray.png (already exists)\nSkipping 750/1025: mudsdale.png (already exists)\nSkipping 751/1025: dewpider.png (already exists)\nSkipping 752/1025: araquanid.png (already exists)\nSkipping 753/1025: fomantis.png (already exists)\nSkipping 754/1025: lurantis.png (already exists)\nSkipping 755/1025: morelull.png (already exists)\nSkipping 756/1025: shiinotic.png (already exists)\nSkipping 757/1025: salandit.png (already exists)\nSkipping 758/1025: salazzle.png (already exists)\nSkipping 759/1025: stufful.png (already exists)\nSkipping 760/1025: bewear.png (already exists)\nSkipping 761/1025: bounsweet.png (already exists)\nSkipping 762/1025: steenee.png (already exists)\nSkipping 763/1025: tsareena.png (already exists)\nSkipping 764/1025: comfey.png (already exists)\nSkipping 765/1025: oranguru.png (already exists)\nSkipping 766/1025: passimian.png (already exists)\nSkipping 767/1025: wimpod.png (already exists)\nSkipping 768/1025: golisopod.png (already exists)\nSkipping 769/1025: sandygast.png (already exists)\nSkipping 770/1025: palossand.png (already exists)\nSkipping 771/1025: pyukumuku.png (already exists)\nSkipping 772/1025: type-null.png (already exists)\nSkipping 773/1025: silvally.png (already exists)\nSkipping 774/1025: minior.png (already exists)\nSkipping 775/1025: komala.png (already exists)\nSkipping 776/1025: turtonator.png (already exists)\nSkipping 777/1025: togedemaru.png (already exists)\nSkipping 778/1025: mimikyu.png (already exists)\nSkipping 779/1025: bruxish.png (already exists)\nSkipping 780/1025: drampa.png (already exists)\nSkipping 781/1025: dhelmise.png (already exists)\nSkipping 782/1025: jangmo-o.png (already exists)\nSkipping 783/1025: hakamo-o.png (already exists)\nSkipping 784/1025: kommo-o.png (already exists)\nSkipping 785/1025: tapu-koko.png (already exists)\nSkipping 786/1025: tapu-lele.png (already exists)\nSkipping 787/1025: tapu-bulu.png (already exists)\nSkipping 788/1025: tapu-fini.png (already exists)\nSkipping 789/1025: cosmog.png (already exists)\nSkipping 790/1025: cosmoem.png (already exists)\nSkipping 791/1025: solgaleo.png (already exists)\nSkipping 792/1025: lunala.png (already exists)\nSkipping 793/1025: nihilego.png (already exists)\nSkipping 794/1025: buzzwole.png (already exists)\nSkipping 795/1025: pheromosa.png (already exists)\nSkipping 796/1025: xurkitree.png (already exists)\nSkipping 797/1025: celesteela.png (already exists)\nSkipping 798/1025: kartana.png (already exists)\nSkipping 799/1025: guzzlord.png (already exists)\nSkipping 800/1025: necrozma.png (already exists)\nSkipping 801/1025: magearna.png (already exists)\nSkipping 802/1025: marshadow.png (already exists)\nSkipping 803/1025: poipole.png (already exists)\nSkipping 804/1025: naganadel.png (already exists)\nSkipping 805/1025: stakataka.png (already exists)\nSkipping 806/1025: blacephalon.png (already exists)\nSkipping 807/1025: zeraora.png (already exists)\nSkipping 808/1025: meltan.png (already exists)\nSkipping 809/1025: melmetal.png (already exists)\nSkipping 810/1025: grookey.png (already exists)\nSkipping 811/1025: thwackey.png (already exists)\nSkipping 812/1025: rillaboom.png (already exists)\nSkipping 813/1025: scorbunny.png (already exists)\nSkipping 814/1025: raboot.png (already exists)\nSkipping 815/1025: cinderace.png (already exists)\nSkipping 816/1025: sobble.png (already exists)\nSkipping 817/1025: drizzile.png (already exists)\nSkipping 818/1025: inteleon.png (already exists)\nSkipping 819/1025: skwovet.png (already exists)\nSkipping 820/1025: greedent.png (already exists)\nSkipping 821/1025: rookidee.png (already exists)\nSkipping 822/1025: corvisquire.png (already exists)\nSkipping 823/1025: corviknight.png (already exists)\nSkipping 824/1025: blipbug.png (already exists)\nSkipping 825/1025: dottler.png (already exists)\nSkipping 826/1025: orbeetle.png (already exists)\nSkipping 827/1025: nickit.png (already exists)\nSkipping 828/1025: thievul.png (already exists)\nSkipping 829/1025: gossifleur.png (already exists)\nSkipping 830/1025: eldegoss.png (already exists)\nSkipping 831/1025: wooloo.png (already exists)\nSkipping 832/1025: dubwool.png (already exists)\nSkipping 833/1025: chewtle.png (already exists)\nSkipping 834/1025: drednaw.png (already exists)\nSkipping 835/1025: yamper.png (already exists)\nSkipping 836/1025: boltund.png (already exists)\nSkipping 837/1025: rolycoly.png (already exists)\nSkipping 838/1025: carkol.png (already exists)\nSkipping 839/1025: coalossal.png (already exists)\nSkipping 840/1025: applin.png (already exists)\nSkipping 841/1025: flapple.png (already exists)\nSkipping 842/1025: appletun.png (already exists)\nSkipping 843/1025: silicobra.png (already exists)\nSkipping 844/1025: sandaconda.png (already exists)\nSkipping 845/1025: cramorant.png (already exists)\nSkipping 846/1025: arrokuda.png (already exists)\nSkipping 847/1025: barraskewda.png (already exists)\nSkipping 848/1025: toxel.png (already exists)\nSkipping 849/1025: toxtricity.png (already exists)\nSkipping 850/1025: sizzlipede.png (already exists)\nSkipping 851/1025: centiskorch.png (already exists)\nSkipping 852/1025: clobbopus.png (already exists)\nSkipping 853/1025: grapploct.png (already exists)\nSkipping 854/1025: sinistea.png (already exists)\nSkipping 855/1025: polteageist.png (already exists)\nSkipping 856/1025: hatenna.png (already exists)\nSkipping 857/1025: hattrem.png (already exists)\nSkipping 858/1025: hatterene.png (already exists)\nSkipping 859/1025: impidimp.png (already exists)\nSkipping 860/1025: morgrem.png (already exists)\nSkipping 861/1025: grimmsnarl.png (already exists)\nSkipping 862/1025: obstagoon.png (already exists)\nSkipping 863/1025: perrserker.png (already exists)\nSkipping 864/1025: cursola.png (already exists)\nSkipping 865/1025: sirfetchd.png (already exists)\nSkipping 866/1025: mr-rime.png (already exists)\nSkipping 867/1025: runerigus.png (already exists)\nSkipping 868/1025: milcery.png (already exists)\nSkipping 869/1025: alcremie.png (already exists)\nSkipping 870/1025: falinks.png (already exists)\nSkipping 871/1025: pincurchin.png (already exists)\nSkipping 872/1025: snom.png (already exists)\nSkipping 873/1025: frosmoth.png (already exists)\nSkipping 874/1025: stonjourner.png (already exists)\nSkipping 875/1025: eiscue.png (already exists)\nSkipping 876/1025: indeedee.png (already exists)\nSkipping 877/1025: morpeko.png (already exists)\nSkipping 878/1025: cufant.png (already exists)\nSkipping 879/1025: copperajah.png (already exists)\nSkipping 880/1025: dracozolt.png (already exists)\nSkipping 881/1025: arctozolt.png (already exists)\nSkipping 882/1025: dracovish.png (already exists)\nSkipping 883/1025: arctovish.png (already exists)\nSkipping 884/1025: duraludon.png (already exists)\nSkipping 885/1025: dreepy.png (already exists)\nSkipping 886/1025: drakloak.png (already exists)\nSkipping 887/1025: dragapult.png (already exists)\nSkipping 888/1025: zacian.png (already exists)\nSkipping 889/1025: zamazenta.png (already exists)\nSkipping 890/1025: eternatus.png (already exists)\nSkipping 891/1025: kubfu.png (already exists)\nSkipping 892/1025: urshifu.png (already exists)\nSkipping 893/1025: zarude.png (already exists)\nSkipping 894/1025: regieleki.png (already exists)\nSkipping 895/1025: regidrago.png (already exists)\nSkipping 896/1025: glastrier.png (already exists)\nSkipping 897/1025: spectrier.png (already exists)\nSkipping 898/1025: calyrex.png (already exists)\nSkipping 899/1025: wyrdeer.png (already exists)\nSkipping 900/1025: kleavor.png (already exists)\nSkipping 901/1025: ursaluna.png (already exists)\nSkipping 902/1025: basculegion.png (already exists)\nSkipping 903/1025: sneasler.png (already exists)\nSkipping 904/1025: overqwil.png (already exists)\nSkipping 905/1025: enamorus.png (already exists)\nSkipping 906/1025: sprigatito.png (already exists)\nSkipping 907/1025: floragato.png (already exists)\nSkipping 908/1025: meowscarada.png (already exists)\nSkipping 909/1025: fuecoco.png (already exists)\nSkipping 910/1025: crocalor.png (already exists)\nSkipping 911/1025: skeledirge.png (already exists)\nSkipping 912/1025: quaxly.png (already exists)\nSkipping 913/1025: quaxwell.png (already exists)\nSkipping 914/1025: quaquaval.png (already exists)\nSkipping 915/1025: lechonk.png (already exists)\nSkipping 916/1025: oinkologne.png (already exists)\nSkipping 917/1025: tarountula.png (already exists)\nSkipping 918/1025: spidops.png (already exists)\nSkipping 919/1025: nymble.png (already exists)\nSkipping 920/1025: lokix.png (already exists)\nSkipping 921/1025: pawmi.png (already exists)\nSkipping 922/1025: pawmo.png (already exists)\nSkipping 923/1025: pawmot.png (already exists)\nSkipping 924/1025: tandemaus.png (already exists)\nSkipping 925/1025: maushold.png (already exists)\nSkipping 926/1025: fidough.png (already exists)\nSkipping 927/1025: dachsbun.png (already exists)\nSkipping 928/1025: smoliv.png (already exists)\nSkipping 929/1025: dolliv.png (already exists)\nSkipping 930/1025: arboliva.png (already exists)\nSkipping 931/1025: squawkabilly.png (already exists)\nSkipping 932/1025: nacli.png (already exists)\nSkipping 933/1025: naclstack.png (already exists)\nSkipping 934/1025: garganacl.png (already exists)\nSkipping 935/1025: charcadet.png (already exists)\nSkipping 936/1025: armarouge.png (already exists)\nSkipping 937/1025: ceruledge.png (already exists)\nSkipping 938/1025: tadbulb.png (already exists)\nSkipping 939/1025: bellibolt.png (already exists)\nSkipping 940/1025: wattrel.png (already exists)\nSkipping 941/1025: kilowattrel.png (already exists)\nSkipping 942/1025: maschiff.png (already exists)\nSkipping 943/1025: mabosstiff.png (already exists)\nSkipping 944/1025: shroodle.png (already exists)\nSkipping 945/1025: grafaiai.png (already exists)\nSkipping 946/1025: bramblin.png (already exists)\nSkipping 947/1025: brambleghast.png (already exists)\nSkipping 948/1025: toedscool.png (already exists)\nSkipping 949/1025: toedscruel.png (already exists)\nSkipping 950/1025: klawf.png (already exists)\nSkipping 951/1025: capsakid.png (already exists)\nSkipping 952/1025: scovillain.png (already exists)\nSkipping 953/1025: rellor.png (already exists)\nSkipping 954/1025: rabsca.png (already exists)\nSkipping 955/1025: flittle.png (already exists)\nSkipping 956/1025: espathra.png (already exists)\nSkipping 957/1025: tinkatink.png (already exists)\nSkipping 958/1025: tinkatuff.png (already exists)\nSkipping 959/1025: tinkaton.png (already exists)\nSkipping 960/1025: wiglett.png (already exists)\nSkipping 961/1025: wugtrio.png (already exists)\nSkipping 962/1025: bombirdier.png (already exists)\nSkipping 963/1025: finizen.png (already exists)\nSkipping 964/1025: palafin.png (already exists)\nSkipping 965/1025: varoom.png (already exists)\nSkipping 966/1025: revavroom.png (already exists)\nSkipping 967/1025: cyclizar.png (already exists)\nSkipping 968/1025: orthworm.png (already exists)\nSkipping 969/1025: glimmet.png (already exists)\nSkipping 970/1025: glimmora.png (already exists)\nSkipping 971/1025: greavard.png (already exists)\nSkipping 972/1025: houndstone.png (already exists)\nSkipping 973/1025: flamigo.png (already exists)\nSkipping 974/1025: cetoddle.png (already exists)\nSkipping 975/1025: cetitan.png (already exists)\nSkipping 976/1025: veluza.png (already exists)\nSkipping 977/1025: dondozo.png (already exists)\nSkipping 978/1025: tatsugiri.png (already exists)\nSkipping 979/1025: annihilape.png (already exists)\nSkipping 980/1025: clodsire.png (already exists)\nSkipping 981/1025: farigiraf.png (already exists)\nSkipping 982/1025: dudunsparce.png (already exists)\nSkipping 983/1025: kingambit.png (already exists)\nSkipping 984/1025: great-tusk.png (already exists)\nSkipping 985/1025: scream-tail.png (already exists)\nSkipping 986/1025: brute-bonnet.png (already exists)\nSkipping 987/1025: flutter-mane.png (already exists)\nSkipping 988/1025: slither-wing.png (already exists)\nSkipping 989/1025: sandy-shocks.png (already exists)\nSkipping 990/1025: iron-treads.png (already exists)\nSkipping 991/1025: iron-bundle.png (already exists)\nSkipping 992/1025: iron-hands.png (already exists)\nSkipping 993/1025: iron-jugulis.png (already exists)\nSkipping 994/1025: iron-moth.png (already exists)\nSkipping 995/1025: iron-thorns.png (already exists)\nSkipping 996/1025: frigibax.png (already exists)\nSkipping 997/1025: arctibax.png (already exists)\nSkipping 998/1025: baxcalibur.png (already exists)\nSkipping 999/1025: gimmighoul.png (already exists)\nSkipping 1000/1025: gholdengo.png (already exists)\nSkipping 1001/1025: wo-chien.png (already exists)\nSkipping 1002/1025: chien-pao.png (already exists)\nSkipping 1003/1025: ting-lu.png (already exists)\nSkipping 1004/1025: chi-yu.png (already exists)\nSkipping 1005/1025: roaring-moon.png (already exists)\nSkipping 1006/1025: iron-valiant.png (already exists)\nSkipping 1007/1025: koraidon.png (already exists)\nSkipping 1008/1025: miraidon.png (already exists)\nSkipping 1009/1025: walking-wake.png (already exists)\nSkipping 1010/1025: iron-leaves.png (already exists)\nSkipping 1011/1025: dipplin.png (already exists)\nSkipping 1012/1025: poltchageist.png (already exists)\nSkipping 1013/1025: sinistcha.png (already exists)\nSkipping 1014/1025: okidogi.png (already exists)\nSkipping 1015/1025: munkidori.png (already exists)\nSkipping 1016/1025: fezandipiti.png (already exists)\nSkipping 1017/1025: ogerpon.png (already exists)\nSkipping 1018/1025: archaludon.png (already exists)\nSkipping 1019/1025: hydrapple.png (already exists)\nSkipping 1020/1025: gouging-fire.png (already exists)\nSkipping 1021/1025: raging-bolt.png (already exists)\nSkipping 1022/1025: iron-boulder.png (already exists)\nSkipping 1023/1025: iron-crown.png (already exists)\nSkipping 1024/1025: terapagos.png (already exists)\nSkipping 1025/1025: pecharunt.png (already exists)\nDownload complete! Downloaded: 0, Skipped: 1025, Total found: 1025\nProcessing 1025 Pokemon sprites...\nCanvas size: 48x48\nSprite will occupy 95.0% of canvas (45 pixels max)\nBinarization using 'adaptive' method\nProcessed 1/1025: parasect.png\nProcessed 2/1025: sobble.png\nProcessed 3/1025: lumineon.png\nProcessed 4/1025: raikou.png\nProcessed 5/1025: runerigus.png\nProcessed 6/1025: dedenne.png\nProcessed 7/1025: pyroar.png\nProcessed 8/1025: pawmi.png\nProcessed 9/1025: articuno.png\nProcessed 10/1025: meowstic.png\nProcessed 11/1025: magmortar.png\nProcessed 12/1025: bulbasaur.png\nProcessed 13/1025: banette.png\nProcessed 14/1025: staraptor.png\nProcessed 15/1025: pidove.png\nProcessed 16/1025: morgrem.png\nProcessed 17/1025: comfey.png\nProcessed 18/1025: taillow.png\nProcessed 19/1025: charizard.png\nProcessed 20/1025: infernape.png\nProcessed 21/1025: sandshrew.png\nProcessed 22/1025: marshadow.png\nProcessed 23/1025: alakazam.png\nProcessed 24/1025: lickitung.png\nProcessed 25/1025: starmie.png\nProcessed 26/1025: wishiwashi.png\nProcessed 27/1025: yanmega.png\nProcessed 28/1025: yveltal.png\nProcessed 29/1025: zigzagoon.png\nProcessed 30/1025: petilil.png\nProcessed 31/1025: torterra.png\nProcessed 32/1025: purugly.png\nProcessed 33/1025: tandemaus.png\nProcessed 34/1025: cresselia.png\nProcessed 35/1025: regigigas.png\nProcessed 36/1025: palkia.png\nProcessed 37/1025: impidimp.png\nProcessed 38/1025: froakie.png\nProcessed 39/1025: kilowattrel.png\nProcessed 40/1025: arrokuda.png\nProcessed 41/1025: munna.png\nProcessed 42/1025: bellossom.png\nProcessed 43/1025: gabite.png\nProcessed 44/1025: tapu-koko.png\nProcessed 45/1025: mareep.png\nProcessed 46/1025: shieldon.png\nProcessed 47/1025: sneasler.png\nProcessed 48/1025: stoutland.png\nProcessed 49/1025: corviknight.png\nProcessed 50/1025: walking-wake.png\nProcessed 51/1025: accelgor.png\nProcessed 52/1025: makuhita.png\nProcessed 53/1025: seviper.png\nProcessed 54/1025: tirtouga.png\nProcessed 55/1025: lampent.png\nProcessed 56/1025: chi-yu.png\nProcessed 57/1025: ceruledge.png\nProcessed 58/1025: nidoking.png\nProcessed 59/1025: huntail.png\nProcessed 60/1025: nosepass.png\nProcessed 61/1025: carracosta.png\nProcessed 62/1025: nidoqueen.png\nProcessed 63/1025: inkay.png\nProcessed 64/1025: archeops.png\nProcessed 65/1025: avalugg.png\nProcessed 66/1025: grubbin.png\nProcessed 67/1025: sentret.png\nProcessed 68/1025: golbat.png\nProcessed 69/1025: skorupi.png\nProcessed 70/1025: conkeldurr.png\nProcessed 71/1025: jellicent.png\nProcessed 72/1025: yungoos.png\nProcessed 73/1025: meltan.png\nProcessed 74/1025: arctovish.png\nProcessed 75/1025: barraskewda.png\nProcessed 76/1025: luxio.png\nProcessed 77/1025: duraludon.png\nProcessed 78/1025: binacle.png\nProcessed 79/1025: galvantula.png\nProcessed 80/1025: crawdaunt.png\nProcessed 81/1025: hippopotas.png\nProcessed 82/1025: swirlix.png\nProcessed 83/1025: finneon.png\nProcessed 84/1025: kleavor.png\nProcessed 85/1025: medicham.png\nProcessed 86/1025: yamper.png\nProcessed 87/1025: poliwrath.png\nProcessed 88/1025: deoxys.png\nProcessed 89/1025: magnemite.png\nProcessed 90/1025: florges.png\nProcessed 91/1025: serperior.png\nProcessed 92/1025: coalossal.png\nProcessed 93/1025: shinx.png\nProcessed 94/1025: quaquaval.png\nProcessed 95/1025: budew.png\nProcessed 96/1025: shellos.png\nProcessed 97/1025: gothita.png\nProcessed 98/1025: overqwil.png\nProcessed 99/1025: porygon.png\nProcessed 100/1025: alomomola.png\nProcessed 101/1025: urshifu.png\nProcessed 102/1025: electrike.png\nProcessed 103/1025: dratini.png\nProcessed 104/1025: mienfoo.png\nProcessed 105/1025: reshiram.png\nProcessed 106/1025: sawsbuck.png\nProcessed 107/1025: blaziken.png\nProcessed 108/1025: flareon.png\nProcessed 109/1025: gliscor.png\nProcessed 110/1025: heatran.png\nProcessed 111/1025: gholdengo.png\nProcessed 112/1025: thievul.png\nProcessed 113/1025: beedrill.png\nProcessed 114/1025: butterfree.png\nProcessed 115/1025: tadbulb.png\nProcessed 116/1025: fletchinder.png\nProcessed 117/1025: tsareena.png\nProcessed 118/1025: ludicolo.png\nProcessed 119/1025: phanpy.png\nProcessed 120/1025: toxicroak.png\nProcessed 121/1025: cyndaquil.png\nProcessed 122/1025: swellow.png\nProcessed 123/1025: machop.png\nProcessed 124/1025: eternatus.png\nProcessed 125/1025: fletchling.png\nProcessed 126/1025: naganadel.png\nProcessed 127/1025: ariados.png\nProcessed 128/1025: mr-mime.png\nProcessed 129/1025: passimian.png\nProcessed 130/1025: frogadier.png\nProcessed 131/1025: roggenrola.png\nProcessed 132/1025: shroodle.png\nProcessed 133/1025: shedinja.png\nProcessed 134/1025: duosion.png\nProcessed 135/1025: scraggy.png\nProcessed 136/1025: gumshoos.png\nProcessed 137/1025: latias.png\nProcessed 138/1025: shelmet.png\nProcessed 139/1025: sawk.png\nProcessed 140/1025: pawmo.png\nProcessed 141/1025: venusaur.png\nProcessed 142/1025: azelf.png\nProcessed 143/1025: sinistea.png\nProcessed 144/1025: tornadus.png\nProcessed 145/1025: quaxwell.png\nProcessed 146/1025: obstagoon.png\nProcessed 147/1025: musharna.png\nProcessed 148/1025: drampa.png\nProcessed 149/1025: liepard.png\nProcessed 150/1025: aipom.png\nProcessed 151/1025: seaking.png\nProcessed 152/1025: arboliva.png\nProcessed 153/1025: xurkitree.png\nProcessed 154/1025: meditite.png\nProcessed 155/1025: heatmor.png\nProcessed 156/1025: chandelure.png\nProcessed 157/1025: ivysaur.png\nProcessed 158/1025: calyrex.png\nProcessed 159/1025: gible.png\nProcessed 160/1025: corphish.png\nProcessed 161/1025: octillery.png\nProcessed 162/1025: cubone.png\nProcessed 163/1025: crocalor.png\nProcessed 164/1025: dusclops.png\nProcessed 165/1025: koffing.png\nProcessed 166/1025: wo-chien.png\nProcessed 167/1025: gengar.png\nProcessed 168/1025: larvitar.png\nProcessed 169/1025: rampardos.png\nProcessed 170/1025: tyrogue.png\nProcessed 171/1025: wigglytuff.png\nProcessed 172/1025: oricorio.png\nProcessed 173/1025: castform.png\nProcessed 174/1025: quaxly.png\nProcessed 175/1025: dusknoir.png\nProcessed 176/1025: chinchou.png\nProcessed 177/1025: litleo.png\nProcessed 178/1025: floragato.png\nProcessed 179/1025: manectric.png\nProcessed 180/1025: grapploct.png\nProcessed 181/1025: ditto.png\nProcessed 182/1025: scrafty.png\nProcessed 183/1025: great-tusk.png\nProcessed 184/1025: kakuna.png\nProcessed 185/1025: gastrodon.png\nProcessed 186/1025: wingull.png\nProcessed 187/1025: flaaffy.png\nProcessed 188/1025: sunflora.png\nProcessed 189/1025: iron-jugulis.png\nProcessed 190/1025: gastly.png\nProcessed 191/1025: tarountula.png\nProcessed 192/1025: delcatty.png\nProcessed 193/1025: granbull.png\nProcessed 194/1025: iron-leaves.png\nProcessed 195/1025: whismur.png\nProcessed 196/1025: gourgeist.png\nProcessed 197/1025: cleffa.png\nProcessed 198/1025: growlithe.png\nProcessed 199/1025: sigilyph.png\nProcessed 200/1025: cascoon.png\nProcessed 201/1025: pecharunt.png\nProcessed 202/1025: rapidash.png\nProcessed 203/1025: hydreigon.png\nProcessed 204/1025: cryogonal.png\nProcessed 205/1025: stunfisk.png\nProcessed 206/1025: stantler.png\nProcessed 207/1025: dracovish.png\nProcessed 208/1025: tapu-fini.png\nProcessed 209/1025: steenee.png\nProcessed 210/1025: slither-wing.png\nProcessed 211/1025: snubbull.png\nProcessed 212/1025: crabominable.png\nProcessed 213/1025: surskit.png\nProcessed 214/1025: ampharos.png\nProcessed 215/1025: monferno.png\nProcessed 216/1025: terapagos.png\nProcessed 217/1025: rhydon.png\nProcessed 218/1025: orthworm.png\nProcessed 219/1025: clodsire.png\nProcessed 220/1025: arbok.png\nProcessed 221/1025: amaura.png\nProcessed 222/1025: lucario.png\nProcessed 223/1025: hatterene.png\nProcessed 224/1025: pupitar.png\nProcessed 225/1025: elekid.png\nProcessed 226/1025: goomy.png\nProcessed 227/1025: vullaby.png\nProcessed 228/1025: venipede.png\nProcessed 229/1025: doublade.png\nProcessed 230/1025: bellibolt.png\nProcessed 231/1025: regieleki.png\nProcessed 232/1025: tangela.png\nProcessed 233/1025: pignite.png\nProcessed 234/1025: sandile.png\nProcessed 235/1025: brionne.png\nProcessed 236/1025: machamp.png\nProcessed 237/1025: wailord.png\nProcessed 238/1025: rhyhorn.png\nProcessed 239/1025: ninjask.png\nProcessed 240/1025: salandit.png\nProcessed 241/1025: grotle.png\nProcessed 242/1025: dragonair.png\nProcessed 243/1025: mime-jr.png\nProcessed 244/1025: zebstrika.png\nProcessed 245/1025: zarude.png\nProcessed 246/1025: pikachu.png\nProcessed 247/1025: typhlosion.png\nProcessed 248/1025: sizzlipede.png\nProcessed 249/1025: aromatisse.png\nProcessed 250/1025: chimecho.png\nProcessed 251/1025: diggersby.png\nProcessed 252/1025: trevenant.png\nProcessed 253/1025: audino.png\nProcessed 254/1025: dodrio.png\nProcessed 255/1025: azumarill.png\nProcessed 256/1025: perrserker.png\nProcessed 257/1025: aggron.png\nProcessed 258/1025: dialga.png\nProcessed 259/1025: tapu-bulu.png\nProcessed 260/1025: totodile.png\nProcessed 261/1025: tauros.png\nProcessed 262/1025: silcoon.png\nProcessed 263/1025: cottonee.png\nProcessed 264/1025: araquanid.png\nProcessed 265/1025: gallade.png\nProcessed 266/1025: tropius.png\nProcessed 267/1025: bouffalant.png\nProcessed 268/1025: wattrel.png\nProcessed 269/1025: frigibax.png\nProcessed 270/1025: woobat.png\nProcessed 271/1025: archen.png\nProcessed 272/1025: ho-oh.png\nProcessed 273/1025: scorbunny.png\nProcessed 274/1025: iron-valiant.png\nProcessed 275/1025: noibat.png\nProcessed 276/1025: wartortle.png\nProcessed 277/1025: chien-pao.png\nProcessed 278/1025: eevee.png\nProcessed 279/1025: mewtwo.png\nProcessed 280/1025: buneary.png\nProcessed 281/1025: sableye.png\nProcessed 282/1025: bidoof.png\nProcessed 283/1025: rookidee.png\nProcessed 284/1025: scovillain.png\nProcessed 285/1025: braixen.png\nProcessed 286/1025: raging-bolt.png\nProcessed 287/1025: elgyem.png\nProcessed 288/1025: gloom.png\nProcessed 289/1025: electabuzz.png\nProcessed 290/1025: piloswine.png\nProcessed 291/1025: miltank.png\nProcessed 292/1025: guzzlord.png\nProcessed 293/1025: cinccino.png\nProcessed 294/1025: greavard.png\nProcessed 295/1025: hattrem.png\nProcessed 296/1025: hatenna.png\nProcessed 297/1025: anorith.png\nProcessed 298/1025: wurmple.png\nProcessed 299/1025: shelgon.png\nProcessed 300/1025: toxapex.png\nProcessed 301/1025: swalot.png\nProcessed 302/1025: lurantis.png\nProcessed 303/1025: tyrunt.png\nProcessed 304/1025: pidgey.png\nProcessed 305/1025: quagsire.png\nProcessed 306/1025: gurdurr.png\nProcessed 307/1025: amoonguss.png\nProcessed 308/1025: archaludon.png\nProcessed 309/1025: mankey.png\nProcessed 310/1025: illumise.png\nProcessed 311/1025: cacturne.png\nProcessed 312/1025: tinkatink.png\nProcessed 313/1025: igglybuff.png\nProcessed 314/1025: maschiff.png\nProcessed 315/1025: qwilfish.png\nProcessed 316/1025: goldeen.png\nProcessed 317/1025: abra.png\nProcessed 318/1025: tinkatuff.png\nProcessed 319/1025: nidoran-f.png\nProcessed 320/1025: purrloin.png\nProcessed 321/1025: gulpin.png\nProcessed 322/1025: munchlax.png\nProcessed 323/1025: crobat.png\nProcessed 324/1025: volcanion.png\nProcessed 325/1025: lillipup.png\nProcessed 326/1025: xerneas.png\nProcessed 327/1025: lechonk.png\nProcessed 328/1025: lapras.png\nProcessed 329/1025: gyarados.png\nProcessed 330/1025: walrein.png\nProcessed 331/1025: sneasel.png\nProcessed 332/1025: lickilicky.png\nProcessed 333/1025: skuntank.png\nProcessed 334/1025: murkrow.png\nProcessed 335/1025: bibarel.png\nProcessed 336/1025: tentacool.png\nProcessed 337/1025: watchog.png\nProcessed 338/1025: oshawott.png\nProcessed 339/1025: pidgeotto.png\nProcessed 340/1025: espurr.png\nProcessed 341/1025: houndoom.png\nProcessed 342/1025: nidorino.png\nProcessed 343/1025: hariyama.png\nProcessed 344/1025: milcery.png\nProcessed 345/1025: talonflame.png\nProcessed 346/1025: dustox.png\nProcessed 347/1025: mimikyu.png\nProcessed 348/1025: kommo-o.png\nProcessed 349/1025: weepinbell.png\nProcessed 350/1025: seel.png\nProcessed 351/1025: houndstone.png\nProcessed 352/1025: pyukumuku.png\nProcessed 353/1025: naclstack.png\nProcessed 354/1025: annihilape.png\nProcessed 355/1025: pineco.png\nProcessed 356/1025: cosmog.png\nProcessed 357/1025: kadabra.png\nProcessed 358/1025: swablu.png\nProcessed 359/1025: skitty.png\nProcessed 360/1025: regidrago.png\nProcessed 361/1025: masquerain.png\nProcessed 362/1025: minun.png\nProcessed 363/1025: heracross.png\nProcessed 364/1025: cursola.png\nProcessed 365/1025: trapinch.png\nProcessed 366/1025: girafarig.png\nProcessed 367/1025: tyrantrum.png\nProcessed 368/1025: graveler.png\nProcessed 369/1025: corvisquire.png\nProcessed 370/1025: poochyena.png\nProcessed 371/1025: electivire.png\nProcessed 372/1025: shiinotic.png\nProcessed 373/1025: melmetal.png\nProcessed 374/1025: ledian.png\nProcessed 375/1025: cherrim.png\nProcessed 376/1025: popplio.png\nProcessed 377/1025: sandy-shocks.png\nProcessed 378/1025: polteageist.png\nProcessed 379/1025: primeape.png\nProcessed 380/1025: glastrier.png\nProcessed 381/1025: vanillish.png\nProcessed 382/1025: aegislash.png\nProcessed 383/1025: probopass.png\nProcessed 384/1025: skiploom.png\nProcessed 385/1025: chesnaught.png\nProcessed 386/1025: kyurem.png\nProcessed 387/1025: drakloak.png\nProcessed 388/1025: carnivine.png\nProcessed 389/1025: torracat.png\nProcessed 390/1025: gardevoir.png\nProcessed 391/1025: pinsir.png\nProcessed 392/1025: snorunt.png\nProcessed 393/1025: samurott.png\nProcessed 394/1025: necrozma.png\nProcessed 395/1025: darmanitan.png\nProcessed 396/1025: charcadet.png\nProcessed 397/1025: smeargle.png\nProcessed 398/1025: cetoddle.png\nProcessed 399/1025: rellor.png\nProcessed 400/1025: inteleon.png\nProcessed 401/1025: magby.png\nProcessed 402/1025: ursaring.png\nProcessed 403/1025: crabrawler.png\nProcessed 404/1025: haxorus.png\nProcessed 405/1025: geodude.png\nProcessed 406/1025: foongus.png\nProcessed 407/1025: hippowdon.png\nProcessed 408/1025: poltchageist.png\nProcessed 409/1025: toedscool.png\nProcessed 410/1025: ursaluna.png\nProcessed 411/1025: delibird.png\nProcessed 412/1025: oranguru.png\nProcessed 413/1025: drapion.png\nProcessed 414/1025: koraidon.png\nProcessed 415/1025: smoochum.png\nProcessed 416/1025: scyther.png\nProcessed 417/1025: stunky.png\nProcessed 418/1025: swadloon.png\nProcessed 419/1025: klefki.png\nProcessed 420/1025: vivillon.png\nProcessed 421/1025: wormadam.png\nProcessed 422/1025: honedge.png\nProcessed 423/1025: entei.png\nProcessed 424/1025: servine.png\nProcessed 425/1025: zubat.png\nProcessed 426/1025: umbreon.png\nProcessed 427/1025: reuniclus.png\nProcessed 428/1025: froslass.png\nProcessed 429/1025: pheromosa.png\nProcessed 430/1025: brute-bonnet.png\nProcessed 431/1025: magcargo.png\nProcessed 432/1025: boldore.png\nProcessed 433/1025: genesect.png\nProcessed 434/1025: larvesta.png\nProcessed 435/1025: meowscarada.png\nProcessed 436/1025: golett.png\nProcessed 437/1025: kartana.png\nProcessed 438/1025: voltorb.png\nProcessed 439/1025: regirock.png\nProcessed 440/1025: cherubi.png\nProcessed 441/1025: barboach.png\nProcessed 442/1025: wooper.png\nProcessed 443/1025: escavalier.png\nProcessed 444/1025: vileplume.png\nProcessed 445/1025: mismagius.png\nProcessed 446/1025: blastoise.png\nProcessed 447/1025: litten.png\nProcessed 448/1025: sealeo.png\nProcessed 449/1025: dreepy.png\nProcessed 450/1025: snivy.png\nProcessed 451/1025: keldeo.png\nProcessed 452/1025: snorlax.png\nProcessed 453/1025: zamazenta.png\nProcessed 454/1025: lopunny.png\nProcessed 455/1025: persian.png\nProcessed 456/1025: beartic.png\nProcessed 457/1025: gothorita.png\nProcessed 458/1025: giratina.png\nProcessed 459/1025: rattata.png\nProcessed 460/1025: starly.png\nProcessed 461/1025: bewear.png\nProcessed 462/1025: nymble.png\nProcessed 463/1025: houndour.png\nProcessed 464/1025: eelektrik.png\nProcessed 465/1025: appletun.png\nProcessed 466/1025: quilava.png\nProcessed 467/1025: whimsicott.png\nProcessed 468/1025: magikarp.png\nProcessed 469/1025: skeledirge.png\nProcessed 470/1025: drowzee.png\nProcessed 471/1025: squawkabilly.png\nProcessed 472/1025: claydol.png\nProcessed 473/1025: clamperl.png\nProcessed 474/1025: seedot.png\nProcessed 475/1025: dottler.png\nProcessed 476/1025: garchomp.png\nProcessed 477/1025: skiddo.png\nProcessed 478/1025: hypno.png\nProcessed 479/1025: weedle.png\nProcessed 480/1025: garganacl.png\nProcessed 481/1025: greedent.png\nProcessed 482/1025: stakataka.png\nProcessed 483/1025: ambipom.png\nProcessed 484/1025: spoink.png\nProcessed 485/1025: pikipek.png\nProcessed 486/1025: frillish.png\nProcessed 487/1025: cutiefly.png\nProcessed 488/1025: honchkrow.png\nProcessed 489/1025: tympole.png\nProcessed 490/1025: ting-lu.png\nProcessed 491/1025: meganium.png\nProcessed 492/1025: incineroar.png\nProcessed 493/1025: landorus.png\nProcessed 494/1025: lileep.png\nProcessed 495/1025: pachirisu.png\nProcessed 496/1025: iron-moth.png\nProcessed 497/1025: tyranitar.png\nProcessed 498/1025: magnezone.png\nProcessed 499/1025: beldum.png\nProcessed 500/1025: glameow.png\nProcessed 501/1025: dondozo.png\nProcessed 502/1025: swanna.png\nProcessed 503/1025: primarina.png\nProcessed 504/1025: nuzleaf.png\nProcessed 505/1025: zangoose.png\nProcessed 506/1025: gogoat.png\nProcessed 507/1025: jolteon.png\nProcessed 508/1025: moltres.png\nProcessed 509/1025: dragonite.png\nProcessed 510/1025: golurk.png\nProcessed 511/1025: metapod.png\nProcessed 512/1025: golisopod.png\nProcessed 513/1025: marowak.png\nProcessed 514/1025: gigalith.png\nProcessed 515/1025: emboar.png\nProcessed 516/1025: exeggutor.png\nProcessed 517/1025: magneton.png\nProcessed 518/1025: mesprit.png\nProcessed 519/1025: dipplin.png\nProcessed 520/1025: groudon.png\nProcessed 521/1025: wobbuffet.png\nProcessed 522/1025: delphox.png\nProcessed 523/1025: exploud.png\nProcessed 524/1025: dracozolt.png\nProcessed 525/1025: mantyke.png\nProcessed 526/1025: malamar.png\nProcessed 527/1025: simisage.png\nProcessed 528/1025: espeon.png\nProcessed 529/1025: paras.png\nProcessed 530/1025: noctowl.png\nProcessed 531/1025: hitmonlee.png\nProcessed 532/1025: slaking.png\nProcessed 533/1025: solrock.png\nProcessed 534/1025: pelipper.png\nProcessed 535/1025: komala.png\nProcessed 536/1025: poliwhirl.png\nProcessed 537/1025: palpitoad.png\nProcessed 538/1025: eelektross.png\nProcessed 539/1025: patrat.png\nProcessed 540/1025: spheal.png\nProcessed 541/1025: seadra.png\nProcessed 542/1025: lotad.png\nProcessed 543/1025: flutter-mane.png\nProcessed 544/1025: ralts.png\nProcessed 545/1025: registeel.png\nProcessed 546/1025: slurpuff.png\nProcessed 547/1025: arcanine.png\nProcessed 548/1025: fomantis.png\nProcessed 549/1025: smoliv.png\nProcessed 550/1025: eiscue.png\nProcessed 551/1025: tangrowth.png\nProcessed 552/1025: aerodactyl.png\nProcessed 553/1025: pincurchin.png\nProcessed 554/1025: bronzor.png\nProcessed 555/1025: sylveon.png\nProcessed 556/1025: munkidori.png\nProcessed 557/1025: kyogre.png\nProcessed 558/1025: sharpedo.png\nProcessed 559/1025: cufant.png\nProcessed 560/1025: flabebe.png\nProcessed 561/1025: quilladin.png\nProcessed 562/1025: feebas.png\nProcessed 563/1025: metagross.png\nProcessed 564/1025: maractus.png\nProcessed 565/1025: muk.png\nProcessed 566/1025: empoleon.png\nProcessed 567/1025: chimchar.png\nProcessed 568/1025: sandaconda.png\nProcessed 569/1025: staryu.png\nProcessed 570/1025: scream-tail.png\nProcessed 571/1025: pancham.png\nProcessed 572/1025: dwebble.png\nProcessed 573/1025: bruxish.png\nProcessed 574/1025: joltik.png\nProcessed 575/1025: armaldo.png\nProcessed 576/1025: grimmsnarl.png\nProcessed 577/1025: phione.png\nProcessed 578/1025: frosmoth.png\nProcessed 579/1025: rhyperior.png\nProcessed 580/1025: pansear.png\nProcessed 581/1025: alcremie.png\nProcessed 582/1025: happiny.png\nProcessed 583/1025: spearow.png\nProcessed 584/1025: helioptile.png\nProcessed 585/1025: kingambit.png\nProcessed 586/1025: fearow.png\nProcessed 587/1025: wooloo.png\nProcessed 588/1025: prinplup.png\nProcessed 589/1025: diancie.png\nProcessed 590/1025: rabsca.png\nProcessed 591/1025: poipole.png\nProcessed 592/1025: ogerpon.png\nProcessed 593/1025: klink.png\nProcessed 594/1025: farfetchd.png\nProcessed 595/1025: gorebyss.png\nProcessed 596/1025: iron-bundle.png\nProcessed 597/1025: noivern.png\nProcessed 598/1025: herdier.png\nProcessed 599/1025: tynamo.png\nProcessed 600/1025: whiscash.png\nProcessed 601/1025: nidoran-m.png\nProcessed 602/1025: diglett.png\nProcessed 603/1025: wyrdeer.png\nProcessed 604/1025: krokorok.png\nProcessed 605/1025: scizor.png\nProcessed 606/1025: nihilego.png\nProcessed 607/1025: skarmory.png\nProcessed 608/1025: remoraid.png\nProcessed 609/1025: shaymin.png\nProcessed 610/1025: hitmontop.png\nProcessed 611/1025: snom.png\nProcessed 612/1025: cosmoem.png\nProcessed 613/1025: miraidon.png\nProcessed 614/1025: slowbro.png\nProcessed 615/1025: numel.png\nProcessed 616/1025: glalie.png\nProcessed 617/1025: vaporeon.png\nProcessed 618/1025: porygon-z.png\nProcessed 619/1025: lunatone.png\nProcessed 620/1025: machoke.png\nProcessed 621/1025: mareanie.png\nProcessed 622/1025: bastiodon.png\nProcessed 623/1025: mandibuzz.png\nProcessed 624/1025: arceus.png\nProcessed 625/1025: throh.png\nProcessed 626/1025: abomasnow.png\nProcessed 627/1025: bronzong.png\nProcessed 628/1025: flittle.png\nProcessed 629/1025: greninja.png\nProcessed 630/1025: hoppip.png\nProcessed 631/1025: kecleon.png\nProcessed 632/1025: crustle.png\nProcessed 633/1025: shuppet.png\nProcessed 634/1025: stonjourner.png\nProcessed 635/1025: gossifleur.png\nProcessed 636/1025: ducklett.png\nProcessed 637/1025: plusle.png\nProcessed 638/1025: poliwag.png\nProcessed 639/1025: treecko.png\nProcessed 640/1025: torchic.png\nProcessed 641/1025: kabutops.png\nProcessed 642/1025: dunsparce.png\nProcessed 643/1025: flygon.png\nProcessed 644/1025: morpeko.png\nProcessed 645/1025: spectrier.png\nProcessed 646/1025: orbeetle.png\nProcessed 647/1025: dugtrio.png\nProcessed 648/1025: marill.png\nProcessed 649/1025: magearna.png\nProcessed 650/1025: toxtricity.png\nProcessed 651/1025: fezandipiti.png\nProcessed 652/1025: silvally.png\nProcessed 653/1025: cacnea.png\nProcessed 654/1025: slugma.png\nProcessed 655/1025: oddish.png\nProcessed 656/1025: jirachi.png\nProcessed 657/1025: veluza.png\nProcessed 658/1025: ribombee.png\nProcessed 659/1025: glimmora.png\nProcessed 660/1025: litwick.png\nProcessed 661/1025: pidgeot.png\nProcessed 662/1025: sandslash.png\nProcessed 663/1025: drilbur.png\nProcessed 664/1025: cranidos.png\nProcessed 665/1025: chansey.png\nProcessed 666/1025: blacephalon.png\nProcessed 667/1025: camerupt.png\nProcessed 668/1025: arctozolt.png\nProcessed 669/1025: dudunsparce.png\nProcessed 670/1025: pichu.png\nProcessed 671/1025: duskull.png\nProcessed 672/1025: palafin.png\nProcessed 673/1025: kangaskhan.png\nProcessed 674/1025: chingling.png\nProcessed 675/1025: whirlipede.png\nProcessed 676/1025: silicobra.png\nProcessed 677/1025: ferroseed.png\nProcessed 678/1025: toucannon.png\nProcessed 679/1025: barbaracle.png\nProcessed 680/1025: salamence.png\nProcessed 681/1025: zekrom.png\nProcessed 682/1025: haunter.png\nProcessed 683/1025: glaceon.png\nProcessed 684/1025: golduck.png\nProcessed 685/1025: milotic.png\nProcessed 686/1025: politoed.png\nProcessed 687/1025: indeedee.png\nProcessed 688/1025: absol.png\nProcessed 689/1025: mienshao.png\nProcessed 690/1025: sprigatito.png\nProcessed 691/1025: aron.png\nProcessed 692/1025: tepig.png\nProcessed 693/1025: klang.png\nProcessed 694/1025: zapdos.png\nProcessed 695/1025: clauncher.png\nProcessed 696/1025: riolu.png\nProcessed 697/1025: skwovet.png\nProcessed 698/1025: raichu.png\nProcessed 699/1025: varoom.png\nProcessed 700/1025: hitmonchan.png\nProcessed 701/1025: slakoth.png\nProcessed 702/1025: ponyta.png\nProcessed 703/1025: togepi.png\nProcessed 704/1025: snover.png\nProcessed 705/1025: dubwool.png\nProcessed 706/1025: solosis.png\nProcessed 707/1025: fraxure.png\nProcessed 708/1025: caterpie.png\nProcessed 709/1025: iron-hands.png\nProcessed 710/1025: gimmighoul.png\nProcessed 711/1025: psyduck.png\nProcessed 712/1025: teddiursa.png\nProcessed 713/1025: tapu-lele.png\nProcessed 714/1025: piplup.png\nProcessed 715/1025: flamigo.png\nProcessed 716/1025: okidogi.png\nProcessed 717/1025: nickit.png\nProcessed 718/1025: darumaka.png\nProcessed 719/1025: finizen.png\nProcessed 720/1025: weavile.png\nProcessed 721/1025: mawile.png\nProcessed 722/1025: sewaddle.png\nProcessed 723/1025: unfezant.png\nProcessed 724/1025: buzzwole.png\nProcessed 725/1025: rockruff.png\nProcessed 726/1025: manaphy.png\nProcessed 727/1025: bunnelby.png\nProcessed 728/1025: seismitoad.png\nProcessed 729/1025: simipour.png\nProcessed 730/1025: vanilluxe.png\nProcessed 731/1025: cubchoo.png\nProcessed 732/1025: mantine.png\nProcessed 733/1025: maushold.png\nProcessed 734/1025: basculin.png\nProcessed 735/1025: dartrix.png\nProcessed 736/1025: weezing.png\nProcessed 737/1025: corsola.png\nProcessed 738/1025: roserade.png\nProcessed 739/1025: carbink.png\nProcessed 740/1025: heliolisk.png\nProcessed 741/1025: omanyte.png\nProcessed 742/1025: rufflet.png\nProcessed 743/1025: zygarde.png\nProcessed 744/1025: mothim.png\nProcessed 745/1025: klinklang.png\nProcessed 746/1025: sliggoo.png\nProcessed 747/1025: raticate.png\nProcessed 748/1025: grookey.png\nProcessed 749/1025: carkol.png\nProcessed 750/1025: dhelmise.png\nProcessed 751/1025: vibrava.png\nProcessed 752/1025: wiglett.png\nProcessed 753/1025: shiftry.png\nProcessed 754/1025: swampert.png\nProcessed 755/1025: excadrill.png\nProcessed 756/1025: brambleghast.png\nProcessed 757/1025: morelull.png\nProcessed 758/1025: fuecoco.png\nProcessed 759/1025: chatot.png\nProcessed 760/1025: klawf.png\nProcessed 761/1025: jigglypuff.png\nProcessed 762/1025: enamorus.png\nProcessed 763/1025: loudred.png\nProcessed 764/1025: forretress.png\nProcessed 765/1025: togetic.png\nProcessed 766/1025: feraligatr.png\nProcessed 767/1025: bisharp.png\nProcessed 768/1025: lycanroc.png\nProcessed 769/1025: iron-treads.png\nProcessed 770/1025: mabosstiff.png\nProcessed 771/1025: vikavolt.png\nProcessed 772/1025: victreebel.png\nProcessed 773/1025: decidueye.png\nProcessed 774/1025: cinderace.png\nProcessed 775/1025: spinarak.png\nProcessed 776/1025: onix.png\nProcessed 777/1025: wugtrio.png\nProcessed 778/1025: aurorus.png\nProcessed 779/1025: venonat.png\nProcessed 780/1025: thundurus.png\nProcessed 781/1025: wimpod.png\nProcessed 782/1025: nidorina.png\nProcessed 783/1025: tranquill.png\nProcessed 784/1025: hydrapple.png\nProcessed 785/1025: farigiraf.png\nProcessed 786/1025: buizel.png\nProcessed 787/1025: mightyena.png\nProcessed 788/1025: vulpix.png\nProcessed 789/1025: clawitzer.png\nProcessed 790/1025: revavroom.png\nProcessed 791/1025: lilligant.png\nProcessed 792/1025: togedemaru.png\nProcessed 793/1025: minccino.png\nProcessed 794/1025: roaring-moon.png\nProcessed 795/1025: charjabug.png\nProcessed 796/1025: doduo.png\nProcessed 797/1025: ferrothorn.png\nProcessed 798/1025: misdreavus.png\nProcessed 799/1025: kingdra.png\nProcessed 800/1025: shuckle.png\nProcessed 801/1025: rayquaza.png\nProcessed 802/1025: palossand.png\nProcessed 803/1025: spidops.png\nProcessed 804/1025: mudbray.png\nProcessed 805/1025: kricketune.png\nProcessed 806/1025: skrelp.png\nProcessed 807/1025: luxray.png\nProcessed 808/1025: ledyba.png\nProcessed 809/1025: baltoy.png\nProcessed 810/1025: basculegion.png\nProcessed 811/1025: trumbeak.png\nProcessed 812/1025: dewpider.png\nProcessed 813/1025: applin.png\nProcessed 814/1025: sandygast.png\nProcessed 815/1025: yanma.png\nProcessed 816/1025: kubfu.png\nProcessed 817/1025: lugia.png\nProcessed 818/1025: turtonator.png\nProcessed 819/1025: hawlucha.png\nProcessed 820/1025: leavanny.png\nProcessed 821/1025: vespiquen.png\nProcessed 822/1025: solgaleo.png\nProcessed 823/1025: squirtle.png\nProcessed 824/1025: slowking.png\nProcessed 825/1025: exeggcute.png\nProcessed 826/1025: emolga.png\nProcessed 827/1025: carvanha.png\nProcessed 828/1025: zacian.png\nProcessed 829/1025: grafaiai.png\nProcessed 830/1025: clefairy.png\nProcessed 831/1025: bayleef.png\nProcessed 832/1025: drifloon.png\nProcessed 833/1025: stufful.png\nProcessed 834/1025: sceptile.png\nProcessed 835/1025: azurill.png\nProcessed 836/1025: chespin.png\nProcessed 837/1025: jangmo-o.png\nProcessed 838/1025: panpour.png\nProcessed 839/1025: meloetta.png\nProcessed 840/1025: glimmet.png\nProcessed 841/1025: sinistcha.png\nProcessed 842/1025: staravia.png\nProcessed 843/1025: linoone.png\nProcessed 844/1025: tinkaton.png\nProcessed 845/1025: iron-crown.png\nProcessed 846/1025: spiritomb.png\nProcessed 847/1025: zoroark.png\nProcessed 848/1025: chikorita.png\nProcessed 849/1025: vanillite.png\nProcessed 850/1025: scatterbug.png\nProcessed 851/1025: spinda.png\nProcessed 852/1025: beheeyem.png\nProcessed 853/1025: grimer.png\nProcessed 854/1025: pawmot.png\nProcessed 855/1025: leafeon.png\nProcessed 856/1025: cyclizar.png\nProcessed 857/1025: deerling.png\nProcessed 858/1025: celebi.png\nProcessed 859/1025: breloom.png\nProcessed 860/1025: turtwig.png\nProcessed 861/1025: zweilous.png\nProcessed 862/1025: burmy.png\nProcessed 863/1025: dolliv.png\nProcessed 864/1025: copperajah.png\nProcessed 865/1025: swinub.png\nProcessed 866/1025: axew.png\nProcessed 867/1025: dragapult.png\nProcessed 868/1025: pangoro.png\nProcessed 869/1025: darkrai.png\nProcessed 870/1025: krabby.png\nProcessed 871/1025: dewgong.png\nProcessed 872/1025: eldegoss.png\nProcessed 873/1025: sirfetchd.png\nProcessed 874/1025: volcarona.png\nProcessed 875/1025: vigoroth.png\nProcessed 876/1025: bombirdier.png\nProcessed 877/1025: falinks.png\nProcessed 878/1025: ninetales.png\nProcessed 879/1025: pawniard.png\nProcessed 880/1025: rolycoly.png\nProcessed 881/1025: blitzle.png\nProcessed 882/1025: porygon2.png\nProcessed 883/1025: croagunk.png\nProcessed 884/1025: xatu.png\nProcessed 885/1025: lairon.png\nProcessed 886/1025: tentacruel.png\nProcessed 887/1025: toedscruel.png\nProcessed 888/1025: krookodile.png\nProcessed 889/1025: gothitelle.png\nProcessed 890/1025: regice.png\nProcessed 891/1025: goodra.png\nProcessed 892/1025: thwackey.png\nProcessed 893/1025: meowth.png\nProcessed 894/1025: grovyle.png\nProcessed 895/1025: volbeat.png\nProcessed 896/1025: unown.png\nProcessed 897/1025: terrakion.png\nProcessed 898/1025: togekiss.png\nProcessed 899/1025: kricketot.png\nProcessed 900/1025: zeraora.png\nProcessed 901/1025: bellsprout.png\nProcessed 902/1025: luvdisc.png\nProcessed 903/1025: garbodor.png\nProcessed 904/1025: horsea.png\nProcessed 905/1025: nincada.png\nProcessed 906/1025: dragalge.png\nProcessed 907/1025: electrode.png\nProcessed 908/1025: salazzle.png\nProcessed 909/1025: fennekin.png\nProcessed 910/1025: metang.png\nProcessed 911/1025: donphan.png\nProcessed 912/1025: jynx.png\nProcessed 913/1025: yamask.png\nProcessed 914/1025: druddigon.png\nProcessed 915/1025: kabuto.png\nProcessed 916/1025: magmar.png\nProcessed 917/1025: swoobat.png\nProcessed 918/1025: braviary.png\nProcessed 919/1025: iron-thorns.png\nProcessed 920/1025: charmander.png\nProcessed 921/1025: baxcalibur.png\nProcessed 922/1025: rowlet.png\nProcessed 923/1025: lombre.png\nProcessed 924/1025: durant.png\nProcessed 925/1025: golem.png\nProcessed 926/1025: sunkern.png\nProcessed 927/1025: chewtle.png\nProcessed 928/1025: dewott.png\nProcessed 929/1025: croconaw.png\nProcessed 930/1025: clobbopus.png\nProcessed 931/1025: cobalion.png\nProcessed 932/1025: pansage.png\nProcessed 933/1025: deino.png\nProcessed 934/1025: spewpa.png\nProcessed 935/1025: floette.png\nProcessed 936/1025: combee.png\nProcessed 937/1025: flapple.png\nProcessed 938/1025: iron-boulder.png\nProcessed 939/1025: cofagrigus.png\nProcessed 940/1025: drednaw.png\nProcessed 941/1025: furfrou.png\nProcessed 942/1025: gouging-fire.png\nProcessed 943/1025: furret.png\nProcessed 944/1025: clefable.png\nProcessed 945/1025: wailmer.png\nProcessed 946/1025: grumpig.png\nProcessed 947/1025: charmeleon.png\nProcessed 948/1025: phantump.png\nProcessed 949/1025: venomoth.png\nProcessed 950/1025: natu.png\nProcessed 951/1025: boltund.png\nProcessed 952/1025: simisear.png\nProcessed 953/1025: blissey.png\nProcessed 954/1025: spritzee.png\nProcessed 955/1025: virizion.png\nProcessed 956/1025: armarouge.png\nProcessed 957/1025: scolipede.png\nProcessed 958/1025: rillaboom.png\nProcessed 959/1025: lokix.png\nProcessed 960/1025: fidough.png\nProcessed 961/1025: mamoswine.png\nProcessed 962/1025: minior.png\nProcessed 963/1025: mr-rime.png\nProcessed 964/1025: kirlia.png\nProcessed 965/1025: capsakid.png\nProcessed 966/1025: omastar.png\nProcessed 967/1025: drizzile.png\nProcessed 968/1025: bramblin.png\nProcessed 969/1025: hoopa.png\nProcessed 970/1025: steelix.png\nProcessed 971/1025: rotom.png\nProcessed 972/1025: hoothoot.png\nProcessed 973/1025: karrablast.png\nProcessed 974/1025: torkoal.png\nProcessed 975/1025: cramorant.png\nProcessed 976/1025: victini.png\nProcessed 977/1025: cloyster.png\nProcessed 978/1025: espathra.png\nProcessed 979/1025: relicanth.png\nProcessed 980/1025: wynaut.png\nProcessed 981/1025: raboot.png\nProcessed 982/1025: zorua.png\nProcessed 983/1025: blipbug.png\nProcessed 984/1025: timburr.png\nProcessed 985/1025: drifblim.png\nProcessed 986/1025: altaria.png\nProcessed 987/1025: cradily.png\nProcessed 988/1025: mudkip.png\nProcessed 989/1025: toxel.png\nProcessed 990/1025: lunala.png\nProcessed 991/1025: mudsdale.png\nProcessed 992/1025: shellder.png\nProcessed 993/1025: jumpluff.png\nProcessed 994/1025: bounsweet.png\nProcessed 995/1025: trubbish.png\nProcessed 996/1025: beautifly.png\nProcessed 997/1025: bagon.png\nProcessed 998/1025: oinkologne.png\nProcessed 999/1025: celesteela.png\nProcessed 1000/1025: centiskorch.png\nProcessed 1001/1025: marshtomp.png\nProcessed 1002/1025: uxie.png\nProcessed 1003/1025: suicune.png\nProcessed 1004/1025: pumpkaboo.png\nProcessed 1005/1025: sudowoodo.png\nProcessed 1006/1025: bergmite.png\nProcessed 1007/1025: lanturn.png\nProcessed 1008/1025: kingler.png\nProcessed 1009/1025: shroomish.png\nProcessed 1010/1025: gligar.png\nProcessed 1011/1025: roselia.png\nProcessed 1012/1025: bonsly.png\nProcessed 1013/1025: tatsugiri.png\nProcessed 1014/1025: type-null.png\nProcessed 1015/1025: dachsbun.png\nProcessed 1016/1025: combusken.png\nProcessed 1017/1025: arctibax.png\nProcessed 1018/1025: slowpoke.png\nProcessed 1019/1025: nacli.png\nProcessed 1020/1025: floatzel.png\nProcessed 1021/1025: latios.png\nProcessed 1022/1025: mew.png\nProcessed 1023/1025: ekans.png\nProcessed 1024/1025: cetitan.png\nProcessed 1025/1025: hakamo-o.png\nProcessing complete! Successfully processed 1025/1025 sprites.",
    "crumbs": [
      "lib",
      "data_utils"
    ]
  }
]