{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.linalg import svd\n",
    "\n",
    "from pattern_lens.activations import activations_main\n",
    "from pattern_lens.attn_figure_funcs import register_attn_figure_func\n",
    "from pattern_lens.figure_util import matplotlib_figure_saver, save_matrix_wrapper\n",
    "from pattern_lens.figures import figures_main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# defining your own functions\n",
    "\n",
    "by default, only the raw attention matrices (saved as png) are saved. you can define your own functions like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and register your own functions\n",
    "# don't take these too seriously, they're just examples\n",
    "\n",
    "\n",
    "# using matplotlib_figure_saver -- define a function that takes matrix and `plt.Axes`, modify the axes\n",
    "@register_attn_figure_func\n",
    "@matplotlib_figure_saver(fmt=\"svgz\")\n",
    "def svd_spectra(attn_matrix: np.ndarray, ax: plt.Axes) -> None:\n",
    "\t# Perform SVD\n",
    "\tU, s, Vh = svd(attn_matrix)\n",
    "\n",
    "\t# Plot singular values\n",
    "\tax.plot(s, \"o-\")\n",
    "\tax.set_yscale(\"log\")\n",
    "\tax.set_xlabel(\"Singular Value Index\")\n",
    "\tax.set_ylabel(\"Singular Value\")\n",
    "\tax.set_title(\"Singular Value Spectrum of Attention Matrix\")\n",
    "\n",
    "\n",
    "# manually creating and saving a figure\n",
    "@register_attn_figure_func\n",
    "def attention_flow(attn_matrix: np.ndarray, path: Path) -> None:\n",
    "\t\"\"\"Visualize attention as flows between tokens.\n",
    "\n",
    "\tCreates a simplified Sankey-style diagram where line thickness and color\n",
    "\tintensity represent attention strength.\n",
    "\t\"\"\"\n",
    "\tfig, ax = plt.subplots(figsize=(6, 6))\n",
    "\tn_tokens: int = attn_matrix.shape[0]\n",
    "\n",
    "\t# Create positions for tokens on left and right\n",
    "\tleft_pos: np.ndarray = np.arange(n_tokens)\n",
    "\tright_pos: np.ndarray = np.arange(n_tokens)\n",
    "\n",
    "\t# Plot flows\n",
    "\tfor i in range(n_tokens):\n",
    "\t\tfor j in range(n_tokens):\n",
    "\t\t\tweight = attn_matrix[i, j]\n",
    "\t\t\tif weight > 0.05:  # Only plot stronger connections\n",
    "\t\t\t\tax.plot(\n",
    "\t\t\t\t\t[0, 1],\n",
    "\t\t\t\t\t[left_pos[i], right_pos[j]],\n",
    "\t\t\t\t\talpha=weight,\n",
    "\t\t\t\t\tlinewidth=weight * 5,\n",
    "\t\t\t\t\tcolor=\"blue\",\n",
    "\t\t\t\t)\n",
    "\n",
    "\tax.set_xlim(-0.1, 1.1)\n",
    "\tax.set_ylim(-1, n_tokens)\n",
    "\tax.axis(\"off\")\n",
    "\tax.set_title(\"Attention Flow Between Positions\")\n",
    "\n",
    "\t# be sure to save the figure as `function_name.format` in the given location\n",
    "\tfig.savefig(path / \"attention_flow.svgz\", format=\"svgz\")\n",
    "\n",
    "\n",
    "@register_attn_figure_func\n",
    "@save_matrix_wrapper(fmt=\"svgz\")\n",
    "def gram_matrix(attn_matrix: np.ndarray) -> np.ndarray:\n",
    "\treturn attn_matrix @ attn_matrix.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# running the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up directories\n",
    "PATH: Path = Path(\"tests/_temp/nb-demo/\")\n",
    "PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "\\ (0.80s) loading model                                                        Loaded pretrained model pythia-14m into HookedTransformer\n",
      "✔️  (0.87s) loading model                                                      \n",
      "loaded pythia-14m with 14M (14114688) parameters\n",
      "\tmodel devices: {device(type='cuda', index=0)}\n",
      "✔️  (0.00s) saving model info to tests/_temp/nb-demo/pythia-14m                \n",
      "✔️  (0.00s) loading prompts from prompts_path = 'data/pile_5.jsonl'            \n",
      "5 prompts loaded\n",
      "✔️  (0.00s) writing index.html                                                 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing activations: 100%|██████████| 5/5 [00:00<00:00, 18.96prompt/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️  (0.00s) updating jsonl metadata for models and prompts                     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# generate activations\n",
    "activations_main(\n",
    "\tmodel_name=\"pythia-14m\",\n",
    "\tsave_path=PATH,\n",
    "\tprompts_path=\"data/pile_5.jsonl\",\n",
    "\tmin_chars=10,\n",
    "\tmax_chars=100,\n",
    "\tn_samples=5,\n",
    "\traw_prompts=True,\n",
    "\tforce=True,\n",
    "\tno_index_html=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️  (0.00s) setting up paths                                                   \n",
      "✔️  (0.00s) loading prompts                                                    \n",
      "5 prompts loaded\n",
      "4 figure functions loaded\n",
      "\traw, svd_spectra, attention_flow, gram_matrix\n",
      "chunksize: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Making figures:   0%|          | 0/5 [00:00<?, ?prompt/s]/home/miv/projects/attn/pattern-lens/pattern_lens/figure_util.py:110: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig, ax = plt.subplots(figsize=(10, 10))\n",
      "/tmp/ipykernel_350852/45618092.py:28: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig, ax = plt.subplots(figsize=(6, 6))\n",
      "Making figures:  20%|██        | 1/5 [00:10<00:42, 10.58s/prompt]/home/miv/projects/attn/pattern-lens/pattern_lens/figure_util.py:110: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig, ax = plt.subplots(figsize=(10, 10))\n",
      "/home/miv/projects/attn/pattern-lens/pattern_lens/figure_util.py:110: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig, ax = plt.subplots(figsize=(10, 10))\n",
      "/home/miv/projects/attn/pattern-lens/pattern_lens/figure_util.py:110: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig, ax = plt.subplots(figsize=(10, 10))\n",
      "/tmp/ipykernel_350852/45618092.py:28: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig, ax = plt.subplots(figsize=(6, 6))\n",
      "/tmp/ipykernel_350852/45618092.py:28: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig, ax = plt.subplots(figsize=(6, 6))\n",
      "/home/miv/projects/attn/pattern-lens/pattern_lens/figure_util.py:110: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig, ax = plt.subplots(figsize=(10, 10))\n",
      "/tmp/ipykernel_350852/45618092.py:28: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig, ax = plt.subplots(figsize=(6, 6))\n",
      "/tmp/ipykernel_350852/45618092.py:28: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig, ax = plt.subplots(figsize=(6, 6))\n",
      "Making figures: 100%|██████████| 5/5 [00:14<00:00,  2.81s/prompt]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️  (0.00s) updating jsonl metadata for models and functions                   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# generate figures\n",
    "figures_main(\n",
    "\tmodel_name=\"pythia-14m\",\n",
    "\tsave_path=PATH,\n",
    "\tn_samples=5,\n",
    "\tforce=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
