{"n_layers": 6, "d_model": 128, "n_ctx": 2048, "d_head": 32, "model_name": "pythia-14m", "n_heads": 4, "d_mlp": 512, "act_fn": "gelu", "d_vocab": 50304, "eps": 1e-05, "use_attn_result": false, "use_attn_scale": true, "attn_scale": 5.656854249492381, "use_split_qkv_input": false, "use_hook_mlp_in": false, "use_attn_in": false, "use_local_attn": false, "ungroup_grouped_query_attention": false, "original_architecture": "GPTNeoXForCausalLM", "from_checkpoint": false, "checkpoint_index": null, "checkpoint_label_type": null, "checkpoint_value": null, "tokenizer_name": "EleutherAI/pythia-14m", "window_size": null, "attn_types": null, "init_mode": "gpt2", "normalization_type": "LNPre", "device": "cuda", "n_devices": 1, "attention_dir": "causal", "attn_only": false, "seed": null, "initializer_range": 0.07071067811865475, "init_weights": false, "scale_attn_by_inverse_layer_idx": false, "positional_embedding_type": "rotary", "final_rms": false, "d_vocab_out": 50304, "parallel_attn_mlp": true, "rotary_dim": 8, "n_params": 1179648, "use_hook_tokens": false, "gated_mlp": false, "default_prepend_bos": true, "dtype": "torch.float32", "tokenizer_prepends_bos": true, "n_key_value_heads": null, "post_embedding_ln": false, "rotary_base": 10000, "trust_remote_code": false, "rotary_adjacent_pairs": false, "load_in_4bit": false, "num_experts": null, "experts_per_token": null, "relative_attention_max_distance": null, "relative_attention_num_buckets": null, "decoder_start_token_id": null, "tie_word_embeddings": false, "use_normalization_before_and_after": false, "attn_scores_soft_cap": -1.0, "output_logits_soft_cap": -1.0, "use_NTK_by_parts_rope": false, "NTK_by_parts_low_freq_factor": 1.0, "NTK_by_parts_high_freq_factor": 4.0, "NTK_by_parts_factor": 8.0}
{"n_layers": 8, "d_model": 64, "n_ctx": 2048, "d_head": 4, "model_name": "tiny-stories-1M", "n_heads": 16, "d_mlp": 256, "act_fn": "gelu_new", "d_vocab": 50257, "eps": 1e-05, "use_attn_result": false, "use_attn_scale": false, "attn_scale": -1.0, "use_split_qkv_input": false, "use_hook_mlp_in": false, "use_attn_in": false, "use_local_attn": true, "ungroup_grouped_query_attention": false, "original_architecture": "GPTNeoForCausalLM", "from_checkpoint": false, "checkpoint_index": null, "checkpoint_label_type": null, "checkpoint_value": null, "tokenizer_name": "roneneldan/TinyStories-1M", "window_size": 256, "attn_types": ["global", "local", "global", "local", "global", "local", "global", "local"], "init_mode": "gpt2", "normalization_type": "LNPre", "device": "cuda", "n_devices": 1, "attention_dir": "causal", "attn_only": false, "seed": null, "initializer_range": 0.1, "init_weights": false, "scale_attn_by_inverse_layer_idx": false, "positional_embedding_type": "standard", "final_rms": false, "d_vocab_out": 50257, "parallel_attn_mlp": false, "rotary_dim": null, "n_params": 393216, "use_hook_tokens": false, "gated_mlp": false, "default_prepend_bos": true, "dtype": "torch.float32", "tokenizer_prepends_bos": false, "n_key_value_heads": null, "post_embedding_ln": false, "rotary_base": 10000, "trust_remote_code": false, "rotary_adjacent_pairs": false, "load_in_4bit": false, "num_experts": null, "experts_per_token": null, "relative_attention_max_distance": null, "relative_attention_num_buckets": null, "decoder_start_token_id": null, "tie_word_embeddings": false, "use_normalization_before_and_after": false, "attn_scores_soft_cap": -1.0, "output_logits_soft_cap": -1.0, "use_NTK_by_parts_rope": false, "NTK_by_parts_low_freq_factor": 1.0, "NTK_by_parts_high_freq_factor": 4.0, "NTK_by_parts_factor": 8.0}
