import functools
import logging
import tempfile
import time
from pathlib import Path
from typing import Any, Optional, Union

import mlflow
import mlflow.pytorch
import torch

from .onnx_client import OnnxClient

# onnx_client ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
onnx_client = OnnxClient()
logger = logging.getLogger(__name__)


# ============================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ============================================================================


def _convert_to_numpy(
    tensor_data: Union[torch.Tensor, dict[str, torch.Tensor], tuple, list],
) -> Union[Any, dict[str, Any]]:
    """PyTorch í…ì„œë¥¼ NumPy ë°°ì—´ë¡œ ë³€í™˜ (MLflow infer_signature ìš©)."""
    if isinstance(tensor_data, torch.Tensor):
        return tensor_data.detach().cpu().numpy()
    elif isinstance(tensor_data, dict):
        return {key: _convert_to_numpy(value) for key, value in tensor_data.items()}
    elif isinstance(tensor_data, (tuple, list)):
        return [_convert_to_numpy(item) for item in tensor_data]
    else:
        return tensor_data


def _infer_model_schema(
    model: torch.nn.Module, sample_input: Union[torch.Tensor, dict[str, torch.Tensor]]
) -> "mlflow.models.signature.ModelSignature":
    """
    PyTorch ëª¨ë¸ë¡œë¶€í„° ìë™ìœ¼ë¡œ ì…ë ¥/ì¶œë ¥ ìŠ¤í‚¤ë§ˆ ì¶”ì¶œ

    Args:
        model: PyTorch ëª¨ë¸
        sample_input: ìƒ˜í”Œ ì…ë ¥ (ì‹¤ì œ ëª¨ë¸ ì‹¤í–‰ìš©)

    Returns:
        ModelSignature: ìë™ ì¶”ì¶œëœ ìŠ¤í‚¤ë§ˆ

    """
    from mlflow.models.signature import infer_signature

    model.eval()
    device = next(model.parameters()).device

    # ìƒ˜í”Œ ì…ë ¥ì„ ëª¨ë¸ê³¼ ê°™ì€ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
    if isinstance(sample_input, torch.Tensor):
        sample_input = sample_input.to(device)
    elif isinstance(sample_input, dict):
        sample_input = {k: v.to(device) for k, v in sample_input.items()}

    # ì‹¤ì œ ëª¨ë¸ ì‹¤í–‰í•˜ì—¬ ì¶œë ¥ í™•ì¸
    with torch.no_grad():
        if isinstance(sample_input, dict):
            sample_output = model(**sample_input)
        else:
            sample_output = model(sample_input)

    # PyTorch í…ì„œë¥¼ NumPyë¡œ ë³€í™˜
    numpy_input = _convert_to_numpy(sample_input)
    numpy_output = _convert_to_numpy(sample_output)

    # MLflow ìë™ ì¶”ë¡  ì‚¬ìš©
    signature = infer_signature(numpy_input, numpy_output)

    logger.info(f"ìë™ ì¶”ì¶œëœ ìŠ¤í‚¤ë§ˆ: {signature}")
    return signature


def _generate_input_output_names(
    signature: "mlflow.models.signature.ModelSignature",
) -> tuple[list[str], list[str]]:
    """
    MLflow signatureë¡œë¶€í„° input/output ì´ë¦„ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤.

    ë‹¤ì–‘í•œ MLflow ë²„ì „ í˜¸í™˜ì„±ì„ ê³ ë ¤í•©ë‹ˆë‹¤.
    """
    input_names: list[str] = []
    output_names: list[str] = []

    # ì…ë ¥ ì´ë¦„ ìƒì„± - ì—¬ëŸ¬ ë°©ë²• ì‹œë„
    try:
        # ë°©ë²• 2: ìŠ¤í‚¤ë§ˆì—ì„œ ì´ë¦„ ì¶”ì¶œ
        if not input_names and hasattr(signature.inputs, "schema"):
            schema = signature.inputs.schema
            if hasattr(schema, "names") and schema.names:
                input_names = list(schema.names)
            elif hasattr(schema, "input_names") and callable(schema.input_names):
                potential_names = schema.input_names()
                if potential_names:
                    input_names = list(potential_names)

        # ë°©ë²• 3: í…ì„œ ì •ë³´ì—ì„œ ì¶”ì¶œ ì‹œë„
        if not input_names:
            try:
                input_spec = str(signature.inputs)
                if "'" in input_spec:  # 'image': Tensor, 'mask': Tensor í˜•íƒœ
                    import re

                    names = re.findall(r"'([^']+)':", input_spec)
                    if names:
                        input_names = names
            except Exception:
                pass

        # ë°©ë²• 4: ê¸°ë³¸ ì´ë¦„ ìƒì„±
        if not input_names:
            # signature.inputsë¥¼ ë¶„ì„í•˜ì—¬ ê°œìˆ˜ ì¶”ì •
            inputs_str = str(signature.inputs)
            if "Tensor" in inputs_str:
                tensor_count = inputs_str.count("Tensor")
                input_names = [f"input_{i}" for i in range(max(1, tensor_count))]
            else:
                input_names = ["input_0"]

    except Exception as e:
        logger.debug(f"ì…ë ¥ ì´ë¦„ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        input_names = ["input_0"]

    # ì¶œë ¥ ì´ë¦„ ìƒì„± - ìœ ì‚¬í•œ ë°©ë²•ë“¤
    try:
        # MLflow outputsëŠ” ì¼ë°˜ì ìœ¼ë¡œ input_names ë©”ì„œë“œê°€ ì—†ìŒ
        if hasattr(signature.outputs, "schema"):
            schema = signature.outputs.schema
            if hasattr(schema, "names") and schema.names:
                output_names = list(schema.names)

        # ê¸°ë³¸ ì´ë¦„ ìƒì„±
        if not output_names:
            outputs_str = str(signature.outputs)
            if "Tensor" in outputs_str:
                tensor_count = outputs_str.count("Tensor")
                output_names = [f"output_{i}" for i in range(max(1, tensor_count))]
            else:
                output_names = ["output_0"]

    except Exception as e:
        logger.debug(f"ì¶œë ¥ ì´ë¦„ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        output_names = ["output_0"]

    logger.debug(f"ìƒì„±ëœ ì´ë¦„ - ì…ë ¥: {input_names}, ì¶œë ¥: {output_names}")
    return input_names, output_names


def _convert_pytorch_to_onnx_with_client(
    model: torch.nn.Module,
    sample_input: Union[torch.Tensor, dict[str, torch.Tensor]],
    signature: "mlflow.models.signature.ModelSignature",
    onnx_opset_version: int = 17,
    custom_dynamic_axes: Optional[dict[str, dict[int, str]]] = None,  # ìƒˆë¡œ ì¶”ê°€
) -> Optional[str]:
    """
    PyTorch ëª¨ë¸ì„ ONNXë¡œ ë³€í™˜í•˜ê³  onnx_clientë¥¼ í†µí•´ ì—…ë¡œë“œí•©ë‹ˆë‹¤.

    Args:
        model: PyTorch ëª¨ë¸
        sample_input: ìƒ˜í”Œ ì…ë ¥
        signature: MLflow ì‹œê·¸ë‹ˆì²˜
        onnx_opset_version: ONNX opset ë²„ì „
        custom_dynamic_axes: ì‚¬ìš©ì ì •ì˜ dynamic_axes (ì„ íƒì‚¬í•­)

    Returns:
        Optional[str]: ì—…ë¡œë“œëœ ëª¨ë¸ ê²½ë¡œ (í”„ë¡œë•ì…˜ ëª¨ë“œê°€ ì•„ë‹Œ ê²½ìš°)

    """
    try:
        # ìŠ¤í‚¤ë§ˆë¡œë¶€í„° input/output ì´ë¦„ ìë™ ìƒì„±
        input_names, output_names = _generate_input_output_names(signature)

        logger.info(f"ONNX ë³€í™˜ ì‹œì‘ - ì…ë ¥: {input_names}, ì¶œë ¥: {output_names}")

        # ì„ì‹œ ONNX íŒŒì¼ ìƒì„±
        with tempfile.NamedTemporaryFile(suffix=".onnx", delete=False) as tmp_file:
            onnx_path = tmp_file.name

        conversion_start = time.time()

        # ğŸ¯ ê°œì„ ëœ dynamic_axes êµ¬ì„±
        dynamic_axes = {}

        # 1. ê¸°ë³¸ ë°°ì¹˜ ì°¨ì› ì„¤ì • (ëª¨ë“  ì…ë ¥/ì¶œë ¥ì— ì ìš©)
        for input_name in input_names:
            dynamic_axes[input_name] = {0: "batch_size"}

        for output_name in output_names:
            dynamic_axes[output_name] = {0: "batch_size"}

        # 2. ì‚¬ìš©ì ì •ì˜ dynamic_axes ë³‘í•©
        if custom_dynamic_axes:
            for tensor_name, axes_dict in custom_dynamic_axes.items():
                if tensor_name in dynamic_axes:
                    # ê¸°ì¡´ ì¶• ì •ë³´ì™€ ë³‘í•©
                    dynamic_axes[tensor_name].update(axes_dict)
                else:
                    # ìƒˆë¡œìš´ í…ì„œ ì¶”ê°€
                    dynamic_axes[tensor_name] = axes_dict.copy()

        logger.info(f"ìµœì¢… Dynamic axes êµ¬ì„±: {dynamic_axes}")

        # PyTorch â†’ ONNX ë³€í™˜ (í˜¸í™˜ì„± ìš°ì„ )
        # sample_inputì„ ì ì ˆí•œ í˜•íƒœë¡œ ë³€í™˜
        export_args: Any
        if isinstance(sample_input, torch.Tensor):
            export_args = (sample_input,)
        elif isinstance(sample_input, dict):
            # dict í˜•íƒœì˜ ì…ë ¥ì€ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            export_args = sample_input
        else:
            export_args = sample_input

        try:
            # ë™ì  í¬ê¸° ì§€ì› ì‹œë„
            torch.onnx.export(
                model,
                export_args,  # type: ignore[arg-type]
                onnx_path,
                export_params=True,
                opset_version=onnx_opset_version,
                do_constant_folding=True,
                input_names=input_names,
                output_names=output_names,
                dynamic_axes=dynamic_axes,
                verbose=False,
            )
            logger.info("ë™ì  í¬ê¸° ONNX ëª¨ë¸ ë³€í™˜ ì™„ë£Œ")

        except Exception as e:
            # ê³ ì • í¬ê¸°ë¡œ ì¬ì‹œë„ (dynamic_axes ì œê±°)
            logger.warning(
                f"ë™ì  í¬ê¸° ONNX ë³€í™˜ ì‹¤íŒ¨, ê³ ì • í¬ê¸°ë¡œ ì¬ì‹œë„: {str(e)[:100]}..."
            )
            try:
                torch.onnx.export(
                    model,
                    export_args,  # type: ignore[arg-type]
                    onnx_path,
                    export_params=True,
                    opset_version=onnx_opset_version,
                    do_constant_folding=True,
                    input_names=input_names,
                    output_names=output_names,
                    verbose=False,
                )
                logger.info("ê³ ì • í¬ê¸° ONNX ëª¨ë¸ ë³€í™˜ ì™„ë£Œ")
            except Exception as e2:
                # ìµœì†Œí•œì˜ ì„¤ì •ìœ¼ë¡œ ë§ˆì§€ë§‰ ì‹œë„
                logger.warning(
                    f"í‘œì¤€ ONNX ë³€í™˜ë„ ì‹¤íŒ¨, ìµœì†Œ ì„¤ì •ìœ¼ë¡œ ì¬ì‹œë„: {str(e2)[:100]}..."
                )
                torch.onnx.export(
                    model,
                    export_args,  # type: ignore[arg-type]
                    onnx_path,
                    export_params=True,
                    opset_version=onnx_opset_version,
                )
                logger.info("ìµœì†Œ ì„¤ì • ONNX ëª¨ë¸ ë³€í™˜ ì™„ë£Œ")

        conversion_time = time.time() - conversion_start

        onnx_path_obj = Path(onnx_path)
        if not onnx_path_obj.exists():
            raise FileNotFoundError("ONNX íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        file_size_mb = onnx_path_obj.stat().st_size / (1024 * 1024)

        # ONNX ë©”íƒ€ë°ì´í„° ë¡œê¹…
        onnx_metadata = {
            "onnx_conversion_time": conversion_time,
            "onnx_file_size_mb": file_size_mb,
            "onnx_opset_version": onnx_opset_version,
        }
        mlflow.log_metrics(onnx_metadata)
        mlflow.log_params(
            {
                "onnx_input_names": input_names,
                "onnx_output_names": output_names,
            }
        )

        logger.info(f"ONNX ë³€í™˜ ì™„ë£Œ: {onnx_path} ({file_size_mb:.2f}MB)")

        # ğŸ”¥ onnx_clientë¥¼ í†µí•œ ì—…ë¡œë“œ ë° RabbitMQ ë°œí–‰
        try:
            upload_result = onnx_client.upload(onnx_path)
            logger.info("âœ… ONNX ëª¨ë¸ ì—…ë¡œë“œ ë° RabbitMQ ë°œí–‰ ì™„ë£Œ")

            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            onnx_path_obj.unlink()

            return upload_result

        except Exception as e:
            logger.error(f"ONNX í´ë¼ì´ì–¸íŠ¸ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            if onnx_path_obj.exists():
                onnx_path_obj.unlink()
            raise

    except Exception as e:
        logger.error(f"ONNX ë³€í™˜ ì‹¤íŒ¨: {e}")
        mlflow.log_param("onnx_conversion_error", str(e))
        return None


# ============================================================================
# ìƒˆë¡œìš´ ì™„ì „ ìë™í™”ëœ API (MLflow 3.11.1 auto-inference í™œìš©)
# ============================================================================


def trace_pytorch(
    experiment_name: str,
    sample_input: Union[torch.Tensor, dict[str, torch.Tensor]],
    run_name: Optional[str] = None,
    device: str = "cuda" if torch.cuda.is_available() else "cpu",
    onnx_opset_version: int = 17,
    auto_convert_onnx: bool = True,
    log_model_info: bool = True,
    enable_autolog: bool = True,
    dynamic_axes: Optional[dict[str, dict[int, str]]] = None,
):
    """
    ì™„ì „ ìë™í™”ëœ PyTorch ëª¨ë¸ ì¶”ì  (MLflow 3.11.1 auto-inference í™œìš©)

    ì‚¬ìš©ìëŠ” decoratorì— ìƒ˜í”Œ ì…ë ¥ì„ ì œê³µí•˜ê³ , í•¨ìˆ˜ì—ì„œëŠ” ëª¨ë¸ë§Œ ë°˜í™˜í•˜ë©´ ë©ë‹ˆë‹¤.
    ì‹¤ì œ ëª¨ë¸ ì‹¤í–‰ìœ¼ë¡œ ì •í™•í•œ ìŠ¤í‚¤ë§ˆë¥¼ ìë™ ì¶”ì¶œí•©ë‹ˆë‹¤.

    Args:
        experiment_name: MLflow ì‹¤í—˜ ì´ë¦„
        sample_input: ìƒ˜í”Œ ì…ë ¥ (torch.Tensor ë˜ëŠ” Dict[str, torch.Tensor])
        run_name: MLflow ëŸ° ì´ë¦„ (ì„ íƒì‚¬í•­)
        device: ë””ë°”ì´ìŠ¤ ("cuda" ë˜ëŠ” "cpu")
        onnx_opset_version: ONNX opset ë²„ì „
        auto_convert_onnx: PyTorch â†’ ONNX ìë™ ë³€í™˜ ì—¬ë¶€
        log_model_info: ëª¨ë¸ ì •ë³´ ë¡œê¹… ì—¬ë¶€
        enable_autolog: MLflow autolog í™œì„±í™” ì—¬ë¶€

    Returns:
        í•¨ìˆ˜ decorator

    ì‚¬ìš© ì˜ˆì‹œ:
        ```python
        # ê¸°ë³¸ ì‚¬ìš©ë²• (ONNX ë³€í™˜ + MLflow ë¡œê¹…)
        @trace_pytorch("my_experiment", torch.randn(1, 3, 224, 224))
        def train_model():
            model = MyModel()

            # í•™ìŠµ ì¤‘ ë©”íŠ¸ë¦­ ë¡œê¹… (ê¶Œì¥)
            for epoch in range(epochs):
                # í•™ìŠµ ì½”ë“œ...
                train_loss, train_acc = train_one_epoch(model, ...)
                mlflow.log_metrics({
                    "train_loss": train_loss,
                    "train_accuracy": train_acc
                }, step=epoch)

            return model  # ëª¨ë¸ë§Œ ë°˜í™˜!

        # ë‹¤ì¤‘ ì…ë ¥ ëª¨ë¸
        @trace_pytorch("multi_experiment", {
            "image": torch.randn(1, 3, 224, 224),
            "mask": torch.randn(1, 1, 224, 224)
        })
        def train_multi_input_model():
            model = MultiInputModel()
            # í•™ìŠµ ì½”ë“œ...
            return model

        # ONNX ë³€í™˜ ë¹„í™œì„±í™” (MLflowë§Œ ì‚¬ìš©)
        @trace_pytorch"mlflow_only", torch.randn(1, 3, 224, 224), auto_convert_onnx=False)
        def train_model_no_onnx():
            model = MyModel()
            # í•™ìŠµ ì½”ë“œ...
            return model
        ```

    """
    # ë””ë°”ì´ìŠ¤ ê²€ì¦
    if not torch.cuda.is_available() and device == "cuda":
        logger.warning("CUDAê°€ ì‚¬ìš© ë¶ˆê°€í•˜ë¯€ë¡œ CPUë¡œ ë³€ê²½í•©ë‹ˆë‹¤.")
        device = "cpu"

    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            if enable_autolog:
                mlflow.pytorch.autolog()
                logger.info("âœ… MLflow PyTorch autolog í™œì„±í™” ì™„ë£Œ")
            else:
                mlflow.pytorch.autolog(disable=True)
                logger.info("ğŸš« MLflow PyTorch autolog ë¹„í™œì„±í™”")

            # ì‹¤í—˜ ì„¤ì •
            experiment = mlflow.get_experiment_by_name(experiment_name)
            if experiment is None:
                experiment_id = mlflow.create_experiment(experiment_name)
                logger.info(f"ìƒˆ ì‹¤í—˜ ìƒì„±: {experiment_name}")
            else:
                experiment_id = experiment.experiment_id
                logger.info(f"ê¸°ì¡´ ì‹¤í—˜ ì‚¬ìš©: {experiment_name}")

            start_time = time.time()

            with mlflow.start_run(
                experiment_id=experiment_id, run_name=run_name
            ) as run:
                try:
                    logger.info(f"MLflow ì‹¤í–‰ ì‹œì‘ (run_id: {run.info.run_id})")

                    # ì‚¬ìš©ì í•¨ìˆ˜ ì‹¤í–‰
                    result = func(*args, **kwargs)

                    # ë°˜í™˜ê°’ ê²€ì¦ - ëª¨ë¸ë§Œ ë°˜í™˜í•´ì•¼ í•¨!
                    if not isinstance(result, torch.nn.Module):
                        raise ValueError(
                            "í•¨ìˆ˜ëŠ” torch.nn.Moduleë§Œ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤.\n"
                            f"ë°›ì€ íƒ€ì…: {type(result)}\n"
                            "ì˜ˆì‹œ: return model"
                        )

                    model = result

                    # ëª¨ë¸ì„ ì§€ì •ëœ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                    model = model.to(device)
                    logger.info(f"ëª¨ë¸ì´ {device} ë””ë°”ì´ìŠ¤ë¡œ ì´ë™ë˜ì—ˆìŠµë‹ˆë‹¤.")

                    # sample_inputë„ ë™ì¼í•œ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                    if isinstance(sample_input, torch.Tensor):
                        device_sample_input = sample_input.to(device)
                    elif isinstance(sample_input, dict):
                        device_sample_input = {
                            k: v.to(device) for k, v in sample_input.items()
                        }
                    else:
                        raise ValueError(
                            f"ì§€ì›ë˜ì§€ ì•ŠëŠ” sample_input íƒ€ì…: {type(sample_input)}"
                        )

                    # ğŸš€ í•µì‹¬: ì‹¤ì œ ëª¨ë¸ë¡œë¶€í„° ìŠ¤í‚¤ë§ˆ ìë™ ì¶”ì¶œ
                    signature = _infer_model_schema(model, device_sample_input)

                    # ëª¨ë¸ ì •ë³´ ë¡œê¹…
                    if log_model_info:
                        model_info = {
                            "model_class": model.__class__.__name__,
                            "device": str(device),
                            "total_params": sum(p.numel() for p in model.parameters()),
                            "trainable_params": sum(
                                p.numel() for p in model.parameters() if p.requires_grad
                            ),
                        }

                        # ì…ë ¥ ì •ë³´ ìë™ ì¶”ì¶œ
                        if isinstance(device_sample_input, torch.Tensor):
                            model_info["input_shape"] = tuple(device_sample_input.shape)
                            model_info["input_dtype"] = str(device_sample_input.dtype)
                        elif isinstance(device_sample_input, dict):
                            model_info["input_shapes"] = {
                                k: tuple(v.shape)
                                for k, v in device_sample_input.items()
                            }
                            model_info["input_dtypes"] = {
                                k: str(v.dtype) for k, v in device_sample_input.items()
                            }

                        mlflow.log_params(model_info)
                        logger.info(f"ëª¨ë¸ ì •ë³´ ë¡œê¹… ì™„ë£Œ: {model_info['model_class']}")

                    # ğŸ¤ Autologì™€ ìˆ˜ë™ ë¡œê¹…ì˜ ì¡°í™”
                    # autologê°€ í™œì„±í™”ë˜ì–´ ìˆìœ¼ë©´ ì¤‘ë³µ ë¡œê¹… ë°©ì§€
                    if not enable_autolog:
                        # autologê°€ ë¹„í™œì„±í™”ëœ ê²½ìš°ì—ë§Œ ìˆ˜ë™ìœ¼ë¡œ ëª¨ë¸ ë¡œê¹…
                        model_info = mlflow.pytorch.log_model(
                            pytorch_model=model,
                            artifact_path="model",
                            signature=signature,
                            input_example=_convert_to_numpy(device_sample_input),
                        )
                        logger.info("PyTorch ëª¨ë¸ ìˆ˜ë™ ë¡œê¹… ì™„ë£Œ")
                    else:
                        logger.info("PyTorch ëª¨ë¸ì€ autologì— ì˜í•´ ìë™ ë¡œê¹…ë©ë‹ˆë‹¤")

                    # ğŸ”¥ ONNX ë³€í™˜ ë° ì—…ë¡œë“œ (onnx_client í™œìš©)
                    if auto_convert_onnx:
                        upload_result = _convert_pytorch_to_onnx_with_client(
                            model=model,
                            sample_input=device_sample_input,
                            signature=signature,
                            onnx_opset_version=onnx_opset_version,
                            custom_dynamic_axes=dynamic_axes,  # ìƒˆë¡œ ì¶”ê°€
                        )

                        if upload_result:
                            mlflow.log_param("onnx_upload_path", upload_result)
                            mlflow.log_param(
                                "custom_dynamic_axes", str(dynamic_axes)
                            )  # ë¡œê¹… ì¶”ê°€
                            logger.info(
                                f"ğŸš€ ONNX ëª¨ë¸ ì„œë¹„ìŠ¤ ì—…ë¡œë“œ ì™„ë£Œ: {upload_result}"
                            )
                        else:
                            logger.warning("âš ï¸ ONNX ì—…ë¡œë“œ ì‹¤íŒ¨")

                    # ì‹¤í–‰ ì‹œê°„ ë¡œê¹…
                    total_time = time.time() - start_time
                    mlflow.log_metric("total_execution_time", total_time)

                    logger.info(f"ğŸ‰ ëª¨ë¸ ì¶”ì  ì™„ë£Œ (ì‹¤í–‰ì‹œê°„: {total_time:.2f}ì´ˆ)")
                    logger.info(f"ìë™ ì¶”ì¶œëœ ìŠ¤í‚¤ë§ˆ: {signature}")

                    return model

                except Exception as e:
                    logger.error(f"ëª¨ë¸ ì¶”ì  ì‹¤íŒ¨: {e}")
                    mlflow.log_param("execution_error", str(e))
                    raise

        return wrapper

    return decorator


# ============================================================================
# í”„ë ˆì„ì›Œí¬ ë…ë¦½ì ì¸ ONNX ëª¨ë¸ ë¡œê¹… API
# ============================================================================


def log_onnx_model(
    experiment_name: str,
    onnx_model_path: Union[str, Path],
    run_name: Optional[str] = None,
    model_name: Optional[str] = None,
    signature: Optional["mlflow.models.signature.ModelSignature"] = None,
    input_example: Optional[Union[Any, dict[str, Any]]] = None,
    metadata: Optional[dict[str, Any]] = None,
) -> Optional[str]:
    """
    í”„ë ˆì„ì›Œí¬ ë…ë¦½ì ì¸ ONNX ëª¨ë¸ ë¡œê¹… ë° ë°°í¬

    PyTorchê°€ ì•„ë‹Œ ë‹¤ë¥¸ í”„ë ˆì„ì›Œí¬(TensorFlow, JAX, MXNet ë“±)ì—ì„œ
    í•™ìŠµí•œ ëª¨ë¸ì„ ONNXë¡œ ë³€í™˜í•œ í›„, ì´ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ MLflowì—
    ë¡œê¹…í•˜ê³  ì¶”ë¡  ì„œë¹„ìŠ¤ì— ë°°í¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    ì´ í•¨ìˆ˜ëŠ” @trace_pytorch ë°ì½”ë ˆì´í„°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ìƒí™©ì—ì„œ
    ONNX ëª¨ë¸ì„ ì§ì ‘ ì—…ë¡œë“œí•˜ê¸° ìœ„í•œ ëŒ€ì•ˆì…ë‹ˆë‹¤.

    Args:
        experiment_name: MLflow ì‹¤í—˜ ì´ë¦„
        onnx_model_path: ONNX ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
        run_name: MLflow ëŸ° ì´ë¦„ (ì„ íƒì‚¬í•­)
        model_name: ëª¨ë¸ ì´ë¦„ (ì„ íƒì‚¬í•­, ê¸°ë³¸ê°’: íŒŒì¼ëª…)
        signature: MLflow ëª¨ë¸ ì‹œê·¸ë‹ˆì²˜ (ì„ íƒì‚¬í•­)
        input_example: ì…ë ¥ ì˜ˆì‹œ (ì„ íƒì‚¬í•­)
        metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„° (ì„ íƒì‚¬í•­)

    Returns:
        Optional[str]: ì—…ë¡œë“œëœ ëª¨ë¸ ê²½ë¡œ (í”„ë¡œë•ì…˜ ëª¨ë“œê°€ ì•„ë‹Œ ê²½ìš°)

    ì‚¬ìš© ì˜ˆì‹œ:
        ```python
        # TensorFlow ëª¨ë¸ ì‚¬ìš© ì˜ˆ
        import tensorflow as tf
        import tf2onnx

        # TensorFlow ëª¨ë¸ì„ ONNXë¡œ ë³€í™˜
        model = tf.keras.models.load_model('my_model.h5')
        spec = (tf.TensorSpec((None, 224, 224, 3), tf.float32, name="input"),)
        output_path = "model.onnx"

        model_proto, _ = tf2onnx.convert.from_keras(model, input_signature=spec)
        with open(output_path, "wb") as f:
            f.write(model_proto.SerializeToString())

        # ONNX ëª¨ë¸ ë¡œê¹… ë° ì—…ë¡œë“œ
        upload_path = log_onnx_model(
            experiment_name="tensorflow_experiment",
            onnx_model_path=output_path,
            metadata={"framework": "tensorflow", "model_type": "classification"}
        )

        # JAX/Flax ëª¨ë¸ ì‚¬ìš© ì˜ˆ
        # ... JAX ëª¨ë¸ì„ ONNXë¡œ ë³€í™˜ ...
        upload_path = log_onnx_model(
            experiment_name="jax_experiment",
            onnx_model_path="jax_model.onnx",
            metadata={"framework": "jax", "optimizer": "adam"}
        )
        ```

    """
    try:
        # ê²½ë¡œ ê°ì²´ë¡œ ë³€í™˜
        onnx_path = Path(onnx_model_path)
        if not onnx_path.exists():
            raise FileNotFoundError(f"ONNX ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {onnx_path}")

        # ONNX íŒŒì¼ ê²€ì¦
        if onnx_path.suffix.lower() != ".onnx":
            logger.warning(f"íŒŒì¼ í™•ì¥ìê°€ .onnxê°€ ì•„ë‹™ë‹ˆë‹¤: {onnx_path.suffix}")

        # íŒŒì¼ í¬ê¸° ê²€ì¦ (ìµœì†Œ í¬ê¸°)
        file_size = onnx_path.stat().st_size
        if file_size < 1024:  # 1KB ë¯¸ë§Œ
            logger.warning(f"ONNX íŒŒì¼ í¬ê¸°ê°€ ë§¤ìš° ì‘ìŠµë‹ˆë‹¤: {file_size} bytes")

        # ëª¨ë¸ ì´ë¦„ ì„¤ì •
        if model_name is None:
            model_name = onnx_path.stem

        # ì‹¤í—˜ ì„¤ì •
        experiment = mlflow.get_experiment_by_name(experiment_name)
        if experiment is None:
            experiment_id = mlflow.create_experiment(experiment_name)
            logger.info(f"ìƒˆ ì‹¤í—˜ ìƒì„±: {experiment_name}")
        else:
            experiment_id = experiment.experiment_id
            logger.info(f"ê¸°ì¡´ ì‹¤í—˜ ì‚¬ìš©: {experiment_name}")

        start_time = time.time()

        with mlflow.start_run(experiment_id=experiment_id, run_name=run_name) as run:
            logger.info(f"MLflow ì‹¤í–‰ ì‹œì‘ (run_id: {run.info.run_id})")

            # ë©”íƒ€ë°ì´í„° ë¡œê¹…
            if metadata:
                mlflow.log_params(metadata)

            # ê¸°ë³¸ ì •ë³´ ë¡œê¹…
            file_size_mb = onnx_path.stat().st_size / (1024 * 1024)
            mlflow.log_params(
                {
                    "model_name": model_name,
                    "onnx_file_size_mb": file_size_mb,
                    "source_framework": (
                        metadata.get("framework", "unknown") if metadata else "unknown"
                    ),
                }
            )

            # ONNX ëª¨ë¸ì„ MLflowì— ë¡œê¹…
            import onnx
            onnx_model = onnx.load(str(onnx_path))
            mlflow.onnx.log_model(
                onnx_model=onnx_model,
                artifact_path="model",
                signature=signature,
                input_example=input_example,
            )
            logger.info("ONNX ëª¨ë¸ MLflow ë¡œê¹… ì™„ë£Œ")

            # onnx_clientë¥¼ í†µí•œ ì—…ë¡œë“œ
            try:
                upload_result = onnx_client.upload(onnx_path)
                if upload_result:
                    mlflow.log_param("onnx_upload_path", upload_result)
                    logger.info(f"ğŸš€ ONNX ëª¨ë¸ ì„œë¹„ìŠ¤ ì—…ë¡œë“œ ì™„ë£Œ: {upload_result}")
                else:
                    logger.warning("âš ï¸ ONNX ì—…ë¡œë“œ ì‹¤íŒ¨")

            except Exception as e:
                logger.error(f"ONNX í´ë¼ì´ì–¸íŠ¸ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
                mlflow.log_param("upload_error", str(e))
                upload_result = None

            # ì‹¤í–‰ ì‹œê°„ ë¡œê¹…
            total_time = time.time() - start_time
            mlflow.log_metric("total_execution_time", total_time)

            logger.info(f"ğŸ‰ ONNX ëª¨ë¸ ë¡œê¹… ì™„ë£Œ (ì‹¤í–‰ì‹œê°„: {total_time:.2f}ì´ˆ)")

            return upload_result

    except Exception as e:
        logger.error(f"ONNX ëª¨ë¸ ë¡œê¹… ì‹¤íŒ¨: {e}")
        raise
