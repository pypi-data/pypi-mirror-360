Metadata-Version: 2.4
Name: ryzenai_onnx_utils
Version: 1.5.0
Summary: onnx utils
Requires-Python: >=3.9
Requires-Dist: ml-dtypes
Requires-Dist: numpy
Requires-Dist: onnx
Requires-Dist: onnx-tool
Requires-Dist: pyyaml
Requires-Dist: rich
Requires-Dist: sympy
Provides-Extra: dev
Requires-Dist: pre-commit; extra == 'dev'
Requires-Dist: pytest; extra == 'dev'
Provides-Extra: docs
Requires-Dist: sphinx; extra == 'docs'
Requires-Dist: sphinx-argparse; extra == 'docs'
Requires-Dist: sphinx-copybutton; extra == 'docs'
Requires-Dist: sphinx-favicon; extra == 'docs'
Requires-Dist: sphinx-issues; extra == 'docs'
Requires-Dist: sphinx-rtd-theme; extra == 'docs'
Requires-Dist: sphinx-tabs; extra == 'docs'
Requires-Dist: sphinx-tippy; extra == 'docs'
Requires-Dist: sphinx-toolbox; extra == 'docs'
Requires-Dist: sphinxcontrib-jquery; extra == 'docs'
Requires-Dist: sphinxemoji; extra == 'docs'
Description-Content-Type: text/markdown
License-File: LICENSE.txt

<!--
Copyright (c) 2024 Advanced Micro Devices, Inc.
-->


# Introduction

RyzenAI ONNX Utils is a collection of tools to work with ONNX models and deploy them to RyzenAI hardware.
Itâ€™s made up of a command-line Python executable, onnx_utils, and a shared library for custom ONNX ops.
Using the onnx_utils executable, you can process and offline partition any ONNX model for RyzenAI by inserting custom ops representing RyzenAI operations.
For the NPU, this compute is handled using [DynamicDispatch](https://gitenterprise.xilinx.com/VitisAI/dynamicdispatch) and its fusion runtime.

# Features

The core feature set is in the onnx_utils executable and its components:

* *match* - Given an ONNX model, print an onnx_utils compatible pattern from the model or a subset of it
* *preprocess* - Given an ONNX model, fix its dynamic shapes, infer all shapes and run any model-specific optimizations
* *partition* - Run a set of passes on an ONNX model to pattern match on selected nodes and transform them
* *postprocess* - Given an ONNX model, run additional model-specific optimizations after partitioning
* *extract* - Using a specified pattern, extract subgraphs from an ONNX model and save input and output data to create a self-contained test case
* *report* - Print different reports from an ONNX model

The shared library for custom ops allows injecting custom operators into arbitrary ONNXRuntime execution providers.
These custom operators can execute custom code to leverage hardware accelerators like GPU and NPU.

# Documentation

The full documentation for this project is available [online](https://pages.gitenterprise.xilinx.com/varunsh/onnx_utils/index.html).

# Support

The documentation is your best source of support.
You can also raise issues on [Github](https://gitenterprise.xilinx.com/varunsh/onnx_utils/issues) if you run into a bug or have a question.
