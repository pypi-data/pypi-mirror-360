global:
  namespace: "default"
  # If the namespace is not "default" or does not exist, set this configuration to true to enable automatic creation.
  autoCreatedNamespace: false

  # Image prefix. Docker registry URL. Empty means using Docker Hub.
  imageRegistry: ""
  images:
    # Image name and tag in 'name:tag' format
    datasystem: "chariotds:v0.1"

  # Config ETCD table prefix, the value should only contain english alphabetics (a-zA-Z), numbers (0-9) only.
  azName: "AZ1"

  etcd:
    # ETCD configuration
    # Configure ETCD server address. Should not be empty.
    etcdAddress: ""
    # Whether to enable ETCD auth.
    enableEtcdAuth: false
    # Set ETCD target name override for SSL host name checking.
    # The configuration value should be consistent with the DNS content of the Subject Alternate Names of the TLS certificate.
    etcdTargetNameOverride: ""
    # The CA certificate. No encryption required. Base64 encoding is required.
    etcdCa: ""
    # The client cert. No encryption required, base64 transcoding required.
    etcdCert: ""
    # The client private key.
    # Base64 transcoding is required, and whether encryption is required depends on whether passphrase is applied:
    etcdKey: ""
    # The path where the encrypted etcd certificate is mounted. This must be specified when etcd authentication is enabled.
    etcdCertDir: /home/sn/datasystem/etcd_cert_dir
    # The value of passphrase. Encryption is required, base64 transcoding required.
    passphraseValue: ""
    # ETCD metadata async operation pool size.
    # If key size is large under WRITE_BACK_L2_CACHE mode, you may need to increase this value.
    etcdMetaPoolSize: 8

  # Port value (suggested range: 30000-32767).
  port:
    datasystemWorker: 31501

  resources:
    # Resource configuration for datasystem worker container.
    # For cpu: "1" means 1 vCPU core, "500m" means 0.5 core.
    # For memory: should specify unit like Mi/Gi(e.g. "4Gi", not just "4").
    datasystemWorker:
      # Max allowable CPU cores and memory.
      limits:
        cpu: "3"
        memory: "4Gi"
      # Minimum required resources.
      requests:
        cpu: "3"
        memory: "4Gi"

      # Upper limit of the shared memory, the default unit for shared memory is MB.
      sharedMemory: 2048

      # Maximum number of clients that can be connected to a worker.
      # Value range: [1, 10000], default value: 200.
      maxClientNum: 200

  spill:
    # The path of the spilling, empty means local_dick spill disabled.
    # It will create a new subdirectory("datasystem_spill_data") under the SPILL_DIRECTORY to store the spill file.
    # Example: If SPILL_DIRECTORY is "/home/spill", spill files will exist in the "/home/spill/datasystem_spill_data".
    spillDirectory: ""
    # Maximum amount of spilled data that can be stored in the spill directory. If spill is enable and spillSizeLimit is 0, spillSizeLimit will be set to 95% of the spill directory.
    # Unit for spillSizeLimit is Bytes.
    spillSizeLimit: "0"
    # It represents the maximum parallelism of writing files, more threads will consume more CPU and I/O resources.
    spillThreadNum: 8
    # The size limit of single spill file, spilling objects which lager than that value with one object per file.
    # If there are some big objects, you can increase this value to avoid run out of inodes quickly.
    # The valid range is 200-10240.
    spillFileMaxSizeMb: 200
    # The maximum number of open file descriptors about spill. If opened file exceed this value,
    # some files will be temporarily closed to prevent exceeding the maximum system limit. You need reduce this value if your system resources are limited.
    # The valid range is greater than or equal to 8.
    spillFileOpenLimit: 512
    # Disable readahead can mitigate the read amplification problem for offset read, default is true
    spillEnableReadahead: true
    # Thread number of eviction for object cache, suggested value range: (0, 4096].
    evictionThreadNum: 1

  log:
    # Log config.
    # The directory where log files are stored.
    logDir: /home/sn/datasystem/logs
    # Async log buffer, unit is MB.
    logAsyncLogBufferMb: 32
    # Vlog level, a larger value indicates more detailed logs. The value is between 0-3.
    logLevel: 0
    # The maximum log file size (in MB), which must be greater than 0.
    maxLogSize: 400
    # Maximum number of log files to retain per severity level. And every log file size is limited by max log size.
    maxLogFileNum: 25
    # If log-retention-day is greater than 0,
    # any log file from your project whose last modified time is greater than log-retention-day days will be unlinked.
    # If log-retention-day is equal 0, will not unlink log file by time.
    logRetentionDay: 0
    # Flush log files with async mode.
    logAsync: true
    # Compress old log files in .gz format.
    # This parameter takes effect only when the size of the generated log is greater than max log size.
    logCompress: true
    # Prefix of log filename, default is program invocation short name. Use standard characters only.
    logFilename: ""
    # Log messages below this level will not actually be recorded anywhere.
    minLogLevel: 0

  observability:
    # Record performance and resource logs
    logMonitor: true
    # Specify the type of exporter, [harddisk]. Takes effect only when logMonitor is true.
    logMonitorExporter: harddisk
    # The sleep time between iterations of observability collector scan, default value is 10000ms. (Must be greater than 0)
    logMonitorIntervalMs: 10000

  l2Cache:
    # Config the l2 cache type support obs, none by default. Optional value: 'obs', 'sfs', 'none'.
    l2CacheType: "none"

    obs:
      # The access key for obs AK/SK authentication.
      obsAccessKey: ""
      # The secret key for obs AK/SK authentication.
      obsSecretKey: ""
      # OBS endpoint. Example: "xxx.hwcloudtest.cn"
      obsEndpoint: ""
      # OBS bucket name.
      obsBucket: ""
      # Whether to enable the https in obs. false: use HTTP (default), true: use HTTPS
      obsHttpsEnabled: false
      # Use cloud service token rotation to connect obs.
      cloudServiceTokenRotation:
        # Whether to use ccms credential rotation mode to access OBS, default is false. If it is enabled, need to specify
        # iamHostName, identityProvider, projectId, regionId at least.
        # In addition, obsEndpoint and obsBucket need to be specified.
        enable: false
        # Domain name of the IAM token to be obtained. Example: iam.cn-north-7.myhuaweicloud.com.
        iamHostName: ""
        # Provider that provides permissions for the ds-worker. Example: csms-datasystem.
        identityProvider: ""
        # Project id of the OBS to be accessed. Example: fb6a00ff7ae54a5fbb8ff855d0841d00.
        projectId: ""
        # Region id of the OBS to be accessed. Example: cn-north-7.
        regionId: ""
        # Whether to access OBS of other accounts by agency, default is false. If is true, need to specify tokenAgencyName
        # and tokenAgencyDomain.
        enableTokenByAgency: false
        # Agency name for proxy access to other accounts. Example: obs_access.
        tokenAgencyName: ""
        # Agency domain for proxy access to other accounts. Example: op_svc_cff.
        tokenAgencyDomain: ""

    sfsTurbo:
      # Endpoint of sfs-turbo, which is used to concatenate the shared path in sfs-turbo. such as '172.21.7.239'.
      endpoint: ""
      # Sfs-turbo sub-path mounted to ds-worker. If this parameter is not specified, the root directory '/' of sfs-turbo
      # is mounted by default.
      subPath: ""
      # Specifies the sfs-turbo ID, which can be viewed on the sfs-turbo page.
      id: "0"
      # Specifies the sfs-turbo enterprise project ID, which can be viewed on the sfs-turbo page.
      projectId: "0"
      # Specifies the capacity of using sfs-turbo. Note that the size must be smaller than the size of sfsTurbo.
      capacity: "500Gi"

  metadata:
    # Controls whether to enable multiple meta replica
    enableMetaReplica: false
    # Config MASTER back store directory and must specify in rocksdb scenario.
    # The rocksdb database is used to persistently store the metadata stored in the master
    # so that the metadata before the restart can be re-obtained when the master restarts.
    rocksdbStoreDir: /home/sn/datasystem/rocksdb
    # Number of open files that can be used by the rocksdb. The default value is 128, which is suitable for general
    # small and medium-sized applications. This value can be modified according to system limitations.
    rocksdbMaxOpenFile: 128
    # Number of background threads rocksdb can use for flushing and compacting, default value is 16. The value can be
    # modified according to cpu limitations but should be greater than 0 to ensure that there are backend threads to handle tasks.
    rocksdbBackgroundThreads: 16

  rpc:
    # Whether to enable the authentication function between components(worker, master)
    enableCurveZmq: false
    # The directory to find ZMQ curve key files.
    # This path must be specified when zmq authentication is enabled.
    curveKeyDir: /home/sn/datasystem/curve_key_dir
    curveZmqKey:
      # Control whether try to get data from other AZ's worker firstly, if false then get data from L2 cache directly.
      # The service mapping, public key and private key are configured with base64 to avoid the special characters problem when restore to files.
      # These keys must be replaced.
      # The format example: workerPublicKey: V11lTG59ZC5wYX0lTkNrMmlrUFZXJkBTRz5LVHtpMjhBWVhaTlA5Kw==
      # Worker's public key in the curve encryption environment.
      workerPublicKey: ""
      # Worker's private key in the curve encryption environment.
      workerSecretKey: ""               
      # Client's public key in the curve encryption environment.
      clientPublicKey: ""

    # A direct TCP/IP port for worker-to-worker scenarios to improve latency.
    # Acceptable value:0, or some positive integer. 0 means disabled.
    ocWorkerWorkerDirectPort: 0
    # Number of parallel connections from worker to worker scenarios to improve throughput.
    # OC_WORKER_WORKER_DIRECT_PORT must be enabled to take effect.
    ocWorkerWorkerPoolSize: 3

    # Minimum payload size in bytes to trigger direct write into shared memory.
    # May incur extra network cost
    payloadNocopyThreshold: "104857600"
    # Config rpc server thread number, must be greater than 0.
    rpcThreadNum: 128
    # The number of worker service for object cache.
    ocThreadNum: 64
    # Optimize the performance of the customer. Default server is 5.
    # The higher the throughput, the higher the value, but should be in range [1, 32]
    zmqServerIoContext: 5
    # Optimize the performance of the client stub. Default value is 5.
    # The higher the throughput, the higher the value, but should be in range [1, 32]
    zmqClientIoContext: 5
    # Parallel payload split chunk size. Default to 1048756 bytes.
    zmqChunkSz: 1048576
    # Maximum number of sessions that can be cached, must be within [512, 10'000]
    maxRpcSessionNum: 2048

  ipc:
    # Determines whether the shared memory feature is enabled.
    ipcThroughSharedMemory: true
    # Unix domain socket (UDS) file directory with 80-character path limit
    udsDir: /home/uds

  reliability:
    # Client reconnect wait seconds, default value is 5 seconds. (Must be greater than 0)
    clientReconnectWaitS: 5
    # Maximum time interval for the worker to determine client death, value range: [15, UINT64_MAX).
    clientDeadTimeoutS: 120
    # Time interval between worker and etcd heartbeats.
    heartbeatIntervalMs: 1000
    # Maximum time interval before a node is considered lost, the unit is second. (Must be greater than 5s)
    nodeTimeoutS: 60
    # Maximum time interval for the etcd to determine node death, the unit is second. (Must be greater than nodeTimeoutS)
    nodeDeadTimeoutS: 300
    # Control whether to enable reconciliation, default is true.
    enableReconciliation: true
    # Whether to support self-healing when the hash ring is in an abnormal state.
    enableHashRingSelfHealing: false
    # Timeout interval of kubernetes liveness probe, default is 150 seconds.
    livenessProbeTimeoutS: 150
    # Time to wait for the first node that wants to join a working hash ring.
    addNodeWaitTimeS: 60
    # Decide whether to remove the node from hash ring or not when node is dead
    autoDelDeadNode: true

  gracefulShutdown:
    # Scale in taint, format is key=value:effect.
    # Supports configuring multiple types of taints, and multiple taints are only separated by ",".
    scaleInTaint: "datasystem/offline=true:NoExecute"
    # Decide whether to migrate data to other nodes or not when current node exits. If this is the only node in the cluster, exits directly and the data will be lost.
    enableLosslessDataExitMode: false
    # The worker ensures a certain period of time that the asynchronous queues for sending messages to ETCD and
    # L2 cache remain empty before it can exit properly.
    checkAsyncQueueEmptyTimeS: 15
    # Data migration rate limit for every node when scaling down.
    dataMigrateRateLimitMb: 40
    # Overwrite the global default terminationGracePeriodSeconds when liveness probe failed, enable when greater than 0.
    livenessProbeTerminationGracePeriodSeconds: 0

  performance:
    # This is controlled by the flag of mmap(MAP_HUGETLB) which can improve memory access and reducing the overhead of page table, default is disable .
    # Huge pages require system-level configuration. use command "echo [pageNums] > /proc/sys/vm/nr_hugepages" to config the number of huge page.
    # If the system does not have enough physical memory to allocate huge pages, memory allocation will fail.
    enableHugeTlb: false
    #Due to Kubernetes' (k8s) resource calculation policies, shared memory is sometimes counted twice, which can lead to client crashes.
    #To address this issue, fallocate is employed to link the client and worker nodes for shared memory,
    #thus correcting the memory calculation errors. By default, fallocate is enabled.
    #Enabling fallocate will lower the efficiency of memory allocation
    enableFallocate: true
    # Control this process by enabling transparent huge pages, default is disabled.
    # Enabling Transparent Huge Pages (THP) can enhance performance and reduce page table overhead,
    # but it may also lead to increased memory usage, leading to worker being terminated by the OOM Killer.
    enableThp: false

    # The arena count for each tenant. Multiple arenas can improve the performance of share memory allocation for the first time,
    # but each arena will use one more fd, value range: [1, 32]
    arenaPerTenant: 16
    # The memory reclamation time after free, default is 600 seconds.
    memoryReclamationTimeSecond: 600

    # Set whether to delete object asynchronously. If set to true, master will notify workers to delete objects asynchronously.
    # Client doesn't need to wait for all workers to delete objects.
    # The default value is false.
    asyncDelete: false

  crossAz:
    # Specify other az names using the same etcd. Only split by ','
    otherAzNames: ""
    # Control whether to try to get data from other AZ's worker first. If false, data will be retrieved directly from the L2 cache.
    crossAzGetDataFromWorker: true
    # Control whether to get meta data from other AZ's worker, if false then get meta data from local AZ.
    crossAzGetMetaFromWorker: false

  akSk:
    # The access key used by the system.
    systemAccessKey: ""
    # The secret key used by the system.
    systemSecretKey: ""
    # The access key used by the tenant.
    tenantAccessKey: ""
    # The secret key used by the tenant.
    tenantSecretKey: ""
    # Request expiration time in seconds, the maximum value is 300s.
    requestExpireTimeS: 300
  
  # fsGroup configuration
  # All processes of the container are also part of the supplementary group ID.
  fsGid: "1002"

  # To support hybrid deployment.
  # For example:
  # multiSpec:
  # - name: "ds-worker-mid"
  #   affinityLabel: "mid"
  #   resources:
  #     limits:
  #       cpu: "3"
  #       memory: "4Gi"
  #     requests:
  #       cpu: "3"
  #       memory: "4Gi"
  #   workerResources:
  #     sharedMemory: 1024
  # - name: "ds-worker-small"
  #   affinityLabel: "small"
  #   resources:
  #     limits:
  #       cpu: "2"
  #       memory: "3Gi"
  #     requests:
  #       cpu: "1"
  #       memory: "2Gi"
  #   workerResources:
  #     sharedMemory: 1024
  # - name: "ds-worker-big"
  #   affinityLabel: "big"
  #   resources:
  #     limits:
  #       cpu: "5"
  #       memory: "6Gi"
  #     requests:
  #       cpu: "4"
  #       memory: "5Gi"
  #   workerResources:
  #     sharedMemory: 2048
  multiSpec: []

  # Mount the host path into the container.
  # Take effect when both HOST_PATH and MOUNT_PATH are non-empty.
  # Example for mount multiple directories:
  # MOUNT:
  # - HOST_PATH: "/host/ssd-1"
  #   MOUNT_PATH: "/home/spill-1"
  # - HOST_PATH: "/host/ssd-2"
  #   MOUNT_PATH: "/home/spill-2"
  # Example 2:
  # Use ssd to store spill file, you can configure SPILL_DIRECTORY: "/home/ssd/spill", and MOUNT_PATH is as follows:
  # MOUNT:
  # - HOST_PATH: "/host/ssd"
  #   MOUNT_PATH: "/home/ssd"
  # Example 3:
  # Use ssd to store rocksdb file, you can configure ROCKSDB_STORE_DIR: "/home/ssd/rocksdb", and
  # MOUNT_PATH is as follows:
  # MOUNT:
  # - HOST_PATH: "/host/ssd"
  #   MOUNT_PATH: "/home/ssd"
  mount:
    - hostPath: ""
      # The path that you want to mount to your pods.
      mountPath: ""

  # K8s affinity configuration.
  # When configuring the affinity attribute, nodeSelector and nodeAffinity will take effect at the same time.
  # When the two configurations conflict, the pending component may not be pulled up.
  # It is recommended to configure only one.
  # For example:
  # affinity:
  #   nodeSelector:
  #     node-role.kubernetes.io/datasystem: "true"
  #   tolerations:
  #   - key: "testAffinity"
  #     operator: "Equal"
  #     value: "true"
  #     effect: "NoSchedule"
  #   nodeAffinity:
  #     requiredDuringSchedulingIgnoredDuringExecution:
  #       nodeSelectorTerms:
  #         - matchExpressions:
  #             - key: "datasystem"
  #               operator: "In"
  #               values:
  #                 - "true"
  affinity:
    nodeSelector: {}
    tolerations: []
    nodeAffinity: {}

  # K8s meta annotation
  # For example:
  # annotations:
  #   cluster-autoscaler.kubernetes.io/enable-ds-eviction: "true"
  annotations: {}

  # Configure priorityClass.
  # If the value is false, the default priorityClass is system-cluster-critical.
  # If the value is true, a priorityClass with preemptionPolicy Never is created.
  enableNonPreemptive: false