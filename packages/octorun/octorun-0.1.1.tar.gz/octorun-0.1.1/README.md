<div align="center">

# ğŸ™ OctoRun

**Distributed Parallel Execution Made Simple**

*A powerful command-line tool for running Python scripts across multiple GPUs with intelligent task management and monitoring*

[![Version](https://img.shields.io/badge/version-0.1.1-blue.svg)](https://github.com/HarborYuan/OctoRun/releases)
[![Python](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![CUDA](https://img.shields.io/badge/CUDA-supported-green.svg)](https://developer.nvidia.com/cuda-downloads)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Build Status](https://img.shields.io/badge/build-passing-brightgreen.svg)]()

---

</div>

## ğŸ“‹ Overview

**OctoRun** is designed to help you run computationally intensive Python scripts across multiple GPUs efficiently. It automatically manages GPU allocation, chunks your workload, handles failures with retry mechanisms, and provides comprehensive monitoring and logging.

## âœ¨ Key Features

- ğŸ” **Automatic GPU Detection**: Automatically detects and utilizes available GPUs
- ğŸ§© **Intelligent Chunk Management**: Divides work into chunks and distributes across GPUs
- ğŸ”„ **Failure Recovery**: Automatic retry mechanism for failed chunks
- ğŸ“Š **Comprehensive Logging**: Detailed logging for monitoring and debugging
- âš™ï¸ **Flexible Configuration**: JSON-based configuration with CLI overrides
- ğŸ¯ **Kwargs Support**: Pass custom arguments to your scripts via config or CLI
- ğŸ’¾ **Memory Monitoring**: Monitor GPU memory usage and thresholds
- ğŸ”’ **Lock Management**: Prevent duplicate processing of chunks

## ğŸš€ Installation

### Quick Run via uv (Without Installation) 
```bash
uvx octorun [run, save_config, list_gpus]
```

### Via uv (Installation, Globally) 
```bash
uv tool install octorun
```

### Via uv (Install in Your Own Project)
```bash
uv add octorun
```

### Via pip
```bash
pip install octorun
```

## âš¡ Quick Start

<table>
<tr>
<td>

### 1ï¸âƒ£ Create Configuration
```bash
octorun save_config --script ./your_script.py
```

</td>
<td>

### 2ï¸âƒ£ Run Your Script
```bash
octorun run [--config config.json]
```

</td>
</tr>
<tr>
<td>

### 3ï¸âƒ£ Monitor GPU Usage
```bash
octorun list_gpus [--detailed]
```

</td>
<td>

### 4ï¸âƒ£ View Logs
```bash
tail -f logs/session_*.log
```

</td>
</tr>
</table>

## âš™ï¸ Configuration

### ğŸ“„ Basic Configuration

The configuration file (`config.json`) contains the following options:

```json
{
    "script_path": "./your_script.py",
    "gpus": "auto",
    "total_chunks": 128,
    "log_dir": "./logs",
    "chunk_lock_dir": "./logs/locks",
    "monitor_interval": 60,
    "restart_failed": false,
    "max_retries": 3,
    "memory_threshold": 90,
    "kwargs": {
        "batch_size": 32,
        "learning_rate": 0.001
    }
}
```

### ğŸ”§ Configuration Options

| Option | Description | Default |
|--------|-------------|---------|
| `script_path` | Path to your Python script | - |
| `gpus` | GPU configuration ("auto" or list of GPU IDs) | "auto" |
| `total_chunks` | Number of chunks to divide work into | 128 |
| `log_dir` | Directory for log files | "./logs" |
| `chunk_lock_dir` | Directory for chunk lock files | "./logs/locks" |
| `monitor_interval` | Monitoring interval in seconds | 60 |
| `restart_failed` | Whether to restart failed processes | false |
| `max_retries` | Maximum retries for failed chunks | 3 |
| `memory_threshold` | Memory threshold percentage | 90 |
| `kwargs` | Custom arguments to pass to script | {} |

## ğŸ¯ Using Kwargs

OctoRun supports passing additional keyword arguments to your scripts through both the configuration file and command line interface.

### ğŸ“‹ Configuration File

Add kwargs to your `config.json`:

```json
{
    "script_path": "./train_model.py",
    "gpus": "auto",
    "total_chunks": 128,
    "kwargs": {
        "batch_size": 64,
        "learning_rate": 0.01,
        "model_type": "transformer",
        "epochs": 10,
        "output_dir": "./results"
    }
}
```

### ğŸ–¥ï¸ Command Line Interface

Override or add kwargs via command line:

```bash
# Override config kwargs
octorun run --config config.json --kwargs '{"batch_size": 128, "learning_rate": 0.005}'

# Add new kwargs
octorun run --config config.json --kwargs '{"model_type": "bert", "max_length": 512}'
```

### ğŸ¯ Priority

<div align="center">

**CLI kwargs** > **Config file kwargs**

*CLI kwargs override config file kwargs for the same keys while preserving other config kwargs*

</div>

## ğŸ”§ Script Implementation

Your script must accept the required OctoRun arguments plus any custom kwargs:

```python
import argparse

def main():
    parser = argparse.ArgumentParser()
    
    # ğŸ”§ Required OctoRun arguments
    parser.add_argument('--gpu_id', type=int, required=True)
    parser.add_argument('--chunk_id', type=int, required=True)
    parser.add_argument('--total_chunks', type=int, required=True)
    
    # ğŸ¯ Your custom arguments
    parser.add_argument('--batch_size', type=int, default=32)
    parser.add_argument('--learning_rate', type=float, default=0.001)
    parser.add_argument('--model_type', type=str, default='default')
    parser.add_argument('--epochs', type=int, default=1)
    parser.add_argument('--output_dir', type=str, default='./output')
    
    args = parser.parse_args()
    
    # ğŸ® Device handling - Set the GPU device
    # This is an exmple when using PyTorch
    import torch
    if torch.cuda.is_available():
        torch.cuda.set_device(args.gpu_id)
        print(f"ğŸ® Using GPU {args.gpu_id}: {torch.cuda.get_device_name(args.gpu_id)}")
    else:
        print("âš ï¸  CUDA not available, using CPU")
    
    # âœ¨ Use the arguments in your script
    print(f"ğŸš€ Processing chunk {args.chunk_id}/{args.total_chunks} on GPU {args.gpu_id}")
    print(f"ğŸ¯ Training with batch_size={args.batch_size}, lr={args.learning_rate}")
    
    # Your processing logic here
    ...

if __name__ == "__main__":
    main()
```

## ğŸ® Commands

### ğŸš€ `run`

Run your script with the specified configuration:

```bash
octorun run --config config.json [--kwargs '{"key": "value"}']
```

### ğŸ’¾ `save_config`

Generate a default configuration file:

```bash
octorun save_config [--script ./your_script.py]
```

### ğŸ” `list_gpus`

List available GPUs:

```bash
octorun list_gpus [--detailed]
```

## ğŸ“š Examples

### ğŸ¤– Example 1: Machine Learning Training

<details>
<summary>Click to expand</summary>

Config file (`ml_config.json`):
```json
{
    "script_path": "./train_model.py",
    "total_chunks": 64,
    "kwargs": {
        "batch_size": 32,
        "learning_rate": 0.001,
        "model_type": "resnet50",
        "epochs": 100,
        "dataset_path": "/data/imagenet"
    }
}
```

Command:
```bash
octorun run --config ml_config.json --kwargs '{"batch_size": 64, "learning_rate": 0.01}'
```

</details>

### ğŸ“Š Example 2: Data Processing

<details>
<summary>Click to expand</summary>

```bash
octorun run --config config.json --kwargs '{"input_dir": "/data/raw", "output_dir": "/data/processed", "compression": "gzip"}'
```

</details>

## ğŸ“Š Monitoring and Logging

OctoRun provides comprehensive logging:

| Log Type | Location | Description |
|----------|----------|-------------|
| ğŸ“‹ **Session logs** | `logs/session_TIMESTAMP.log` | Overall session information |
| ğŸ§© **Chunk logs** | `logs/chunk_N.log` | Individual chunk processing logs |
| ğŸ”’ **Lock files** | `logs/locks/` | Chunk completion tracking |

### ğŸ“Š Real-time Monitoring

```bash
# Monitor session progress
tail -f logs/session_*.log

# Monitor specific chunk
tail -f logs/chunk_42.log

# Monitor GPU usage
watch -n 1 'octorun list_gpus --detailed'
```

## ğŸ› ï¸ Error Handling

- ğŸ”„ **Automatic retry** mechanism for failed chunks
- ğŸ“Š **Configurable** maximum retry attempts
- ğŸ’¾ **Memory threshold** monitoring
- ğŸ“ **Comprehensive** error logging

<div align="center">

*Robust error handling ensures your jobs complete successfully*

</div>

## ğŸ“‹ Requirements

- ğŸ **Python** â‰¥ 3.10
- ğŸ® **NVIDIA GPUs** with CUDA support
- ğŸ”§ **nvidia-smi** tool available in PATH

## ğŸ¤ Contributing

We welcome contributions! Here's how to get started:

1. ğŸ´ Fork the repository
2. ğŸŒ¿ Create a feature branch
3. âœ¨ Make your changes
4. ğŸ§ª Add tests
5. ğŸ“¤ Submit a pull request

<div align="center">

[![Contributors](https://img.shields.io/badge/contributors-welcome-brightgreen.svg)]()
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)]()

</div>

## ğŸ“„ License

This project is licensed under the **MIT License**.

## ğŸ‘¨â€ğŸ’» Author

**Haobo Yuan** - [haoboyuan@ucmerced.edu](mailto:haoboyuan@ucmerced.edu)

## ğŸ™ Acknowledgements

The project is highly relied on AI tools for code generation and documentation, enhancing productivity and code quality.

---

<div align="center">

**Made with â¤ï¸ and ğŸ¤– AI assistance**

*Star â­ this repo if you find it useful!*

</div>
