#!/usr/bin/env python3
"""
ğŸš€ CausalEngine å¿«é€Ÿæµ‹è¯•è„šæœ¬
========================

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                    CausalEngine å¿«é€Ÿæµ‹è¯•æµç¨‹æ¶æ„å›¾                                                  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                                                                    â•‘
â•‘  ğŸ“Š æ•°æ®å¤„ç†ç®¡é“ (Data Processing Pipeline)                                                                         â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘
â•‘  â”‚                                                                                                                â”‚  â•‘
â•‘  â”‚  1. åˆæˆæ•°æ®ç”Ÿæˆ â†’ 2. æ•°æ®åˆ†å‰² â†’ 3. æ ‡å‡†åŒ–(æ— æ³„éœ²) â†’ 4. å™ªå£°æ³¨å…¥(ä»…è®­ç»ƒé›†) â†’ 5. æ¨¡å‹è®­ç»ƒ                          â”‚  â•‘
â•‘  â”‚                                                                                                                â”‚  â•‘
â•‘  â”‚  Regression:         Train/Test     X & Y         30% Label     8ç§æ–¹æ³•                                        â”‚  â•‘
â•‘  â”‚  4000 samples    â†’   Split      â†’   Scaling   â†’   Noise     â†’   å¿«é€Ÿå¯¹æ¯”                                        â”‚  â•‘
â•‘  â”‚  12 features         (80/20)       (åŸºäºå¹²å‡€æ•°æ®)   (è®­ç»ƒé›†Only)   Performance                                    â”‚  â•‘
â•‘  â”‚                                     ğŸ“Œæ— æ•°æ®æ³„éœ²    ğŸ“Œæµ‹è¯•é›†ä¿æŒçº¯å‡€                                             â”‚  â•‘
â•‘  â”‚                                                                                                                â”‚  â•‘
â•‘  â”‚  Classification:                                                                                               â”‚  â•‘
â•‘  â”‚  4000 samples, 10 features, 3 classes â†’ åŒæ ·æµç¨‹ â†’ 30% Label Noise â†’ 8ç§æ–¹æ³•å¯¹æ¯”                              â”‚  â•‘
â•‘  â”‚                                                                                                                â”‚  â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘
â•‘                                                                                                                    â•‘
â•‘  ğŸ”§ ç»Ÿä¸€å‚æ•°ä¼ é€’æœºåˆ¶ (Unified Parameter Passing)                                                                     â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘
â•‘  â”‚                                                                                                                â”‚  â•‘
â•‘  â”‚  REGRESSION_CONFIG / CLASSIFICATION_CONFIG                                                                    â”‚  â•‘
â•‘  â”‚  â”œâ”€ ğŸ§  ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®                                                                                          â”‚  â•‘
â•‘  â”‚  â”‚   â”œâ”€ perception_hidden_layers = (128, 64, 32)  # æ‰€æœ‰ç¥ç»ç½‘ç»œä½¿ç”¨ç›¸åŒæ¶æ„                                    â”‚  â•‘
â•‘  â”‚  â”‚   â”œâ”€ max_iter = 3000                          # ç»Ÿä¸€æœ€å¤§è®­ç»ƒè½®æ•°                                            â”‚  â•‘
â•‘  â”‚  â”‚   â”œâ”€ learning_rate = 0.01                     # ç»Ÿä¸€å­¦ä¹ ç‡                                                  â”‚  â•‘
â•‘  â”‚  â”‚   â”œâ”€ patience = 50                            # ç»Ÿä¸€æ—©åœpatience                                            â”‚  â•‘
â•‘  â”‚  â”‚   â””â”€ batch_size = None                        # ç»Ÿä¸€æ‰¹å¤„ç†å¤§å°                                              â”‚  â•‘
â•‘  â”‚  â”‚                                                                                                            â”‚  â•‘
â•‘  â”‚  â”œâ”€ ğŸ¯ CausalEngineç‰¹å®šå‚æ•°                                                                                     â”‚  â•‘
â•‘  â”‚  â”‚   â”œâ”€ gamma_init = 1.0, b_noise_init = 1.0     # CausalEngineåˆå§‹åŒ–å‚æ•°                                      â”‚  â•‘
â•‘  â”‚  â”‚   â”œâ”€ modes = ['deterministic', 'exogenous', 'endogenous', 'standard']                                     â”‚  â•‘
â•‘  â”‚  â”‚   â””â”€ alpha = 0.0001                           # L2æ­£åˆ™åŒ–                                                    â”‚  â•‘
â•‘  â”‚  â”‚                                                                                                            â”‚  â•‘
â•‘  â”‚  â””â”€ ğŸ“Š å®éªŒæ§åˆ¶å‚æ•°                                                                                              â”‚  â•‘
â•‘  â”‚      â”œâ”€ anomaly_ratio = 0.3 (å›å½’)              # 30%å¼‚å¸¸/å™ªå£°æ•°æ®                                             â”‚  â•‘
â•‘  â”‚      â”œâ”€ label_noise_ratio = 0.3 (åˆ†ç±»)         # 30%æ ‡ç­¾å™ªå£°                                                  â”‚  â•‘
â•‘  â”‚      â”œâ”€ test_size = 0.2                         # æµ‹è¯•é›†æ¯”ä¾‹                                                   â”‚  â•‘
â•‘  â”‚      â””â”€ random_state = 42                       # éšæœºç§å­                                                    â”‚  â•‘
â•‘  â”‚                                                                                                                â”‚  â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘
â•‘                                                                                                                    â•‘
â•‘  ğŸ”„ 8ç§æ–¹æ³•å¯¹æ¯”æ¶æ„ (8-Method Comparison Framework)                                                                 â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘
â•‘  â”‚                                                                                                                â”‚  â•‘
â•‘  â”‚  ä¼ ç»Ÿç¥ç»ç½‘ç»œ (4ç§)                            å› æœæ¨ç† (4ç§)                                                    â”‚  â•‘
â•‘  â”‚  â”œâ”€ sklearn MLP                              â”œâ”€ CausalEngine (deterministic)                                  â”‚  â•‘
â•‘  â”‚  â”œâ”€ PyTorch MLP                              â”œâ”€ CausalEngine (exogenous)                                      â”‚  â•‘
â•‘  â”‚  â”œâ”€ sklearn OvR MLP (åˆ†ç±»)                   â”œâ”€ CausalEngine (endogenous)                                     â”‚  â•‘
â•‘  â”‚  â””â”€ PyTorch Shared OvR (åˆ†ç±»)                â””â”€ CausalEngine (standard)                                       â”‚  â•‘
â•‘  â”‚                                                                                                                â”‚  â•‘
â•‘  â”‚  ğŸ’¡ å…³é”®è®¾è®¡äº®ç‚¹:                                                                                                â”‚  â•‘
â•‘  â”‚  â€¢ å¿«é€ŸéªŒè¯è®¾è®¡: åˆæˆæ•°æ® + ä¸­ç­‰è§„æ¨¡ï¼Œå¿«é€ŸéªŒè¯CausalEngineåŸºæœ¬æ€§èƒ½                                               â”‚  â•‘
â•‘  â”‚  â€¢ ç§‘å­¦å®éªŒè®¾è®¡: å…ˆåŸºäºå¹²å‡€æ•°æ®æ ‡å‡†åŒ–ï¼Œå†åœ¨è®­ç»ƒé›†æ³¨å…¥30%å™ªå£°ï¼Œæµ‹è¯•é›†ä¿æŒçº¯å‡€                                      â”‚  â•‘
â•‘  â”‚  â€¢ æ— æ•°æ®æ³„éœ²: æ ‡å‡†åŒ–å™¨åªåœ¨å¹²å‡€è®­ç»ƒæ•°æ®ä¸Šfitï¼Œé¿å…å™ªå£°æ±¡æŸ“ç»Ÿè®¡é‡                                                  â”‚  â•‘
â•‘  â”‚  â€¢ ç»Ÿä¸€æ•°æ®æ ‡å‡†åŒ–ç­–ç•¥: æ‰€æœ‰æ–¹æ³•åœ¨æ ‡å‡†åŒ–ç©ºé—´è®­ç»ƒï¼Œç¡®ä¿å®éªŒå…¬å¹³æ€§                                                     â”‚  â•‘
â•‘  â”‚  â€¢ å‚æ•°å…¬å¹³æ€§: æ‰€æœ‰ç¥ç»ç½‘ç»œä½¿ç”¨ç»Ÿä¸€é…ç½®ï¼Œç¡®ä¿å…¬å¹³æ¯”è¾ƒ                                                             â”‚  â•‘
â•‘  â”‚  â€¢ è¯„ä¼°ä¸€è‡´æ€§: æ‰€æœ‰æ–¹æ³•åœ¨åŸå§‹å°ºåº¦ä¸‹è¯„ä¼°ï¼Œä¾¿äºç»“æœè§£é‡Š                                                             â”‚  â•‘
â•‘  â”‚  â€¢ åŒä»»åŠ¡æ”¯æŒ: å›å½’å’Œåˆ†ç±»ä»»åŠ¡å¹¶è¡Œæµ‹è¯•ï¼Œå…¨é¢éªŒè¯CausalEngineèƒ½åŠ›                                                   â”‚  â•‘
â•‘  â”‚                                                                                                                â”‚  â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘
â•‘                                                                                                                    â•‘
â•‘  ğŸ“ˆ è¾“å‡ºåˆ†æ (Analysis & Visualization)                                                                             â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘
â•‘  â”‚                                                                                                                â”‚  â•‘
â•‘  â”‚  1. æ•°æ®åˆ†æå›¾        2. å›å½’æ€§èƒ½å¯¹æ¯”      3. åˆ†ç±»æ€§èƒ½å¯¹æ¯”      4. å¿«é€Ÿæµ‹è¯•æŠ¥å‘Š                                   â”‚  â•‘
â•‘  â”‚     (ç‰¹å¾åˆ†å¸ƒ)           (MAE, RMSE, RÂ²)    (Acc, F1, Precision)  (æ€§èƒ½æ’å)                                    â”‚  â•‘
â•‘  â”‚                                                                                                                â”‚  â•‘
â•‘  â”‚  ğŸ“Š è¯„ä¼°æŒ‡æ ‡:                                                                                                   â”‚  â•‘
â•‘  â”‚  â€¢ å›å½’: MAE, MdAE, RMSE, RÂ² (åŸå§‹å°ºåº¦ä¸‹ç»Ÿä¸€è¯„ä¼°)                                                              â”‚  â•‘
â•‘  â”‚  â€¢ åˆ†ç±»: Accuracy, Precision, Recall, F1 (åŸå§‹æ ‡ç­¾ç©ºé—´è¯„ä¼°)                                                    â”‚  â•‘
â•‘  â”‚                                                                                                                â”‚  â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸš€ ç›®æ ‡ï¼šå¿«é€ŸéªŒè¯ CausalEngine çš„åŸºæœ¬æ€§èƒ½
ğŸ¯ æ ¸å¿ƒï¼šç»Ÿä¸€æ ‡å‡†åŒ–ç­–ç•¥ï¼Œç¡®ä¿å…¬å¹³æ¯”è¾ƒ

ä¸»è¦ç‰¹æ€§ï¼š
- æ”¯æŒå›å½’å’Œåˆ†ç±»ä»»åŠ¡
- ç»Ÿä¸€æ•°æ®é¢„å¤„ç†ï¼šæ‰€æœ‰æ–¹æ³•ä½¿ç”¨ç›¸åŒçš„æ ‡å‡†åŒ–ç­–ç•¥
- å¿«é€Ÿå¯¹æ¯”ï¼šsklearn MLP, PyTorch MLP, CausalEngine (4ç§æ¨¡å¼)
- ç§‘å­¦è¯„ä¼°ï¼šè®­ç»ƒæ—¶æŠ—å™ªå£°ï¼Œè¯„ä¼°æ—¶åœ¨åŸå§‹å°ºåº¦ä¸Šç»Ÿä¸€æ¯”è¾ƒ
- åŒä»»åŠ¡éªŒè¯ï¼šåŒæ—¶æµ‹è¯•å›å½’å’Œåˆ†ç±»æ€§èƒ½

ä½¿ç”¨æ–¹æ³•ï¼š
1. ç›´æ¥è¿è¡Œï¼špython scripts/quick_test_causal_engine.py
2. è°ƒæ•´å‚æ•°ï¼šä¿®æ”¹ä¸‹æ–¹çš„ REGRESSION_CONFIG å’Œ CLASSIFICATION_CONFIG
3. å¿«é€Ÿæµ‹è¯•ï¼šä½¿ç”¨ quick_regression_test() æˆ– quick_classification_test()

æ•°æ®æµç¨‹ï¼š
åˆæˆæ•°æ®ç”Ÿæˆ â†’ è®­ç»ƒ/æµ‹è¯•åˆ†å‰² â†’ æ ‡å‡†åŒ–(æ— æ³„éœ²) â†’ å™ªå£°æ³¨å…¥(ä»…è®­ç»ƒé›†) â†’ æ¨¡å‹è®­ç»ƒ â†’ åŸå§‹å°ºåº¦è¯„ä¼°
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.neural_network import MLPRegressor, MLPClassifier
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, median_absolute_error
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.datasets import make_regression, make_classification
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import os
import sys
import warnings

# è®¾ç½®matplotlibåç«¯ï¼Œé¿å…å¼¹å‡ºçª—å£
plt.switch_backend('Agg')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥æˆ‘ä»¬çš„CausalEngineå®ç°
from causal_sklearn.regressor import MLPCausalRegressor, MLPPytorchRegressor
from causal_sklearn.classifier import MLPCausalClassifier, MLPPytorchClassifier, MLPSklearnOvRClassifier, MLPPytorchSharedOvRClassifier
from causal_sklearn.data_processing import inject_shuffle_noise

warnings.filterwarnings('ignore')

# =============================================================================
# é…ç½®éƒ¨åˆ† - åœ¨è¿™é‡Œä¿®æ”¹å®éªŒå‚æ•°
# =============================================================================

REGRESSION_CONFIG = {
    # æ•°æ®ç”Ÿæˆ
    'n_samples': 4000,  # æ›´å¤§è§„æ¨¡
    'n_features': 12,
    'noise': 1.0,
    'random_state': 42,
    'test_size': 0.2,  # æµ‹è¯•é›†æ¯”ä¾‹
    'anomaly_ratio': 0.3,  # 40%å¼‚å¸¸æ•°æ®ï¼ŒåŒ¹é…å…¶ä»–è„šæœ¬
    
    # ç½‘ç»œç»“æ„
    'perception_hidden_layers': (128, 64, 32),  # ç»Ÿä¸€ç½‘ç»œç»“æ„
    'abduction_hidden_layers': (),
    'repre_size': None,
    'causal_size': None,
    
    # CausalEngineå‚æ•°
    'gamma_init': 1.0,
    'b_noise_init': 1.0,
    'b_noise_trainable': True,
    'alpha': 0.0001, # æ·»åŠ L2æ­£åˆ™åŒ–ï¼Œä¸sklearné»˜è®¤ä¸€è‡´
    
    # è®­ç»ƒå‚æ•°
    'max_iter': 3000,  # ç»Ÿä¸€æœ€å¤§è¿­ä»£æ¬¡æ•°
    'learning_rate': 0.01,  # é™ä½å­¦ä¹ ç‡ï¼Œæ›´æ¥è¿‘sklearné»˜è®¤
    'patience': 50,  # å‡å°‘patienceï¼Œæ›´æ¥è¿‘sklearné»˜è®¤
    'tol': 1e-4,  # æ›´æ¥è¿‘sklearné»˜è®¤tolerance
    'validation_fraction': 0.2,
    'batch_size': None,  # ç»Ÿä¸€ä½¿ç”¨å…¨é‡è®­ç»ƒ(full-batch)
    
    # æµ‹è¯•æ§åˆ¶
    'test_sklearn': True,
    'test_pytorch': True,
    'test_sklearn_ovr': True,
    'test_pytorch_shared_ovr': True,
    'test_causal_deterministic': True,
    'test_causal_exogenous': True,
    'test_causal_endogenous': True,
    'test_causal_standard': True,
    'verbose': True,
    
    # å¯è§†åŒ–æ§åˆ¶
    'save_plots': True,
    'output_dir': 'results/quick_test_results',
    'figure_dpi': 300
}

CLASSIFICATION_CONFIG = {
    # æ•°æ®ç”Ÿæˆ - ä¸sklearnæ›´ç›¸ä¼¼çš„è®¾ç½®
    'n_samples': 4000,  # å‡å°‘æ ·æœ¬é‡ï¼Œæ›´åƒsklearnç»å…¸æµ‹è¯•
    'n_features': 10,   # å‡å°‘ç‰¹å¾æ•°
    'n_classes': 3,
    'class_sep': 1.0,   # æé«˜ç±»åˆ«åˆ†ç¦»åº¦
    'random_state': 42,
    'test_size': 0.2,   # æµ‹è¯•é›†æ¯”ä¾‹
    'label_noise_ratio': 0.3,  # ç»Ÿä¸€æ ‡ç­¾å™ªå£°æ°´å¹³
    
    # ç½‘ç»œç»“æ„ - æ›´ç®€å•çš„ç½‘ç»œ
    'perception_hidden_layers': (128, 64, 32),  # ç»Ÿä¸€ç½‘ç»œç»“æ„
    'abduction_hidden_layers': (),
    'repre_size': None,
    'causal_size': None,
    
    # CausalEngineå‚æ•°
    'gamma_init': 1.0,
    'b_noise_init': 1.0,
    'b_noise_trainable': True,
    'ovr_threshold': 0.0,
    'alpha': 0.0,  # åŒ¹é…sklearné»˜è®¤L2æ­£åˆ™åŒ–
    
    # è®­ç»ƒå‚æ•° - æ›´æ¥è¿‘sklearné»˜è®¤å€¼
    'max_iter': 3000,   # å‡å°‘æœ€å¤§è¿­ä»£æ¬¡æ•°
    'learning_rate': 0.01,  # ä½¿ç”¨sklearné»˜è®¤å­¦ä¹ ç‡
    'patience': 10,     # ä½¿ç”¨sklearné»˜è®¤patience
    'tol': 1e-4,        # åŒ¹é…sklearné»˜è®¤tolerance
    'validation_fraction': 0.2,  # ä½¿ç”¨sklearné»˜è®¤éªŒè¯é›†æ¯”ä¾‹
    'batch_size': None,  # ç»Ÿä¸€ä½¿ç”¨å…¨é‡è®­ç»ƒ(full-batch)
    
    # æµ‹è¯•æ§åˆ¶
    'test_sklearn': True,
    'test_pytorch': True,
    'test_sklearn_ovr': True,
    'test_pytorch_shared_ovr': True,
    'test_causal_deterministic': True,
    'test_causal_exogenous': True,
    'test_causal_endogenous': True,
    'test_causal_standard': True,
    'verbose': True,
    
    # å¯è§†åŒ–æ§åˆ¶
    'save_plots': True,
    'output_dir': 'results/quick_test_results',
    'figure_dpi': 300
}

# =============================================================================
# æ•°æ®ç”Ÿæˆå‡½æ•°
# =============================================================================

def generate_regression_data(config):
    """ç”Ÿæˆå›å½’æµ‹è¯•æ•°æ® - å…¨å±€æ ‡å‡†åŒ–ç‰ˆæœ¬"""
    print(f"ğŸ“Š ç”Ÿæˆå›å½’æ•°æ®: {config['n_samples']}æ ·æœ¬, {config['n_features']}ç‰¹å¾, å™ªå£°={config['noise']}")
    
    # ç”ŸæˆåŸºç¡€æ•°æ®
    X, y = make_regression(
        n_samples=config['n_samples'], 
        n_features=config['n_features'],
        noise=config['noise'], 
        random_state=config['random_state']
    )
    
    # è¿›è¡Œæ•°æ®åˆ†å‰²
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, 
        test_size=config['test_size'], 
        random_state=config['random_state']
    )
    
    print(f"   è®­ç»ƒé›†: {len(X_train)} | æµ‹è¯•é›†: {len(X_test)}")
    
    # ğŸ¯ å…¨å±€æ ‡å‡†åŒ–ç­–ç•¥ (å…ˆæ ‡å‡†åŒ–ï¼Œåæ³¨å…¥å™ªå£°)
    print(f"   ğŸ¯ å®æ–½å…¨å±€æ ‡å‡†åŒ–ç­–ç•¥ (å…ˆæ ‡å‡†åŒ–ï¼Œåæ³¨å…¥å™ªå£°):")
    
    # ç‰¹å¾æ ‡å‡†åŒ–
    scaler_X = StandardScaler()
    X_train_scaled = scaler_X.fit_transform(X_train)
    X_test_scaled = scaler_X.transform(X_test)
    
    # ç›®æ ‡æ ‡å‡†åŒ–ï¼ˆå…³é”®ï¼ï¼‰- åœ¨å¹²å‡€æ•°æ®ä¸Šæ‹Ÿåˆ
    scaler_y = StandardScaler()
    y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()
    y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).flatten()
    print(f"      - X å’Œ y éƒ½å·²åœ¨å¹²å‡€æ•°æ®ä¸Šå®Œæˆæ ‡å‡†åŒ–")

    # å¯¹ *æ ‡å‡†åŒ–å* çš„è®­ç»ƒé›†æ ‡ç­¾è¿›è¡Œå¼‚å¸¸æ³¨å…¥
    if config['anomaly_ratio'] > 0:
        y_train_scaled_noisy, noise_indices = inject_shuffle_noise(
            y_train_scaled, 
            noise_ratio=config['anomaly_ratio'],
            random_state=config['random_state']
        )
        y_train_for_training = y_train_scaled_noisy
        print(f"   å¼‚å¸¸æ³¨å…¥: åœ¨æ ‡å‡†åŒ–ç©ºé—´ä¸­å¯¹ {config['anomaly_ratio']:.1%} çš„æ ‡ç­¾æ³¨å…¥å™ªå£°")
        print(f"             ({len(noise_indices)}/{len(y_train)} æ ·æœ¬å—å½±å“)")
    else:
        y_train_for_training = y_train_scaled
        print(f"   æ— å¼‚å¸¸æ³¨å…¥: çº¯å‡€ç¯å¢ƒ")
    
    print(f"      - æ‰€æœ‰æ¨¡å‹å°†åœ¨å«å™ªæ ‡å‡†åŒ–ç©ºé—´ä¸­ç«äº‰")
    
    data = {
        # åŸå§‹æ•°æ®ï¼ˆç”¨äºæœ€ç»ˆè¯„ä¼°ï¼‰
        'X_train_original': X_train, 'X_test_original': X_test,
        'y_train_original': y_train, 'y_test_original': y_test,
        
        # æ ‡å‡†åŒ–æ•°æ®ï¼ˆç”¨äºæ¨¡å‹è®­ç»ƒï¼‰
        'X_train': X_train_scaled, 'X_test': X_test_scaled,
        'y_train': y_train_for_training,
        'y_test': y_test_scaled,
        
        # æ ‡å‡†åŒ–å™¨ï¼ˆç”¨äºé€†å˜æ¢ï¼‰
        'scaler_X': scaler_X, 'scaler_y': scaler_y
    }
    
    return data

def generate_classification_data(config):
    """ç”Ÿæˆåˆ†ç±»æµ‹è¯•æ•°æ® - å…¨å±€æ ‡å‡†åŒ–ç‰ˆæœ¬"""
    print(f"ğŸ“Š ç”Ÿæˆåˆ†ç±»æ•°æ®: {config['n_samples']}æ ·æœ¬, {config['n_features']}ç‰¹å¾, {config['n_classes']}ç±»åˆ«")
    
    n_informative = min(config['n_features'], max(2, config['n_features'] // 2))
    
    # ç”ŸæˆåŸºç¡€æ•°æ®
    X, y = make_classification(
        n_samples=config['n_samples'], 
        n_features=config['n_features'], 
        n_classes=config['n_classes'],
        n_informative=n_informative, 
        n_redundant=0, 
        n_clusters_per_class=1,
        class_sep=config['class_sep'], 
        random_state=config['random_state']
    )
    
    # è¿›è¡Œæ•°æ®åˆ†å‰²
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, 
        test_size=config['test_size'], 
        random_state=config['random_state'],
        stratify=y
    )
    
    print(f"   è®­ç»ƒé›†: {len(X_train)} | æµ‹è¯•é›†: {len(X_test)}")
    
    # ğŸ¯ æ— æ³„éœ²æ ‡å‡†åŒ–ç­–ç•¥ï¼šå…ˆæ ‡å‡†åŒ–ç‰¹å¾ï¼Œåæ³¨å…¥æ ‡ç­¾å™ªå£°
    print(f"   ğŸ¯ å®æ–½æ— æ³„éœ²æ ‡å‡†åŒ–ç­–ç•¥ï¼ˆå…ˆæ ‡å‡†åŒ–ï¼Œåæ³¨å…¥å™ªå£°ï¼‰:")
    
    # ç‰¹å¾æ ‡å‡†åŒ– - åœ¨å¹²å‡€æ•°æ®ä¸Šfit
    scaler_X = StandardScaler()
    X_train_scaled = scaler_X.fit_transform(X_train)
    X_test_scaled = scaler_X.transform(X_test)
    print(f"      - X å·²åœ¨å¹²å‡€æ•°æ®ä¸Šå®Œæˆæ ‡å‡†åŒ–")
    
    # å¯¹è®­ç»ƒé›†æ ‡ç­¾è¿›è¡Œå¼‚å¸¸æ³¨å…¥ï¼ˆåœ¨æ ‡å‡†åŒ–åï¼‰
    if config['label_noise_ratio'] > 0:
        y_train_noisy, noise_indices = inject_shuffle_noise(
            y_train, 
            noise_ratio=config['label_noise_ratio'],
            random_state=config['random_state']
        )
        y_train_for_training = y_train_noisy
        print(f"   æ ‡ç­¾å™ªå£°: {config['label_noise_ratio']:.1%} ({len(noise_indices)}/{len(y_train)} æ ·æœ¬å—å½±å“)")
    else:
        y_train_for_training = y_train
        print(f"   æ— æ ‡ç­¾å™ªå£°: çº¯å‡€ç¯å¢ƒ")
    
    print(f"      - åˆ†ç±»æ ‡ç­¾ä¸æ ‡å‡†åŒ–ï¼Œæ‰€æœ‰æ¨¡å‹åœ¨æ ‡å‡†åŒ–ç‰¹å¾ç©ºé—´ä¸­ç«äº‰")
    
    data = {
        # åŸå§‹æ•°æ®ï¼ˆç”¨äºå‚è€ƒï¼‰
        'X_train_original': X_train, 'X_test_original': X_test,
        'y_train_original': y_train, 'y_test_original': y_test,
        
        # æ ‡å‡†åŒ–æ•°æ®ï¼ˆç”¨äºæ¨¡å‹è®­ç»ƒï¼‰
        'X_train': X_train_scaled, 'X_test': X_test_scaled,
        'y_train': y_train_for_training, 'y_test': y_test,  # åˆ†ç±»æ ‡ç­¾ä¸æ ‡å‡†åŒ–
        
        # æ ‡å‡†åŒ–å™¨
        'scaler_X': scaler_X
    }
    
    return data

# =============================================================================
# æ¨¡å‹è®­ç»ƒå‡½æ•°
# =============================================================================

def train_sklearn_regressor(data, config):
    """è®­ç»ƒsklearnå›å½’å™¨"""
    print("ğŸ”§ è®­ç»ƒ sklearn MLPRegressor...")
    
    model = MLPRegressor(
        hidden_layer_sizes=config['perception_hidden_layers'],
        max_iter=config['max_iter'],
        learning_rate_init=config['learning_rate'],
        early_stopping=True,
        validation_fraction=config['validation_fraction'],
        n_iter_no_change=config['patience'],
        tol=config['tol'],
        random_state=config['random_state'],
        alpha=config['alpha'],
        batch_size=len(data['X_train']) if config['batch_size'] is None else config['batch_size']
    )
    
    model.fit(data['X_train'], data['y_train'])
    
    if config['verbose']:
        print(f"   è®­ç»ƒå®Œæˆ: {model.n_iter_} epochs")
    
    return model

def train_sklearn_classifier(data, config):
    """è®­ç»ƒsklearnåˆ†ç±»å™¨"""
    print("ğŸ”§ è®­ç»ƒ sklearn MLPClassifier...")
    
    model = MLPClassifier(
        hidden_layer_sizes=config['perception_hidden_layers'],
        max_iter=config['max_iter'],
        learning_rate_init=config['learning_rate'],
        early_stopping=True,
        validation_fraction=config['validation_fraction'],
        n_iter_no_change=config['patience'],
        tol=config['tol'],
        random_state=config['random_state'],
        alpha=config['alpha'],
        batch_size=len(data['X_train']) if config['batch_size'] is None else config['batch_size']
    )
    
    model.fit(data['X_train'], data['y_train'])
    
    if config['verbose']:
        print(f"   è®­ç»ƒå®Œæˆ: {model.n_iter_} epochs")
    
    return model

def train_pytorch_regressor(data, config):
    """è®­ç»ƒPyTorchå›å½’å™¨"""
    print("ğŸ”§ è®­ç»ƒ PyTorch MLPRegressor...")
    
    model = MLPPytorchRegressor(
        hidden_layer_sizes=config['perception_hidden_layers'],
        max_iter=config['max_iter'],
        learning_rate=config['learning_rate'],
        early_stopping=True,
        validation_fraction=config['validation_fraction'],
        n_iter_no_change=config['patience'],
        tol=config['tol'],
        random_state=config['random_state'],
        verbose=config['verbose'],
        alpha=config['alpha'],
        batch_size=len(data['X_train']) if config['batch_size'] is None else config['batch_size']
    )
    
    model.fit(data['X_train'], data['y_train'])
    
    if config['verbose']:
        n_iter = model.n_iter_
        print(f"   è®­ç»ƒå®Œæˆ: {n_iter} epochs")
    
    return model

def train_pytorch_classifier(data, config):
    """è®­ç»ƒPyTorchåˆ†ç±»å™¨"""
    print("ğŸ”§ è®­ç»ƒ PyTorch MLPClassifier...")
    
    model = MLPPytorchClassifier(
        hidden_layer_sizes=config['perception_hidden_layers'],
        max_iter=config['max_iter'],
        learning_rate=config['learning_rate'],
        early_stopping=True,
        validation_fraction=config['validation_fraction'],
        n_iter_no_change=config['patience'],
        tol=config['tol'],
        random_state=config['random_state'],
        verbose=config['verbose'],
        alpha=config['alpha'],
        batch_size=len(data['X_train']) if config['batch_size'] is None else config['batch_size']
    )
    
    model.fit(data['X_train'], data['y_train'])
    
    if config['verbose']:
        n_iter = model.n_iter_
        print(f"   è®­ç»ƒå®Œæˆ: {n_iter} epochs")
    
    return model

def train_causal_regressor(data, config, mode='standard'):
    """è®­ç»ƒå› æœå›å½’å™¨"""
    print(f"ğŸ”§ è®­ç»ƒ CausalRegressor ({mode})...")
    
    model = MLPCausalRegressor(
        repre_size=config['repre_size'],
        causal_size=config['causal_size'],
        perception_hidden_layers=config['perception_hidden_layers'],
        abduction_hidden_layers=config['abduction_hidden_layers'],
        mode=mode,
        gamma_init=config['gamma_init'],
        b_noise_init=config['b_noise_init'],
        b_noise_trainable=config['b_noise_trainable'],
        max_iter=config['max_iter'],
        learning_rate=config['learning_rate'],
        early_stopping=True,
        validation_fraction=config['validation_fraction'],
        n_iter_no_change=config['patience'],
        tol=config['tol'],
        random_state=config['random_state'],
        verbose=config['verbose'],
        alpha=config['alpha'],
        batch_size=len(data['X_train']) if config['batch_size'] is None else config['batch_size']
    )
    
    model.fit(data['X_train'], data['y_train'])
    
    if config['verbose']:
        n_iter = model.n_iter_
        print(f"   è®­ç»ƒå®Œæˆ: {n_iter} epochs")
    
    return model

def train_sklearn_ovr_classifier(data, config):
    """è®­ç»ƒsklearn OvRåˆ†ç±»å™¨"""
    print("ğŸ”§ è®­ç»ƒ sklearn OvR MLPClassifier...")
    
    model = MLPSklearnOvRClassifier(
        hidden_layer_sizes=config['perception_hidden_layers'],
        max_iter=config['max_iter'],
        learning_rate_init=config['learning_rate'],
        early_stopping=True,
        validation_fraction=config['validation_fraction'],
        n_iter_no_change=config['patience'],
        tol=config['tol'],
        random_state=config['random_state'],
        verbose=config['verbose'],
        alpha=config['alpha'],
        batch_size=len(data['X_train']) if config['batch_size'] is None else config['batch_size']
    )
    
    model.fit(data['X_train'], data['y_train'])
    
    if config['verbose']:
        n_iter = model.n_iter_
        print(f"   è®­ç»ƒå®Œæˆ: å¹³å‡ {n_iter} epochs")
    
    return model

def train_pytorch_shared_ovr_classifier(data, config):
    """è®­ç»ƒPyTorchå…±äº«OvRåˆ†ç±»å™¨"""
    print("ğŸ”§ è®­ç»ƒ PyTorch Shared OvR MLPClassifier...")
    
    model = MLPPytorchSharedOvRClassifier(
        hidden_layer_sizes=config['perception_hidden_layers'],
        max_iter=config['max_iter'],
        learning_rate=config['learning_rate'],
        early_stopping=True,
        validation_fraction=config['validation_fraction'],
        n_iter_no_change=config['patience'],
        tol=config['tol'],
        random_state=config['random_state'],
        verbose=config['verbose'],
        alpha=config['alpha'],
        batch_size=len(data['X_train']) if config['batch_size'] is None else config['batch_size']
    )
    
    model.fit(data['X_train'], data['y_train'])
    
    if config['verbose']:
        n_iter = model.n_iter_
        print(f"   è®­ç»ƒå®Œæˆ: {n_iter} epochs")
    
    return model

def train_causal_classifier(data, config, mode='standard'):
    """è®­ç»ƒå› æœåˆ†ç±»å™¨"""
    print(f"ğŸ”§ è®­ç»ƒ CausalClassifier ({mode})...")
    
    model = MLPCausalClassifier(
        repre_size=config['repre_size'],
        causal_size=config['causal_size'],
        perception_hidden_layers=config['perception_hidden_layers'],
        abduction_hidden_layers=config['abduction_hidden_layers'],
        mode=mode,
        gamma_init=config['gamma_init'],
        b_noise_init=config['b_noise_init'],
        b_noise_trainable=config['b_noise_trainable'],
        ovr_threshold=config['ovr_threshold'],
        max_iter=config['max_iter'],
        learning_rate=config['learning_rate'],
        early_stopping=True,
        validation_fraction=config['validation_fraction'],
        n_iter_no_change=config['patience'],
        tol=config['tol'],
        random_state=config['random_state'],
        verbose=config['verbose'],
        alpha=config['alpha'],
        batch_size=len(data['X_train']) if config['batch_size'] is None else config['batch_size']
    )
    
    model.fit(data['X_train'], data['y_train'])
    
    if config['verbose']:
        n_iter = model.n_iter_
        print(f"   è®­ç»ƒå®Œæˆ: {n_iter} epochs")
    
    return model

# =============================================================================
# è¯„ä¼°å‡½æ•°
# =============================================================================

def evaluate_regression(y_true, y_pred):
    """å›å½’è¯„ä¼°æŒ‡æ ‡"""
    return {
        'MAE': mean_absolute_error(y_true, y_pred),
        'MdAE': median_absolute_error(y_true, y_pred),
        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),
        'RÂ²': r2_score(y_true, y_pred)
    }

def evaluate_classification(y_true, y_pred, n_classes):
    """åˆ†ç±»è¯„ä¼°æŒ‡æ ‡"""
    avg_method = 'binary' if n_classes == 2 else 'macro'
    return {
        'Acc': accuracy_score(y_true, y_pred),
        'Precision': precision_score(y_true, y_pred, average=avg_method, zero_division=0),
        'Recall': recall_score(y_true, y_pred, average=avg_method, zero_division=0),
        'F1': f1_score(y_true, y_pred, average=avg_method, zero_division=0)
    }

def predict_and_evaluate_regression(model, data, model_name, config):
    """å›å½’æ¨¡å‹é¢„æµ‹å’Œè¯„ä¼° - ç»Ÿä¸€é€†å˜æ¢ç­–ç•¥"""
    # åœ¨æ ‡å‡†åŒ–ç©ºé—´ä¸­é¢„æµ‹æµ‹è¯•é›†
    test_pred_scaled = model.predict(data['X_test'])
    
    # å°†é¢„æµ‹ç»“æœè½¬æ¢å›åŸå§‹å°ºåº¦è¿›è¡Œè¯„ä¼°
    test_pred_original = data['scaler_y'].inverse_transform(test_pred_scaled.reshape(-1, 1)).flatten()
    
    # éªŒè¯é›†ï¼šä»åŸå§‹å¹²å‡€æ•°æ®é‡æ–°åˆ†å‰²ï¼ˆç¡®ä¿éªŒè¯è¯„ä¼°çš„ç§‘å­¦æ€§ï¼‰
    X_train_orig, X_val_orig, y_train_orig, y_val_orig = train_test_split(
        data['X_train_original'], data['y_train_original'],
        test_size=config['validation_fraction'],
        random_state=config['random_state']
    )
    
    # å¯¹éªŒè¯é›†åº”ç”¨ç›¸åŒçš„æ ‡å‡†åŒ–
    X_val_scaled = data['scaler_X'].transform(X_val_orig)
    val_pred_scaled = model.predict(X_val_scaled)
    
    # å°†éªŒè¯é›†é¢„æµ‹ç»“æœè½¬æ¢å›åŸå§‹å°ºåº¦
    val_pred_original = data['scaler_y'].inverse_transform(val_pred_scaled.reshape(-1, 1)).flatten()
    
    # åœ¨åŸå§‹å°ºåº¦ä¸‹è¯„ä¼°æ€§èƒ½ï¼ˆæµ‹è¯•é›†å’ŒéªŒè¯é›†éƒ½ç”¨å¹²å‡€æ•°æ®è¯„ä¼°ï¼‰
    results = {
        'test': evaluate_regression(data['y_test_original'], test_pred_original),
        'val': evaluate_regression(y_val_orig, val_pred_original)
    }
    
    return results

def predict_and_evaluate_classification(model, data, model_name, config):
    """åˆ†ç±»æ¨¡å‹é¢„æµ‹å’Œè¯„ä¼° - ç»Ÿä¸€è¯„ä¼°ç­–ç•¥"""
    n_classes = len(np.unique(data['y_train_original']))
    
    # åœ¨æ ‡å‡†åŒ–ç‰¹å¾ç©ºé—´ä¸­é¢„æµ‹æµ‹è¯•é›†ï¼ˆåˆ†ç±»æ ‡ç­¾æ— éœ€è½¬æ¢ï¼‰
    test_pred = model.predict(data['X_test'])
    
    # éªŒè¯é›†ï¼šä»åŸå§‹å¹²å‡€æ•°æ®é‡æ–°åˆ†å‰²ï¼ˆç¡®ä¿éªŒè¯è¯„ä¼°çš„ç§‘å­¦æ€§ï¼‰
    X_train_orig, X_val_orig, y_train_orig, y_val_orig = train_test_split(
        data['X_train_original'], data['y_train_original'],
        test_size=config['validation_fraction'],
        random_state=config['random_state'],
        stratify=data['y_train_original']
    )
    
    # å¯¹éªŒè¯é›†åº”ç”¨ç›¸åŒçš„æ ‡å‡†åŒ–
    X_val_scaled = data['scaler_X'].transform(X_val_orig)
    val_pred = model.predict(X_val_scaled)
    
    # åˆ†ç±»ä»»åŠ¡ï¼šåœ¨åŸå§‹æ ‡ç­¾ç©ºé—´è¯„ä¼°ï¼ˆæµ‹è¯•é›†å’ŒéªŒè¯é›†éƒ½ç”¨å¹²å‡€æ•°æ®è¯„ä¼°ï¼‰
    results = {
        'test': evaluate_classification(data['y_test'], test_pred, n_classes),
        'val': evaluate_classification(y_val_orig, val_pred, n_classes)
    }
    
    return results

# =============================================================================
# å¯è§†åŒ–å’Œç»“æœæ˜¾ç¤ºå‡½æ•°
# =============================================================================

def _ensure_output_dir(output_dir):
    """ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨"""
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"ğŸ“ åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}/")

def _get_output_path(output_dir, filename):
    """è·å–è¾“å‡ºæ–‡ä»¶çš„å®Œæ•´è·¯å¾„"""
    return os.path.join(output_dir, filename)

def create_regression_visualization(results, config):
    """åˆ›å»ºå›å½’ä»»åŠ¡å¯è§†åŒ–å›¾è¡¨"""
    if not config.get('save_plots', False):
        return
        
    _ensure_output_dir(config['output_dir'])
    
    print("\nğŸ“Š åˆ›å»ºå›å½’å¯è§†åŒ–å›¾è¡¨")
    print("-" * 30)
    
    # å‡†å¤‡æ•°æ® - ç¡®ä¿æ–¹æ³•é¡ºåº
    method_order = ['sklearn', 'pytorch', 'sklearn_ovr', 'pytorch_shared_ovr', 'deterministic', 'exogenous', 'endogenous', 'standard']
    methods = [method for method in method_order if method in results]
    method_labels = {
        'sklearn': 'sklearn MLP',
        'pytorch': 'PyTorch MLP',
        'sklearn_ovr': 'sklearn OvR MLP',
        'pytorch_shared_ovr': 'PyTorch Shared OvR',
        'deterministic': 'CausalEngine (deterministic)',
        'exogenous': 'CausalEngine (exogenous)',
        'endogenous': 'CausalEngine (endogenous)',
        'standard': 'CausalEngine (standard)'
    }
    
    metrics = ['MAE', 'MdAE', 'RMSE', 'RÂ²']
    
    # åˆ›å»ºå­å›¾ - è°ƒæ•´å°ºå¯¸ä»¥å®¹çº³æ›´å¤šæ–¹æ³•
    fig, axes = plt.subplots(2, 2, figsize=(24, 20))
    fig.suptitle(f'CausalEngine Quick Regression Test Results ({config["anomaly_ratio"]:.0%} Label Noise)', 
                 fontsize=18, fontweight='bold')
    axes = axes.flatten()
    
    # å®šä¹‰é¢œè‰²æ–¹æ¡ˆ - æ‰©å±•æ”¯æŒ8ç§æ–¹æ³•
    colors = {
        'sklearn': '#1f77b4',           # è“è‰²
        'pytorch': '#ff7f0e',           # æ©™è‰²
        'sklearn_ovr': '#2ca02c',       # ç»¿è‰² (æ–°å¢)
        'pytorch_shared_ovr': '#d62728', # çº¢è‰² (æ–°å¢)
        'deterministic': '#9467bd',     # ç´«è‰²
        'exogenous': '#8c564b',         # æ£•è‰²
        'endogenous': '#e377c2',        # ç²‰è‰²
        'standard': '#7f7f7f'           # ç°è‰²
    }
    
    for i, metric in enumerate(metrics):
        values = [results[method]['test'][metric] for method in methods]
        labels = [method_labels[method] for method in methods]
        bar_colors = [colors[method] for method in methods]
        
        bars = axes[i].bar(labels, values, color=bar_colors, alpha=0.8, edgecolor='black', linewidth=1.5)
        axes[i].set_title(f'{metric} (Test Set)', fontweight='bold', fontsize=14)
        axes[i].set_ylabel(metric, fontsize=12)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, value in zip(bars, values):
            height = bar.get_height()
            if metric == 'RÂ²':
                label_text = f'{value:.4f}'
            else:
                label_text = f'{value:.3f}'
            axes[i].text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                       label_text, ha='center', va='bottom', fontweight='bold', fontsize=10)
        
        # é«˜äº®æœ€ä½³ç»“æœ
        if metric == 'RÂ²':
            best_idx = values.index(max(values))
        else:
            best_idx = values.index(min(values))
        bars[best_idx].set_edgecolor('gold')
        bars[best_idx].set_linewidth(3)
        
        axes[i].tick_params(axis='x', rotation=45, labelsize=10)
        axes[i].grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    
    output_path = _get_output_path(config['output_dir'], 'regression_performance.png')
    plt.savefig(output_path, dpi=config['figure_dpi'], bbox_inches='tight')
    print(f"ğŸ“Š å›å½’æ€§èƒ½å›¾è¡¨å·²ä¿å­˜ä¸º {output_path}")
    plt.close()

def create_classification_visualization(results, config, n_classes):
    """åˆ›å»ºåˆ†ç±»ä»»åŠ¡å¯è§†åŒ–å›¾è¡¨"""
    if not config.get('save_plots', False):
        return
        
    _ensure_output_dir(config['output_dir'])
    
    print("\nğŸ“Š åˆ›å»ºåˆ†ç±»å¯è§†åŒ–å›¾è¡¨")
    print("-" * 30)
    
    # å‡†å¤‡æ•°æ® - ç¡®ä¿æ–¹æ³•é¡ºåºï¼ˆæ”¯æŒ8ç§æ–¹æ³•ï¼‰
    method_order = ['sklearn', 'pytorch', 'sklearn_ovr', 'pytorch_shared_ovr', 'deterministic', 'exogenous', 'endogenous', 'standard']
    methods = [method for method in method_order if method in results]
    method_labels = {
        'sklearn': 'sklearn MLP',
        'pytorch': 'PyTorch MLP',
        'sklearn_ovr': 'sklearn OvR MLP',
        'pytorch_shared_ovr': 'PyTorch Shared OvR',
        'deterministic': 'CausalEngine (deterministic)',
        'exogenous': 'CausalEngine (exogenous)',
        'endogenous': 'CausalEngine (endogenous)',
        'standard': 'CausalEngine (standard)'
    }
    
    metrics = ['Acc', 'Precision', 'Recall', 'F1']
    
    # åˆ›å»ºå­å›¾ï¼ˆæ‰©å±•æ”¯æŒ8ç§æ–¹æ³•ï¼‰
    fig, axes = plt.subplots(2, 2, figsize=(24, 20))
    fig.suptitle(f'CausalEngine Quick {n_classes}-Class Classification Results ({config["label_noise_ratio"]:.0%} Label Noise)', 
                 fontsize=18, fontweight='bold')
    axes = axes.flatten()
    
    # å®šä¹‰é¢œè‰²æ–¹æ¡ˆ - æ”¯æŒ8ç§æ–¹æ³•
    colors = {
        'sklearn': '#1f77b4',       # è“è‰²
        'pytorch': '#ff7f0e',       # æ©™è‰²
        'sklearn_ovr': '#17becf',   # é’è‰²
        'pytorch_shared_ovr': '#e377c2',  # ç²‰è‰²
        'deterministic': '#d62728', # çº¢è‰²
        'exogenous': '#2ca02c',     # ç»¿è‰² 
        'endogenous': '#9467bd',    # ç´«è‰²
        'standard': '#8c564b'       # æ£•è‰²
    }
    
    for i, metric in enumerate(metrics):
        values = [results[method]['test'][metric] for method in methods]
        labels = [method_labels[method] for method in methods]
        bar_colors = [colors[method] for method in methods]
        
        bars = axes[i].bar(labels, values, color=bar_colors, alpha=0.8, edgecolor='black', linewidth=1.5)
        axes[i].set_title(f'{metric} (Test Set)', fontweight='bold', fontsize=14)
        axes[i].set_ylabel(metric, fontsize=12)
        axes[i].set_ylim(0, 1.1)  # åˆ†ç±»æŒ‡æ ‡éƒ½åœ¨0-1ä¹‹é—´
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, value in zip(bars, values):
            height = bar.get_height()
            axes[i].text(bar.get_x() + bar.get_width()/2., height + 0.02,
                       f'{value:.4f}', ha='center', va='bottom', fontweight='bold', fontsize=10)
        
        # é«˜äº®æœ€ä½³ç»“æœ
        best_idx = values.index(max(values))
        bars[best_idx].set_edgecolor('gold')
        bars[best_idx].set_linewidth(3)
        
        axes[i].tick_params(axis='x', rotation=45, labelsize=10)
        axes[i].grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    
    output_path = _get_output_path(config['output_dir'], 'classification_performance.png')
    plt.savefig(output_path, dpi=config['figure_dpi'], bbox_inches='tight')
    print(f"ğŸ“Š åˆ†ç±»æ€§èƒ½å›¾è¡¨å·²ä¿å­˜ä¸º {output_path}")
    plt.close()

def create_data_analysis_visualization(data, config, task_type):
    """åˆ›å»ºæ•°æ®åˆ†æå¯è§†åŒ–å›¾è¡¨"""
    if not config.get('save_plots', False):
        return
        
    _ensure_output_dir(config['output_dir'])
    
    print(f"\nğŸ“Š åˆ›å»º{task_type}æ•°æ®åˆ†æå›¾è¡¨")
    print("-" * 30)
    
    # åˆ›å»ºå›¾å½¢
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    if task_type == 'å›å½’':
        fig.suptitle(f'Regression Data Analysis (Features: {data["X_train"].shape[1]}, Samples: {len(data["X_train"]) + len(data["X_test"])})', 
                     fontsize=16, fontweight='bold')
        
        # 1. ç›®æ ‡å˜é‡åˆ†å¸ƒ (åŸå§‹å°ºåº¦)
        y_all_original = np.concatenate([data['y_train_original'], data['y_test_original']])
        axes[0, 0].hist(y_all_original, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
        axes[0, 0].set_title('Target Distribution (Original Scale)')
        axes[0, 0].set_xlabel('Target Value')
        axes[0, 0].set_ylabel('Frequency')
        axes[0, 0].axvline(y_all_original.mean(), color='red', linestyle='--', 
                          label=f'Mean: {y_all_original.mean():.3f}')
        axes[0, 0].legend()
        
        # 2. å™ªå£°æ³¨å…¥å½±å“å¯¹æ¯”ï¼ˆæ ‡å‡†åŒ–å°ºåº¦ï¼‰
        axes[0, 1].hist(data['y_train_original'], bins=30, alpha=0.7, label='Clean Train', color='lightblue')
        
        # å°†æ ‡å‡†åŒ–ç©ºé—´çš„å¸¦å™ªå£°æ ‡ç­¾è½¬æ¢å›åŸå§‹å°ºåº¦è¿›è¡Œå¯è§†åŒ–
        if 'scaler_y' in data:
            y_train_noisy_original = data['scaler_y'].inverse_transform(data['y_train'].reshape(-1, 1)).flatten()
            axes[0, 1].hist(y_train_noisy_original, bins=30, alpha=0.7, label='With Noise', color='lightcoral')
        else:
            # å¦‚æœæ²¡æœ‰scaler_yï¼Œç›´æ¥ä½¿ç”¨å¸¦å™ªå£°çš„æ ‡ç­¾
            axes[0, 1].hist(data['y_train'], bins=30, alpha=0.7, label='With Noise', color='lightcoral')
        
        axes[0, 1].set_title('Impact of Label Noise (Original Scale)')
        axes[0, 1].set_xlabel('Target Value')
        axes[0, 1].set_ylabel('Frequency')
        axes[0, 1].legend()
        
    else:  # åˆ†ç±»
        fig.suptitle(f'Classification Data Analysis (Features: {data["X_train"].shape[1]}, Classes: {len(np.unique(data["y_train"]))})', 
                     fontsize=16, fontweight='bold')
        
        # 1. ç±»åˆ«åˆ†å¸ƒ
        unique_classes, counts = np.unique(data['y_train_original'], return_counts=True)
        axes[0, 0].bar(unique_classes, counts, alpha=0.7, color='skyblue', edgecolor='black')
        axes[0, 0].set_title('Class Distribution (Training Set)')
        axes[0, 0].set_xlabel('Class')
        axes[0, 0].set_ylabel('Count')
        
        # 2. è®­ç»ƒé›†å™ªå£°å½±å“
        clean_counts = np.unique(data['y_train_original'], return_counts=True)[1]
        noisy_counts = np.unique(data['y_train'], return_counts=True)[1]
        
        x = np.arange(len(unique_classes))
        width = 0.35
        axes[0, 1].bar(x - width/2, clean_counts, width, label='Original', alpha=0.7, color='lightblue')
        axes[0, 1].bar(x + width/2, noisy_counts, width, label='With Noise', alpha=0.7, color='lightcoral')
        axes[0, 1].set_title('Impact of Label Noise')
        axes[0, 1].set_xlabel('Class')
        axes[0, 1].set_ylabel('Count')
        axes[0, 1].set_xticks(x)
        axes[0, 1].set_xticklabels(unique_classes)
        axes[0, 1].legend()
    
    # 3. ç‰¹å¾åˆ†å¸ƒ (æ ‡å‡†åŒ–å)
    n_features = min(data['X_train'].shape[1], 5)  # æœ€å¤šæ˜¾ç¤º5ä¸ªç‰¹å¾
    for i in range(n_features):
        axes[1, 0].hist(data['X_train'][:, i], bins=30, alpha=0.5, 
                       label=f'Feature {i+1}', density=True)
    axes[1, 0].set_title(f'Feature Distributions (Standardized, Top {n_features})')
    axes[1, 0].set_xlabel('Standardized Value')
    axes[1, 0].set_ylabel('Density')
    axes[1, 0].legend()
    
    # 4. æ•°æ®é›†è§„æ¨¡åˆ†æ
    train_size = len(data['X_train'])
    test_size = len(data['X_test'])
    
    axes[1, 1].pie([train_size, test_size], labels=['Training', 'Test'], 
                  autopct='%1.1f%%', colors=['lightblue', 'lightcoral'])
    axes[1, 1].set_title(f'Data Split\n(Total: {train_size + test_size} samples)')
    
    plt.tight_layout()
    
    task_name = 'regression' if task_type == 'å›å½’' else 'classification'
    output_path = _get_output_path(config['output_dir'], f'{task_name}_data_analysis.png')
    plt.savefig(output_path, dpi=config['figure_dpi'], bbox_inches='tight')
    print(f"ğŸ“Š {task_type}æ•°æ®åˆ†æå›¾è¡¨å·²ä¿å­˜ä¸º {output_path}")
    plt.close()

# =============================================================================
# ç»“æœæ˜¾ç¤ºå‡½æ•°
# =============================================================================

def print_regression_results(results):
    """æ‰“å°å›å½’ç»“æœ"""
    print("\nğŸ“Š å›å½’ç»“æœå¯¹æ¯”:")
    print("=" * 120)
    print(f"{'æ–¹æ³•':<20} {'éªŒè¯é›†':<50} {'æµ‹è¯•é›†':<50}")
    print(f"{'':20} {'MAE':<10} {'MdAE':<10} {'RMSE':<10} {'RÂ²':<10} {'MAE':<10} {'MdAE':<10} {'RMSE':<10} {'RÂ²':<10}")
    print("-" * 120)
    
    for method, metrics in results.items():
        val_m = metrics['val']
        test_m = metrics['test']
        print(f"{method:<20} {val_m['MAE']:<10.4f} {val_m['MdAE']:<10.4f} {val_m['RMSE']:<10.4f} {val_m['RÂ²']:<10.4f} "
              f"{test_m['MAE']:<10.4f} {test_m['MdAE']:<10.4f} {test_m['RMSE']:<10.4f} {test_m['RÂ²']:<10.4f}")
    
    print("=" * 120)

def print_classification_results(results, n_classes):
    """æ‰“å°åˆ†ç±»ç»“æœ"""
    print(f"\nğŸ“Š {n_classes}åˆ†ç±»ç»“æœå¯¹æ¯”:")
    print("=" * 120)
    print(f"{'æ–¹æ³•':<20} {'éªŒè¯é›†':<50} {'æµ‹è¯•é›†':<50}")
    print(f"{'':20} {'Acc':<10} {'Precision':<10} {'Recall':<10} {'F1':<10} {'Acc':<10} {'Precision':<10} {'Recall':<10} {'F1':<10}")
    print("-" * 120)
    
    for method, metrics in results.items():
        val_m = metrics['val']
        test_m = metrics['test']
        print(f"{method:<20} {val_m['Acc']:<10.4f} {val_m['Precision']:<10.4f} {val_m['Recall']:<10.4f} {val_m['F1']:<10.4f} "
              f"{test_m['Acc']:<10.4f} {test_m['Precision']:<10.4f} {test_m['Recall']:<10.4f} {test_m['F1']:<10.4f}")
    
    print("=" * 120)

# =============================================================================
# ä¸»æµ‹è¯•å‡½æ•°
# =============================================================================

def test_regression(config=None):
    """å›å½’ä»»åŠ¡æµ‹è¯•"""
    if config is None:
        config = REGRESSION_CONFIG
    
    print("\nğŸ”¬ å›å½’ä»»åŠ¡æµ‹è¯•")
    print("=" * 80)
    print_config_summary(config, 'regression')
    
    # 1. ç”Ÿæˆæ•°æ®
    data = generate_regression_data(config)
    
    # 2. æ•°æ®åˆ†æå¯è§†åŒ–
    if config.get('save_plots', False):
        create_data_analysis_visualization(data, config, 'å›å½’')
    
    results = {}
    
    # 2. è®­ç»ƒå„ç§æ¨¡å‹
    if config['test_sklearn']:
        sklearn_model = train_sklearn_regressor(data, config)
        results['sklearn'] = predict_and_evaluate_regression(sklearn_model, data, 'sklearn', config)
    
    if config['test_pytorch']:
        pytorch_model = train_pytorch_regressor(data, config)
        results['pytorch'] = predict_and_evaluate_regression(pytorch_model, data, 'causal', config)
    
    if config['test_causal_deterministic']:
        causal_det = train_causal_regressor(data, config, 'deterministic')
        results['deterministic'] = predict_and_evaluate_regression(causal_det, data, 'causal', config)
    
    if config['test_causal_exogenous']:
        causal_exo = train_causal_regressor(data, config, 'exogenous')
        results['exogenous'] = predict_and_evaluate_regression(causal_exo, data, 'causal', config)
    
    if config['test_causal_endogenous']:
        causal_endo = train_causal_regressor(data, config, 'endogenous')
        results['endogenous'] = predict_and_evaluate_regression(causal_endo, data, 'causal', config)
    
    if config['test_causal_standard']:
        causal_std = train_causal_regressor(data, config, 'standard')
        results['standard'] = predict_and_evaluate_regression(causal_std, data, 'causal', config)
    
    # 3. æ˜¾ç¤ºç»“æœ
    if config['verbose']:
        print_regression_results(results)
    
    # 4. å¯è§†åŒ–ç»“æœ
    if config.get('save_plots', False):
        create_regression_visualization(results, config)
    
    return results

def test_classification(config=None):
    """åˆ†ç±»ä»»åŠ¡æµ‹è¯•"""
    if config is None:
        config = CLASSIFICATION_CONFIG
    
    print("\nğŸ¯ åˆ†ç±»ä»»åŠ¡æµ‹è¯•")
    print("=" * 80)
    print_config_summary(config, 'classification')
    
    # 1. ç”Ÿæˆæ•°æ®
    data = generate_classification_data(config)
    
    # 2. æ•°æ®åˆ†æå¯è§†åŒ–
    if config.get('save_plots', False):
        create_data_analysis_visualization(data, config, 'åˆ†ç±»')
    
    results = {}
    
    # 2. è®­ç»ƒå„ç§æ¨¡å‹
    if config['test_sklearn']:
        sklearn_model = train_sklearn_classifier(data, config)
        results['sklearn'] = predict_and_evaluate_classification(sklearn_model, data, 'sklearn', config)
    
    if config['test_pytorch']:
        pytorch_model = train_pytorch_classifier(data, config)
        results['pytorch'] = predict_and_evaluate_classification(pytorch_model, data, 'pytorch', config)
    
    if config['test_sklearn_ovr']:
        sklearn_ovr_model = train_sklearn_ovr_classifier(data, config)
        results['sklearn_ovr'] = predict_and_evaluate_classification(sklearn_ovr_model, data, 'sklearn_ovr', config)
    
    if config['test_pytorch_shared_ovr']:
        pytorch_shared_ovr_model = train_pytorch_shared_ovr_classifier(data, config)
        results['pytorch_shared_ovr'] = predict_and_evaluate_classification(pytorch_shared_ovr_model, data, 'pytorch_shared_ovr', config)
    
    if config['test_causal_deterministic']:
        causal_det = train_causal_classifier(data, config, 'deterministic')
        results['deterministic'] = predict_and_evaluate_classification(causal_det, data, 'causal', config)
    
    if config['test_causal_exogenous']:
        causal_exo = train_causal_classifier(data, config, 'exogenous')
        results['exogenous'] = predict_and_evaluate_classification(causal_exo, data, 'causal', config)
    
    if config['test_causal_endogenous']:
        causal_endo = train_causal_classifier(data, config, 'endogenous')
        results['endogenous'] = predict_and_evaluate_classification(causal_endo, data, 'causal', config)
    
    if config['test_causal_standard']:
        causal_std = train_causal_classifier(data, config, 'standard')
        results['standard'] = predict_and_evaluate_classification(causal_std, data, 'causal', config)
    
    # 3. æ˜¾ç¤ºç»“æœ
    if config['verbose']:
        n_classes = len(np.unique(data['y_train']))
        print_classification_results(results, n_classes)
    
    # 4. å¯è§†åŒ–ç»“æœ
    if config.get('save_plots', False):
        n_classes = len(np.unique(data['y_train']))
        create_classification_visualization(results, config, n_classes)
    
    return results

def print_config_summary(config, task_type):
    """æ‰“å°é…ç½®æ‘˜è¦"""
    if task_type == 'regression':
        print(f"æ•°æ®: {config['n_samples']}æ ·æœ¬, {config['n_features']}ç‰¹å¾, å™ªå£°={config['noise']}")
        print(f"å¼‚å¸¸: {config['anomaly_ratio']:.1%} å¼‚å¸¸æ•°æ®æ³¨å…¥")
    else:
        print(f"æ•°æ®: {config['n_samples']}æ ·æœ¬, {config['n_features']}ç‰¹å¾, {config['n_classes']}ç±»åˆ«")
        print(f"å™ªå£°: {config['label_noise_ratio']:.1%} æ ‡ç­¾å™ªå£°, åˆ†ç¦»åº¦={config['class_sep']}")
    
    print(f"ç½‘ç»œ: {config['perception_hidden_layers']}")
    print(f"è®­ç»ƒ: {config['max_iter']} epochs, lr={config['learning_rate']}, patience={config['patience']}")
    print(f"æµ‹è¯•: sklearn={config['test_sklearn']}, pytorch={config['test_pytorch']}, "
          f"deterministic={config['test_causal_deterministic']}, exogenous={config['test_causal_exogenous']}, "
          f"endogenous={config['test_causal_endogenous']}, standard={config['test_causal_standard']}")
    print()

# =============================================================================
# ä¸»ç¨‹åº
# =============================================================================

def main():
    """ä¸»ç¨‹åº - è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("ğŸš€ CausalEngine å¿«é€Ÿæµ‹è¯•è„šæœ¬ - å…¨å±€æ ‡å‡†åŒ–ç‰ˆ")
    print("=" * 60)
    
    # è¿è¡Œå›å½’æµ‹è¯•
    regression_results = test_regression()
    
    # è¿è¡Œåˆ†ç±»æµ‹è¯•  
    classification_results = test_classification()
    
    print(f"\nâœ… æµ‹è¯•å®Œæˆ!")
    
    # æ˜¾ç¤ºç”Ÿæˆçš„æ–‡ä»¶ä¿¡æ¯
    if REGRESSION_CONFIG.get('save_plots', False):
        print(f"\nğŸ“Š ç”Ÿæˆçš„å¯è§†åŒ–æ–‡ä»¶:")
        print(f"   - {REGRESSION_CONFIG['output_dir']}/regression_data_analysis.png")
        print(f"   - {REGRESSION_CONFIG['output_dir']}/regression_performance.png")
        print(f"   - {REGRESSION_CONFIG['output_dir']}/classification_data_analysis.png")
        print(f"   - {REGRESSION_CONFIG['output_dir']}/classification_performance.png")
    
    print("ğŸ’¡ ä¿®æ”¹è„šæœ¬é¡¶éƒ¨çš„ CONFIG éƒ¨åˆ†æ¥è°ƒæ•´å®éªŒå‚æ•°")

def quick_regression_test():
    """å¿«é€Ÿå›å½’æµ‹è¯• - ç”¨äºè°ƒè¯•"""
    quick_config = REGRESSION_CONFIG.copy()
    quick_config.update({
        'n_samples': 1000,
        'max_iter': 500,
        'test_pytorch': False,  # è·³è¿‡pytorchåŸºçº¿ä»¥èŠ‚çœæ—¶é—´
        'test_causal_exogenous': False,  # è·³è¿‡éƒ¨åˆ†æ¨¡å¼ä»¥èŠ‚çœæ—¶é—´
        'test_causal_endogenous': False,
        'verbose': True
    })
    return test_regression(quick_config)

def quick_classification_test():
    """å¿«é€Ÿåˆ†ç±»æµ‹è¯• - ç”¨äºè°ƒè¯•"""
    quick_config = CLASSIFICATION_CONFIG.copy()
    quick_config.update({
        'n_samples': 1500,
        'max_iter': 500,
        'test_pytorch': False,  # è·³è¿‡pytorchåŸºçº¿ä»¥èŠ‚çœæ—¶é—´
        'test_causal_exogenous': False,  # è·³è¿‡éƒ¨åˆ†æ¨¡å¼ä»¥èŠ‚çœæ—¶é—´
        'test_causal_endogenous': False,
        'verbose': True
    })
    return test_classification(quick_config)

if __name__ == "__main__":
    # ä½ å¯ä»¥é€‰æ‹©è¿è¡Œä»¥ä¸‹ä»»ä¸€å‡½æ•°:
    main()                        # å®Œæ•´æµ‹è¯•
    # quick_regression_test()     # å¿«é€Ÿå›å½’æµ‹è¯•
    # quick_classification_test() # å¿«é€Ÿåˆ†ç±»æµ‹è¯•