#!/usr/bin/env python3
"""
PyTorch vs Sklearn MLP å¯¹æ¯”è„šæœ¬ - 2024é‡æ„ç‰ˆ

æœ¬è„šæœ¬å®ç°äº†ä¸‰ä¸ªç‰ˆæœ¬çš„ MLP:
1. ä»é›¶å¼€å§‹æ‰‹åŠ¨å®ç°çš„ PyTorch ç‰ˆæœ¬
2. Sklearn çš„ MLPRegressor
3. Causal-Sklearn åº“ä¸­å°è£…çš„ MLPPytorchRegressor

ğŸ¯ ç›®æ ‡ï¼š
    - åœ¨å«æœ‰æ ‡ç­¾å™ªå£°çš„çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šï¼Œå…¬å¹³åœ°æ¯”è¾ƒä¸‰ç§MLPå®ç°çš„æ€§èƒ½ã€‚
    - é‡‡ç”¨ç§‘å­¦çš„æ•°æ®å¤„ç†æµç¨‹ï¼ˆå…ˆæ ‡å‡†åŒ–ï¼ŒååŠ å™ªï¼‰ï¼Œé¿å…æ•°æ®æ³„éœ²ã€‚
    - å¤šæ¬¡è¿è¡Œå®éªŒä»¥åˆ†æä¸åŒå®ç°çš„æ€§èƒ½ç¨³å®šæ€§å’Œå›ºæœ‰æ–¹å·®ã€‚

âœ¨ å€Ÿé‰´ comprehensive_causal_modes_tutorial_sklearn_style.py çš„ä¼˜ç‚¹ï¼š
    1. å¼•å…¥é…ç½®ç±»ï¼ˆComparisonConfigï¼‰ï¼Œé›†ä¸­ç®¡ç†æ‰€æœ‰å‚æ•°ã€‚
    2. é‡‡ç”¨é¢å‘å¯¹è±¡çš„ç»“æ„ï¼ˆMLPComparisonTutorialï¼‰ï¼Œå°è£…æ•´ä¸ªå®éªŒæµç¨‹ã€‚
    3. çº æ­£æ•°æ®å¤„ç†é€»è¾‘ï¼šå…ˆåœ¨å¹²å‡€æ•°æ®ä¸Šæ ‡å‡†åŒ–ï¼Œå†å¯¹æ ‡å‡†åŒ–åçš„è®­ç»ƒæ ‡ç­¾æ³¨å…¥å™ªå£°ã€‚
    4. å¢å¼ºäº†æ—¥å¿—è¾“å‡ºï¼Œä½¿å®éªŒæµç¨‹æ›´æ¸…æ™°ã€‚
    5. ç»Ÿä¸€çš„è¾“å‡ºç›®å½•ç®¡ç†ã€‚
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.neural_network import MLPRegressor
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, median_absolute_error
import matplotlib.pyplot as plt
import time
import warnings
import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥causal_sklearn
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from causal_sklearn.regressor import MLPPytorchRegressor
from causal_sklearn.data_processing import inject_shuffle_noise

warnings.filterwarnings('ignore')
plt.switch_backend('Agg') # è®¾ç½®matplotlibåç«¯ï¼Œé¿å…å¼¹å‡ºçª—å£


class ComparisonConfig:
    """
    MLPå¯¹æ¯”å®éªŒé…ç½®ç±»
    ğŸ”§ åœ¨è¿™é‡Œä¿®æ”¹å‚æ•°æ¥è‡ªå®šä¹‰å®éªŒè®¾ç½®ï¼
    """
    # å®éªŒæ§åˆ¶
    N_RUNS = 5              # å®éªŒè¿è¡Œæ¬¡æ•°
    BASE_SEED = 42          # åŸºç¡€éšæœºç§å­
    
    # æ•°æ®é›†ä¸é¢„å¤„ç†
    TEST_SIZE = 0.2         # æµ‹è¯•é›†æ¯”ä¾‹
    ANOMALY_RATIO = 0.25    # è®­ç»ƒé›†æ ‡ç­¾å™ªå£°æ¯”ä¾‹
    
    # ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½® - æ‰€æœ‰æ¨¡å‹ä½¿ç”¨ç›¸åŒå‚æ•°ä»¥ç¡®ä¿å…¬å¹³
    MLP_HIDDEN_SIZES = (100, 50)         # éšè—å±‚ç»“æ„
    MLP_MAX_ITER = 1000                  # æœ€å¤§è¿­ä»£æ¬¡æ•°
    MLP_LEARNING_RATE = 0.001            # å­¦ä¹ ç‡
    MLP_ALPHA = 0.0001                   # L2æ­£åˆ™åŒ–ç³»æ•°
    
    # ç»Ÿä¸€æ—©åœé…ç½®
    EARLY_STOPPING_ENABLED = True
    VALIDATION_FRACTION = 0.1
    EARLY_STOPPING_PATIENCE = 20
    EARLY_STOPPING_TOL = 1e-4
    
    # è¾“å‡ºå’Œå¯è§†åŒ–
    OUTPUT_DIR = "results/mlp_comparison"
    FIGURE_DPI = 300
    FIGURE_SIZE = (15, 18)


class PyTorchMLP(nn.Module):
    """æ‰‹åŠ¨å®ç°çš„ PyTorch MLP"""
    
    def __init__(self, input_size, hidden_sizes, output_size=1, random_state=None):
        super(PyTorchMLP, self).__init__()
        if random_state is not None:
            torch.manual_seed(random_state)
        
        layers = []
        prev_size = input_size
        for hidden_size in hidden_sizes:
            layers.append(nn.Linear(prev_size, hidden_size))
            layers.append(nn.ReLU())
            prev_size = hidden_size
        layers.append(nn.Linear(prev_size, output_size))
        
        self.network = nn.Sequential(*layers)
        self.n_iter_ = 0
        
    def fit(self, X, y, epochs, lr, alpha, early_stopping, validation_fraction, n_iter_no_change, tol, random_state):
        if random_state is not None:
            torch.manual_seed(random_state)
            np.random.seed(random_state)
        
        X_tensor = torch.FloatTensor(X)
        y_tensor = torch.FloatTensor(y).reshape(-1, 1)
        
        optimizer = optim.Adam(self.parameters(), lr=lr, weight_decay=alpha)
        criterion = nn.MSELoss()
        
        if early_stopping:
            X_train, X_val, y_train, y_val = train_test_split(
                X_tensor, y_tensor, test_size=validation_fraction, random_state=random_state
            )
        else:
            X_train, y_train = X_tensor, y_tensor
            X_val, y_val = None, None
            
        self.train()
        best_val_loss = float('inf')
        no_improve_count = 0
        best_state_dict = None

        for epoch in range(epochs):
            outputs = self.network(X_train)
            loss = criterion(outputs, y_train)
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            if early_stopping and X_val is not None:
                self.eval()
                with torch.no_grad():
                    val_outputs = self.network(X_val)
                    val_loss = criterion(val_outputs, y_val).item()
                    
                    if val_loss < best_val_loss - tol:
                        best_val_loss = val_loss
                        no_improve_count = 0
                        best_state_dict = self.state_dict().copy()
                    else:
                        no_improve_count += 1
                self.train()
                        
                if no_improve_count >= n_iter_no_change:
                    self.n_iter_ = epoch + 1
                    break
        
        if self.n_iter_ == 0:
            self.n_iter_ = epoch + 1

        if early_stopping and best_state_dict is not None:
            self.load_state_dict(best_state_dict)
    
    def predict(self, X):
        self.eval()
        with torch.no_grad():
            X_tensor = torch.FloatTensor(X)
            predictions = self.network(X_tensor).squeeze().cpu().numpy()
        return predictions


class MLPComparisonTutorial:
    """å°è£… MLP å¯¹æ¯”å®éªŒæµç¨‹çš„ç±»"""

    def __init__(self, config):
        self.config = config
        self.X_train_orig, self.X_test_orig, self.y_train_orig, self.y_test_orig = None, None, None, None
        
        self.results = {
            'pytorch': [],
            'sklearn': [],
            'causal_sklearn': []
        }
        self._ensure_output_dir()

    def _ensure_output_dir(self):
        if not os.path.exists(self.config.OUTPUT_DIR):
            os.makedirs(self.config.OUTPUT_DIR)
            print(f"ğŸ“ åˆ›å»ºè¾“å‡ºç›®å½•: {self.config.OUTPUT_DIR}")

    def _get_output_path(self, filename):
        return os.path.join(self.config.OUTPUT_DIR, filename)

    def load_and_split_data(self):
        print("\nğŸ“Š æ­¥éª¤ 1: åŠ è½½ California Housing æ•°æ®é›†å¹¶åˆ†å‰²")
        housing = fetch_california_housing()
        X, y = housing.data, housing.target
        print(f"   - æ•°æ®é›†åŠ è½½å®Œæˆ: {X.shape[0]} æ ·æœ¬, {X.shape[1]} ç‰¹å¾")
        
        self.X_train_orig, self.X_test_orig, self.y_train_orig, self.y_test_orig = train_test_split(
            X, y, test_size=self.config.TEST_SIZE, random_state=self.config.BASE_SEED
        )
        print(f"   - æ•°æ®åˆ†å‰²å®Œæˆ: è®­ç»ƒé›† {self.X_train_orig.shape[0]} | æµ‹è¯•é›† {self.X_test_orig.shape[0]}")

    def _prepare_data_for_run(self):
        """
        å‡†å¤‡å•æ¬¡è¿è¡Œæ‰€éœ€çš„æ•°æ®ã€‚
        æ­¤å‡½æ•°å®ç°äº†ç§‘å­¦çš„æ•°æ®å¤„ç†æµç¨‹ï¼šå…ˆæ ‡å‡†åŒ–ï¼Œåæ³¨å…¥å™ªå£°ã€‚
        """
        # 1. ç‰¹å¾æ ‡å‡†åŒ– (Scaleråœ¨å¹²å‡€çš„åŸå§‹è®­ç»ƒé›†ä¸Šfit)
        scaler_X = StandardScaler()
        X_train_scaled = scaler_X.fit_transform(self.X_train_orig)
        X_test_scaled = scaler_X.transform(self.X_test_orig)
        
        # 2. ç›®æ ‡æ ‡å‡†åŒ– (Scaleråœ¨å¹²å‡€çš„åŸå§‹è®­ç»ƒé›†ä¸Šfit)
        scaler_y = StandardScaler()
        y_train_clean_scaled = scaler_y.fit_transform(self.y_train_orig.reshape(-1, 1)).flatten()
        
        # 3. åœ¨æ ‡å‡†åŒ–åçš„è®­ç»ƒæ ‡ç­¾ä¸Šæ³¨å…¥å™ªå£°
        y_train_noisy_scaled, noise_indices = inject_shuffle_noise(
            y_train_clean_scaled, 
            noise_ratio=self.config.ANOMALY_RATIO,
            random_state=self.config.BASE_SEED
        )
        
        return {
            "X_train": X_train_scaled,
            "y_train": y_train_noisy_scaled,
            "X_test": X_test_scaled,
            "y_test_orig": self.y_test_orig,
            "scaler_y": scaler_y,
            "noise_indices": noise_indices
        }

    def _run_pytorch_mlp(self, data, run_id, random_state):
        print(f"   - æ­£åœ¨è¿è¡Œæ‰‹åŠ¨ PyTorch MLP (éšæœºç§å­: {random_state})")
        start_time = time.time()
        
        model = PyTorchMLP(
            input_size=data['X_train'].shape[1], 
            hidden_sizes=self.config.MLP_HIDDEN_SIZES, 
            random_state=random_state
        )
        
        model.fit(
            data['X_train'], data['y_train'], 
            epochs=self.config.MLP_MAX_ITER, 
            lr=self.config.MLP_LEARNING_RATE,
            alpha=self.config.MLP_ALPHA,
            early_stopping=self.config.EARLY_STOPPING_ENABLED,
            validation_fraction=self.config.VALIDATION_FRACTION,
            n_iter_no_change=self.config.EARLY_STOPPING_PATIENCE,
            tol=self.config.EARLY_STOPPING_TOL,
            random_state=random_state
        )
        
        y_pred_scaled = model.predict(data['X_test'])
        training_time = time.time() - start_time
        
        return {
            'training_time': training_time,
            'predictions_scaled': y_pred_scaled,
            'n_iter': model.n_iter_
        }

    def _run_sklearn_mlp(self, data, run_id, random_state):
        print(f"   - æ­£åœ¨è¿è¡Œ Sklearn MLP (éšæœºç§å­: {random_state})")
        start_time = time.time()
        
        model_params = {
            'hidden_layer_sizes': self.config.MLP_HIDDEN_SIZES,
            'max_iter': self.config.MLP_MAX_ITER,
            'learning_rate_init': self.config.MLP_LEARNING_RATE,
            'alpha': self.config.MLP_ALPHA,
            'random_state': random_state,
            'solver': 'adam',
            'batch_size': 'auto' # Sklearné»˜è®¤
        }
        if self.config.EARLY_STOPPING_ENABLED:
            model_params.update({
                'early_stopping': True,
                'validation_fraction': self.config.VALIDATION_FRACTION,
                'n_iter_no_change': self.config.EARLY_STOPPING_PATIENCE,
                'tol': self.config.EARLY_STOPPING_TOL
            })

        model = MLPRegressor(**model_params)
        model.fit(data['X_train'], data['y_train'])
        y_pred_scaled = model.predict(data['X_test'])
        training_time = time.time() - start_time
        
        return {
            'training_time': training_time,
            'predictions_scaled': y_pred_scaled,
            'n_iter': model.n_iter_
        }

    def _run_causal_sklearn_mlp(self, data, run_id, random_state):
        print(f"   - æ­£åœ¨è¿è¡Œ Causal-Sklearn MLP (éšæœºç§å­: {random_state})")
        start_time = time.time()

        model_params = {
            'hidden_layer_sizes': self.config.MLP_HIDDEN_SIZES,
            'max_iter': self.config.MLP_MAX_ITER,
            'learning_rate': self.config.MLP_LEARNING_RATE,
            'alpha': self.config.MLP_ALPHA,
            'random_state': random_state,
            'batch_size': None, # Noneè¡¨ç¤ºå…¨æ‰¹é‡
            'verbose': False
        }
        if self.config.EARLY_STOPPING_ENABLED:
            model_params.update({
                'early_stopping': True,
                'validation_fraction': self.config.VALIDATION_FRACTION,
                'n_iter_no_change': self.config.EARLY_STOPPING_PATIENCE,
                'tol': self.config.EARLY_STOPPING_TOL
            })

        model = MLPPytorchRegressor(**model_params)
        model.fit(data['X_train'], data['y_train'])
        y_pred_scaled = model.predict(data['X_test'])
        training_time = time.time() - start_time

        return {
            'training_time': training_time,
            'predictions_scaled': y_pred_scaled,
            'n_iter': model.n_iter_
        }

    def _evaluate_and_store_run(self, run_result, y_test_orig, scaler_y, result_container):
        y_pred_orig_scale = scaler_y.inverse_transform(run_result['predictions_scaled'].reshape(-1, 1)).flatten()
        mse = mean_squared_error(y_test_orig, y_pred_orig_scale)
        
        metrics = {
            'mse': mse,
            'r2': r2_score(y_test_orig, y_pred_orig_scale),
            'mae': mean_absolute_error(y_test_orig, y_pred_orig_scale),
            'mdae': median_absolute_error(y_test_orig, y_pred_orig_scale),
            'rmse': np.sqrt(mse),
            'training_time': run_result['training_time'],
            'n_iter': run_result['n_iter']
        }
        result_container.append(metrics)
        return metrics

    def run_comparison(self):
        print("\nğŸ“Š æ­¥éª¤ 2: å‡†å¤‡æ•°æ® (é‡‡ç”¨ç§‘å­¦çš„'å…ˆæ ‡å‡†åŒ–ã€ååŠ å™ª'ç­–ç•¥)")
        data = self._prepare_data_for_run()
        print(f"   - æ•°æ®æ ‡å‡†åŒ–å®Œæˆ (åŸºäºå¹²å‡€æ•°æ®)")
        print(f"   - å¯¹è®­ç»ƒé›†æ³¨å…¥ {self.config.ANOMALY_RATIO:.0%} çš„å¼‚å¸¸å€¼ ({len(data['noise_indices'])}ä¸ªæ ·æœ¬)")
        
        print(f"\nğŸš€ æ­¥éª¤ 3: å¼€å§‹è¿›è¡Œ {self.config.N_RUNS} æ¬¡å¯¹æ¯”å®éªŒ")
        for i in range(self.config.N_RUNS):
            run_seed = self.config.BASE_SEED + i * 100
            print(f"\n--- å®éªŒ {i+1}/{self.config.N_RUNS} (éšæœºç§å­: {run_seed}) ---")
            
            # è¿è¡Œæ¨¡å‹
            pytorch_run = self._run_pytorch_mlp(data, i+1, run_seed)
            sklearn_run = self._run_sklearn_mlp(data, i+1, run_seed)
            causal_sklearn_run = self._run_causal_sklearn_mlp(data, i+1, run_seed)
            
            # è¯„ä¼°å¹¶å­˜å‚¨ç»“æœ
            p_metrics = self._evaluate_and_store_run(pytorch_run, data['y_test_orig'], data['scaler_y'], self.results['pytorch'])
            s_metrics = self._evaluate_and_store_run(sklearn_run, data['y_test_orig'], data['scaler_y'], self.results['sklearn'])
            c_metrics = self._evaluate_and_store_run(causal_sklearn_run, data['y_test_orig'], data['scaler_y'], self.results['causal_sklearn'])
            
            print(f"   PyTorch        - MdAE: {p_metrics['mdae']:.4f}, R2: {p_metrics['r2']:.4f}, Time: {p_metrics['training_time']:.2f}s, Iters: {p_metrics['n_iter']}")
            print(f"   Sklearn        - MdAE: {s_metrics['mdae']:.4f}, R2: {s_metrics['r2']:.4f}, Time: {s_metrics['training_time']:.2f}s, Iters: {s_metrics['n_iter']}")
            print(f"   Causal-Sklearn - MdAE: {c_metrics['mdae']:.4f}, R2: {c_metrics['r2']:.4f}, Time: {c_metrics['training_time']:.2f}s, Iters: {c_metrics['n_iter']}")
        
        print("\nâœ… æ‰€æœ‰å®éªŒè¿è¡Œå®Œæˆï¼")

    def print_summary_stats(self):
        print("\n" + "=" * 60)
        print("ğŸ“Š æ­¥éª¤ 4: ç»“æœæ±‡æ€»åˆ†æ")
        print("=" * 60)
        
        metrics_to_extract = ['mse', 'r2', 'mae', 'mdae', 'rmse', 'training_time', 'n_iter']
        
        model_metrics = {
            "æ‰‹åŠ¨ PyTorch MLP": {m: [r[m] for r in self.results['pytorch']] for m in metrics_to_extract},
            "Sklearn MLP": {m: [r[m] for r in self.results['sklearn']] for m in metrics_to_extract},
            "Causal-Sklearn MLP": {m: [r[m] for r in self.results['causal_sklearn']] for m in metrics_to_extract}
        }

        def print_stats(name, metrics):
            print(f"\n{name} ç»Ÿè®¡ç»“æœ (å…± {self.config.N_RUNS} æ¬¡è¿è¡Œ):")
            for key, values in metrics.items():
                print(f"   - {key.upper():<13} - å¹³å‡å€¼: {np.mean(values):.4f}, æ ‡å‡†å·®: {np.std(values):.4f}")
        
        for name, metrics in model_metrics.items():
            print_stats(name, metrics)

    def plot_results(self):
        print("\n" + "=" * 60)
        print("ğŸ“Š æ­¥éª¤ 5: ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨")
        print("=" * 60)
        
        fig, axes = plt.subplots(3, 2, figsize=self.config.FIGURE_SIZE)
        fig.suptitle(f'Model Performance Comparison (Boxplot)\n{self.config.N_RUNS} runs on California Housing with {self.config.ANOMALY_RATIO:.0%} training noise', fontsize=16)
        
        all_metrics_map = {
            'MSE': 'mse', 'RMSE': 'rmse', 'MAE': 'mae', 
            'MdAE': 'mdae', 'RÂ²': 'r2', 'Training Time (s)': 'training_time'
        }
        
        labels = ['PyTorch', 'Sklearn', 'Causal-Sklearn']
        axes_flat = axes.flatten()
        
        for i, (title, metric_key) in enumerate(all_metrics_map.items()):
            ax = axes_flat[i]
            data_to_plot = [
                [r[metric_key] for r in self.results['pytorch']],
                [r[metric_key] for r in self.results['sklearn']],
                [r[metric_key] for r in self.results['causal_sklearn']]
            ]
            ax.boxplot(data_to_plot, labels=labels, patch_artist=True)
            ax.set_title(title)
            ax.set_ylabel(title)
            ax.grid(True, linestyle='--', alpha=0.6)

        plt.tight_layout(rect=[0, 0.03, 1, 0.95])
        
        save_path = self._get_output_path('pytorch_vs_sklearn_mlp_comparison.png')
        plt.savefig(save_path, dpi=self.config.FIGURE_DPI)
        print(f"   - å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜è‡³: {save_path}")
        plt.close()
        return save_path


def main():
    """ä¸»å‡½æ•°ï¼Œè¿è¡Œå®Œæ•´çš„å¯¹æ¯”å®éªŒ"""
    print("=" * 60)
    print("PyTorch vs Sklearn vs Causal-Sklearn MLP å®ç°å¯¹æ¯”")
    print("=" * 60)
    
    # 1. åˆ›å»ºé…ç½®å’Œæ•™ç¨‹å®ä¾‹
    config = ComparisonConfig()
    tutorial = MLPComparisonTutorial(config)
    
    # 2. è¿è¡Œå®éªŒæµç¨‹
    tutorial.load_and_split_data()
    tutorial.run_comparison()
    
    # 3. åˆ†æå’Œå¯è§†åŒ–ç»“æœ
    tutorial.print_summary_stats()
    save_path = tutorial.plot_results()
    
    # 4. æ‰“å°æœ€ç»ˆæ€»ç»“
    print("\n" + "=" * 60)
    print("âœ… åˆ†æå®Œæˆ")
    print("=" * 60)
    print(f"   - ç»“æœå›¾å·²ä¿å­˜è‡³: {save_path}")
    print("\nå…³é”®æ´è§:")
    print("1. ä¸‰ç§å®ç°ä½¿ç”¨ç›¸åŒçš„é¢„å¤„ç†ã€ç½‘ç»œç»“æ„å’Œè¶…å‚æ•°ï¼Œåœ¨å«å™ªæ•°æ®ä¸Šè®­ç»ƒã€‚")
    print("2. é‡‡ç”¨ç§‘å­¦çš„'å…ˆæ ‡å‡†åŒ–ã€ååŠ å™ª'æµç¨‹ï¼Œç¡®ä¿äº†å¯¹æ¯”çš„å…¬å¹³æ€§ã€‚")
    print("3. å¤šæ¬¡è¿è¡Œçš„ç»“æœï¼ˆç®±çº¿å›¾ï¼‰å±•ç¤ºäº†ä¸åŒå®ç°çš„æ€§èƒ½ç¨³å®šæ€§å’Œå›ºæœ‰æ–¹å·®ã€‚")
    print("4. è¿™ä¸ºä¸‰ç§MLPå®ç°åœ¨çœŸå®ä¸”æœ‰æŒ‘æˆ˜çš„åœºæ™¯ä¸‹æä¾›äº†ä¸€ä¸ªå…¬å¹³çš„æ€§èƒ½åŸºå‡†ã€‚")


if __name__ == "__main__":
    main()