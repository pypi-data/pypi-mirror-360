#!/usr/bin/env python3
"""
ÂàÜÁ±ªÁÆóÊ≥ïÁúüÂÆûÊï∞ÊçÆÈõÜÂô™Â£∞È≤ÅÊ£íÊÄßÊµãËØïËÑöÊú¨

üéØ ÁõÆÊ†áÔºöÂú®ÁúüÂÆûÊï∞ÊçÆÈõÜ‰∏äÊµãËØïÂàÜÁ±ªÁÆóÊ≥ïÁöÑÂô™Â£∞È≤ÅÊ£íÊÄßË°®Áé∞
üî¨ Ê†∏ÂøÉÔºö‰ΩøÁî®sklearnÂÜÖÁΩÆÂàÜÁ±ªÊï∞ÊçÆÈõÜÔºåÊµãËØï0%-100%Âô™Â£∞Á∫ßÂà´‰∏ãÁöÑÁÆóÊ≥ïÊÄßËÉΩ

‰∏ªË¶ÅÁâπÊÄßÔºö
- ÁúüÂÆûÊï∞ÊçÆÈõÜÔºöWine, Breast Cancer, Digits, IrisÁ≠âsklearnÂÜÖÁΩÆÊï∞ÊçÆÈõÜ
- ÂàÜÁ±ªÁÆóÊ≥ï8ÁßçÔºösklearn MLP, PyTorch MLP, sklearn OvR, PyTorch Shared OvR, CausalEngine(4ÁßçÊ®°Âºè)
- Âô™Â£∞Á∫ßÂà´Ôºö0%, 10%, 20%, ..., 100% (11‰∏™Á∫ßÂà´)
- ÂÆåÊï¥ÊåáÊ†áÔºöAccuracy, Precision, Recall, F1
- ÊäòÁ∫øÂõæÂèØËßÜÂåñÔºöÊ∏ÖÊô∞Â±ïÁ§∫ÁÆóÊ≥ïÂú®ÁúüÂÆûÊï∞ÊçÆ‰∏äÁöÑÈ≤ÅÊ£íÊÄßÂØπÊØî

‰ΩøÁî®ÊñπÊ≥ïÔºö
1. Áõ¥Êé•ËøêË°åÔºöpython scripts/classification_robustness_real_datasets.py
2. Ë∞ÉÊï¥ÂèÇÊï∞Ôºö‰øÆÊîπ‰∏ãÊñπÁöÑ REAL_DATASETS_CONFIG
3. ÈÄâÊã©Êï∞ÊçÆÈõÜÔºöÂú®ÈÖçÁΩÆ‰∏≠ÊåáÂÆö 'dataset' ÂèÇÊï∞
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.datasets import load_wine, load_breast_cancer, load_digits, load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import os
import sys
import warnings
from tqdm import tqdm
import pandas as pd

# ËÆæÁΩÆmatplotlibÂêéÁ´ØÔºåÈÅøÂÖçÂºπÂá∫Á™óÂè£
plt.switch_backend('Agg')

# Ê∑ªÂä†È°πÁõÆÊ†πÁõÆÂΩïÂà∞PythonË∑ØÂæÑ
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# ÂØºÂÖ•ÂàÜÁ±ªÂô®
from causal_sklearn.classifier import (
    MLPCausalClassifier, MLPPytorchClassifier, 
    MLPSklearnOvRClassifier, MLPPytorchSharedOvRClassifier
)
from causal_sklearn.data_processing import inject_shuffle_noise

warnings.filterwarnings('ignore')

# =============================================================================
# ÈÖçÁΩÆÈÉ®ÂàÜ - Âú®ËøôÈáå‰øÆÊîπÂÆûÈ™åÂèÇÊï∞
# =============================================================================

REAL_DATASETS_CONFIG = {
    # Êï∞ÊçÆÈõÜÈÄâÊã© - ÂèØÈÄâÊã©ÁöÑÁúüÂÆûÊï∞ÊçÆÈõÜ
    'dataset': 'breast_cancer',  # 'wine', 'breast_cancer', 'digits', 'iris'
    
    # Âô™Â£∞Á∫ßÂà´ËÆæÁΩÆ
    'noise_levels': np.linspace(0, 1, 11),  # 0%, 10%, 20%, ..., 100%
    
    # Êï∞ÊçÆÂèÇÊï∞
    'random_state': 42,     # Âõ∫ÂÆöÈöèÊú∫ÁßçÂ≠ê
    'test_size': 0.2,       # ÊµãËØïÈõÜÊØî‰æã
    
    # ÁΩëÁªúÁªìÊûÑÔºàÊâÄÊúâÁÆóÊ≥ïÁªü‰∏ÄÔºâ- ‰ºòÂåñÁ®≥ÂÆöÊÄßÈÖçÁΩÆ
    'hidden_layers': (128, 64, 64),      # ‰øùÊåÅÁΩëÁªúÁªìÊûÑ
    'max_iter': 3000,               # Ëøõ‰∏ÄÊ≠•Â¢ûÂä†ÊúÄÂ§ßËø≠‰ª£Ê¨°Êï∞
    'learning_rate': 0.001,        # Ëøõ‰∏ÄÊ≠•Èôç‰ΩéÂ≠¶‰π†ÁéáÊèêÈ´òÁ®≥ÂÆöÊÄß
    'patience': 100,                # Êó©ÂÅúËÄêÂøÉÔºàÁªü‰∏ÄÂèÇÊï∞ÔºåÈÄÇÁî®‰∫éÊâÄÊúâÁÆóÊ≥ïÔºâ
    'tol': 1e-4,                    # Êõ¥‰∏•Ê†ºÁöÑÊî∂ÊïõÂÆπÂøçÂ∫¶
    'batch_size': None,             # ÊâπÂ§ÑÁêÜÂ§ßÂ∞è (None=ÂÖ®ÊâπÊ¨°, Êï∞Â≠ó=Â∞èÊâπÊ¨°)
    
    # Á®≥ÂÆöÊÄßÊîπËøõÂèÇÊï∞
    'n_runs': 5,                     # Â¢ûÂä†Âà∞5Ê¨°ËøêË°å
    'base_random_seed': 42,          # Âü∫Á°ÄÈöèÊú∫ÁßçÂ≠ê
    
    # È¢ùÂ§ñÁ®≥ÂÆöÊÄßÂèÇÊï∞
    'validation_fraction': 0.2,     # È™åËØÅÈõÜÊØî‰æãÔºàÊó©ÂÅúÁî®Ôºâ
    'early_stopping': True,          # Á°Æ‰øùÊó©ÂÅúÂºÄÂêØ
    
    # ËæìÂá∫ÊéßÂà∂
    'output_dir': 'results/classification_real_datasets',
    'save_plots': True,
    'save_data': True,
    'verbose': True,
    'figure_dpi': 300
}

# ÂèØÁî®ÁöÑÁúüÂÆûÊï∞ÊçÆÈõÜ
REAL_DATASETS = {
    'wine': {
        'name': 'Wine Dataset',
        'loader': load_wine,
        'description': '178 samples, 13 features, 3 classes'
    },
    'breast_cancer': {
        'name': 'Breast Cancer Dataset', 
        'loader': load_breast_cancer,
        'description': '569 samples, 30 features, 2 classes'
    },
    'digits': {
        'name': 'Digits Dataset',
        'loader': load_digits,
        'description': '1797 samples, 64 features, 10 classes'
    },
    'iris': {
        'name': 'Iris Dataset',
        'loader': load_iris,
        'description': '150 samples, 4 features, 3 classes'
    }
}

# =============================================================================
# Â∑•ÂÖ∑ÂáΩÊï∞
# =============================================================================

def _ensure_output_dir(output_dir):
    """Á°Æ‰øùËæìÂá∫ÁõÆÂΩïÂ≠òÂú®"""
    if not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)

def _get_output_path(output_dir, filename):
    """Ëé∑ÂèñËæìÂá∫Êñá‰ª∂Ë∑ØÂæÑ"""
    return os.path.join(output_dir, filename)

def load_real_dataset(dataset_name):
    """Âä†ËΩΩÁúüÂÆûÊï∞ÊçÆÈõÜ"""
    if dataset_name not in REAL_DATASETS:
        raise ValueError(f"‰∏çÊîØÊåÅÁöÑÊï∞ÊçÆÈõÜ: {dataset_name}. ÂèØÈÄâÊã©: {list(REAL_DATASETS.keys())}")
    
    dataset_info = REAL_DATASETS[dataset_name]
    data = dataset_info['loader']()
    
    print(f"üìä Âä†ËΩΩÁúüÂÆûÊï∞ÊçÆÈõÜ: {dataset_info['name']}")
    print(f"üìã Êï∞ÊçÆÈõÜÊèèËø∞: {dataset_info['description']}")
    print(f"üìê ÂÆûÈôÖÊï∞ÊçÆÂΩ¢Áä∂: {data.data.shape}, Á±ªÂà´Êï∞: {len(np.unique(data.target))}")
    
    return data.data, data.target

# =============================================================================
# ÂàÜÁ±ªÈ≤ÅÊ£íÊÄßÊµãËØï
# =============================================================================

def test_classification_noise_robustness_real_data(config):
    """ÊµãËØïÂàÜÁ±ªÁÆóÊ≥ïÂú®ÁúüÂÆûÊï∞ÊçÆÈõÜ‰∏äÁöÑÂô™Â£∞È≤ÅÊ£íÊÄß"""
    print("\n" + "="*80)
    print("üéØ ÂàÜÁ±ªÁÆóÊ≥ïÁúüÂÆûÊï∞ÊçÆÈõÜÂô™Â£∞È≤ÅÊ£íÊÄßÊµãËØï")
    print("="*80)
    
    noise_levels = config['noise_levels']
    results = {}
    
    # ÂÆö‰πâÊâÄÊúâÂàÜÁ±ªÁÆóÊ≥ï - ÁÆÄÂåñ‰∏∫‰∏éÂèÇËÄÉËÑöÊú¨‰∏ÄËá¥
    algorithms = {
        # 'sklearn_mlp': ('sklearn MLP', None),
        'pytorch_mlp': ('PyTorch MLP', None),
        'sklearn_ovr': ('sklearn OvR MLP', None),
        'pytorch_shared_ovr': ('PyTorch Shared OvR', None),
        'causal_deterministic': ('CausalEngine (deterministic)', 'deterministic'),
        # 'causal_exogenous': ('CausalEngine (exogenous)', 'exogenous'),
        # 'causal_endogenous': ('CausalEngine (endogenous)', 'endogenous'),
        'causal_standard': ('CausalEngine (standard)', 'standard')
    }
    
    # ÂàùÂßãÂåñÁªìÊûúÂ≠óÂÖ∏
    for algo_key, (algo_name, _) in algorithms.items():
        results[algo_key] = {
            'name': algo_name,
            'noise_levels': [],
            'accuracy': [], 'precision': [], 'recall': [], 'f1': []
        }
    
    # Âä†ËΩΩÁúüÂÆûÊï∞ÊçÆÈõÜ
    X, y = load_real_dataset(config['dataset'])
    
    # ÂàÜÂâ≤Êï∞ÊçÆ
    X_train, X_test, y_train_clean, y_test = train_test_split(
        X, y, test_size=config['test_size'], random_state=config['random_state']
    )
    
    # Ê†áÂáÜÂåñÁâπÂæÅ
    scaler_X = StandardScaler()
    X_train_scaled = scaler_X.fit_transform(X_train)
    X_test_scaled = scaler_X.transform(X_test)
    
    # Âú®‰∏çÂêåÂô™Â£∞Á∫ßÂà´‰∏ãÊµãËØï
    for noise_level in tqdm(noise_levels, desc="Âô™Â£∞Á∫ßÂà´"):
        print(f"\nüìä ÊµãËØïÂô™Â£∞Á∫ßÂà´: {noise_level:.1%}")
        
        # ÂØπËÆ≠ÁªÉÊ†áÁ≠æÊ≥®ÂÖ•Âô™Â£∞
        if noise_level > 0:
            y_train_noisy, noise_indices = inject_shuffle_noise(
                y_train_clean,
                noise_ratio=noise_level,
                random_state=config['random_state']
            )
        else:
            y_train_noisy = y_train_clean.copy()
        
        # ÊµãËØïÊØè‰∏™ÁÆóÊ≥ï
        for algo_key, (algo_name, causal_mode) in algorithms.items():
            try:
                if config['verbose']:
                    print(f"  üîß ËÆ≠ÁªÉ {algo_name}...")
                
                # ÂàõÂª∫ÂíåËÆ≠ÁªÉÊ®°Âûã
                if algo_key == 'sklearn_mlp':
                    model = MLPClassifier(
                        hidden_layer_sizes=config['hidden_layers'],
                        max_iter=config['max_iter'],
                        learning_rate_init=config['learning_rate'],
                        batch_size=config['batch_size'],
                        random_state=config['random_state'],
                        early_stopping=config['early_stopping'],
                        validation_fraction=config['validation_fraction'],
                        n_iter_no_change=config['patience'],
                        tol=config['tol']
                    )
                    model.fit(X_train_scaled, y_train_noisy)
                    
                elif algo_key == 'pytorch_mlp':
                    model = MLPPytorchClassifier(
                        hidden_layer_sizes=config['hidden_layers'],
                        max_iter=config['max_iter'],
                        learning_rate=config['learning_rate'],
                        batch_size=config['batch_size'],
                        random_state=config['random_state'],
                        early_stopping=config['early_stopping'],
                        validation_fraction=config['validation_fraction'],
                        verbose=False
                    )
                    model.fit(X_train_scaled, y_train_noisy)
                    
                elif algo_key == 'sklearn_ovr':
                    model = MLPSklearnOvRClassifier(
                        hidden_layer_sizes=config['hidden_layers'],
                        max_iter=config['max_iter'],
                        learning_rate_init=config['learning_rate'],
                        batch_size=config['batch_size'],
                        random_state=config['random_state'],
                        early_stopping=config['early_stopping'],
                        validation_fraction=config['validation_fraction'],
                        n_iter_no_change=config['patience'],
                        tol=config['tol'],
                        verbose=False
                    )
                    model.fit(X_train_scaled, y_train_noisy)
                    
                elif algo_key == 'pytorch_shared_ovr':
                    model = MLPPytorchSharedOvRClassifier(
                        hidden_layer_sizes=config['hidden_layers'],
                        max_iter=config['max_iter'],
                        learning_rate=config['learning_rate'],
                        batch_size=config['batch_size'],
                        random_state=config['random_state'],
                        early_stopping=config['early_stopping'],
                        validation_fraction=config['validation_fraction'],
                        verbose=False
                    )
                    model.fit(X_train_scaled, y_train_noisy)
                    
                elif algo_key.startswith('causal_'):
                    model = MLPCausalClassifier(
                        perception_hidden_layers=config['hidden_layers'],
                        mode=causal_mode,
                        max_iter=config['max_iter'],
                        learning_rate=config['learning_rate'],
                        batch_size=config['batch_size'],
                        random_state=config['random_state'],
                        early_stopping=config['early_stopping'],
                        validation_fraction=config['validation_fraction'],
                        verbose=False
                    )
                    model.fit(X_train_scaled, y_train_noisy)
                
                # Âú®ÊµãËØïÈõÜ‰∏äËØÑ‰º∞
                y_pred = model.predict(X_test_scaled)
                
                # ËÆ°ÁÆóÊåáÊ†á
                accuracy = accuracy_score(y_test, y_pred)
                precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
                recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
                f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)
                
                # Â≠òÂÇ®ÁªìÊûú
                results[algo_key]['noise_levels'].append(noise_level)
                results[algo_key]['accuracy'].append(accuracy)
                results[algo_key]['precision'].append(precision)
                results[algo_key]['recall'].append(recall)
                results[algo_key]['f1'].append(f1)
                
                if config['verbose']:
                    print(f"    Acc: {accuracy:.4f}, F1: {f1:.4f}")
                
            except Exception as e:
                print(f"    ‚ùå {algo_name} ËÆ≠ÁªÉÂ§±Ë¥•: {str(e)}")
                # Ê∑ªÂä†NaNÂÄº‰øùÊåÅÊï∞ÁªÑÈïøÂ∫¶‰∏ÄËá¥
                results[algo_key]['noise_levels'].append(noise_level)
                results[algo_key]['accuracy'].append(np.nan)
                results[algo_key]['precision'].append(np.nan)
                results[algo_key]['recall'].append(np.nan)
                results[algo_key]['f1'].append(np.nan)
    
    return results

# =============================================================================
# ÂèØËßÜÂåñÂáΩÊï∞
# =============================================================================

def create_classification_robustness_plots(results, config):
    """ÂàõÂª∫ÂàÜÁ±ªÈ≤ÅÊ£íÊÄßÂàÜÊûêÊäòÁ∫øÂõæ"""
    if not config.get('save_plots', False):
        return
    
    _ensure_output_dir(config['output_dir'])
    
    print("\nüìä ÂàõÂª∫ÂàÜÁ±ªÈ≤ÅÊ£íÊÄßÂàÜÊûêÂõæË°®")
    print("-" * 50)
    
    # ËÆæÁΩÆÂõæË°®È£éÊ†º
    plt.style.use('seaborn-v0_8')
    colors = plt.cm.Set3(np.linspace(0, 1, 12))  # 12Áßç‰∏çÂêåÈ¢úËâ≤
    
    # ÂàõÂª∫ÂàÜÁ±ªÈ≤ÅÊ£íÊÄßÂõæË°®
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    dataset_name = REAL_DATASETS[config['dataset']]['name']
    n_runs = config.get('n_runs', 1)
    title_suffix = f' (Averaged over {n_runs} runs)' if n_runs > 1 else ''
    fig.suptitle(f'Classification Algorithms Noise Robustness on {dataset_name}{title_suffix}', 
                 fontsize=16, fontweight='bold')
    
    metrics = ['accuracy', 'precision', 'recall', 'f1']
    metric_names = ['Accuracy', 'Precision', 'Recall', 'F1']
    
    for i, (metric, metric_name) in enumerate(zip(metrics, metric_names)):
        ax = axes[i//2, i%2]
        
        color_idx = 0
        for algo_key, data in results.items():
            if data[metric]:  # Á°Æ‰øùÊúâÊï∞ÊçÆ
                noise_levels = np.array(data['noise_levels']) * 100  # ËΩ¨Êç¢‰∏∫ÁôæÂàÜÊØî
                values = np.array(data[metric])
                
                # ËøáÊª§NaNÂÄº
                valid_mask = ~np.isnan(values)
                if valid_mask.any():
                    # Âà§Êñ≠ÊòØÂê¶‰∏∫Âõ†ÊûúÁÆóÊ≥ï
                    is_causal = algo_key.startswith('causal_')
                    linestyle = '-' if is_causal else '--'  # Âõ†ÊûúÁÆóÊ≥ïÂÆûÁ∫øÔºåÂÖ∂‰ªñËôöÁ∫ø
                    
                    ax.plot(noise_levels[valid_mask], values[valid_mask], 
                           marker='o', linewidth=2, markersize=4, linestyle=linestyle,
                           label=data['name'], color=colors[color_idx])
                    color_idx += 1
        
        ax.set_xlabel('Noise Level (%)')
        ax.set_ylabel(metric_name)
        ax.set_title(f'{metric_name} vs Noise Level')
        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        ax.grid(True, alpha=0.3)
        ax.set_ylim(0, 1.1)
    
    plt.tight_layout()
    
    # ÁîüÊàêÊñá‰ª∂Âêç
    dataset_key = config['dataset']
    plot_filename = f'classification_robustness_{dataset_key}.png'
    plot_path = _get_output_path(config['output_dir'], plot_filename)
    
    plt.savefig(plot_path, dpi=config['figure_dpi'], bbox_inches='tight')
    print(f"üìä ÂàÜÁ±ªÈ≤ÅÊ£íÊÄßÂõæË°®Â∑≤‰øùÂ≠ò‰∏∫ {plot_path}")
    plt.close()

# =============================================================================
# ‰∏ªÂáΩÊï∞
# =============================================================================

def run_single_classification_robustness_analysis(config, run_idx=0):
    """ËøêË°åÂçïÊ¨°ÂàÜÁ±ªÈ≤ÅÊ£íÊÄßÂàÜÊûê"""
    if config['verbose']:
        print(f"\nüîÑ Á¨¨ {run_idx + 1}/{config['n_runs']} Ê¨°ËøêË°å (ÈöèÊú∫ÁßçÂ≠ê: {config['random_state']})")
    
    # ËøêË°åÂàÜÁ±ªÈ≤ÅÊ£íÊÄßÊµãËØï
    results = test_classification_noise_robustness_real_data(config)
    
    return results

def aggregate_classification_results(all_results):
    """ËÅöÂêàÂ§öÊ¨°ËøêË°åÁöÑÂàÜÁ±ªÁªìÊûú"""
    aggregated_results = {}
    
    if all_results and len(all_results) > 0:
        first_result = all_results[0]
        if first_result:
            for algo_key in first_result.keys():
                aggregated_results[algo_key] = {
                    'name': first_result[algo_key]['name'],
                    'noise_levels': first_result[algo_key]['noise_levels'],
                    'accuracy': [], 'precision': [], 'recall': [], 'f1': [],
                    'accuracy_std': [], 'precision_std': [], 'recall_std': [], 'f1_std': []
                }
                
                # Êî∂ÈõÜÊâÄÊúâËøêË°åÁöÑÁªìÊûú
                metrics = ['accuracy', 'precision', 'recall', 'f1']
                for metric in metrics:
                    all_values = []
                    for run_result in all_results:
                        if run_result and algo_key in run_result:
                            all_values.append(run_result[algo_key][metric])
                    
                    if all_values:
                        # ËÆ°ÁÆóÊØè‰∏™Âô™Â£∞Á∫ßÂà´ÁöÑÂùáÂÄºÂíåÊ†áÂáÜÂ∑Æ
                        all_values = np.array(all_values)  # shape: (n_runs, n_noise_levels)
                        means = np.nanmean(all_values, axis=0)
                        stds = np.nanstd(all_values, axis=0)
                        
                        aggregated_results[algo_key][metric] = means.tolist()
                        aggregated_results[algo_key][f'{metric}_std'] = stds.tolist()
    
    return aggregated_results

def run_classification_robustness_analysis(config=None):
    """ËøêË°åÂÆåÊï¥ÁöÑÂ§öÊ¨°ÂàÜÁ±ªÈ≤ÅÊ£íÊÄßÂàÜÊûê"""
    if config is None:
        config = REAL_DATASETS_CONFIG
    
    print("üöÄ ÂàÜÁ±ªÁÆóÊ≥ïÁúüÂÆûÊï∞ÊçÆÈõÜÂô™Â£∞È≤ÅÊ£íÊÄßÂàÜÊûê (Á®≥ÂÆöÊÄß‰ºòÂåñÁâàÊú¨)")
    print("=" * 70)
    print(f"Êï∞ÊçÆÈõÜ: {REAL_DATASETS[config['dataset']]['name']}")
    print(f"Âô™Â£∞Á∫ßÂà´: {config['noise_levels'][0]:.0%} - {config['noise_levels'][-1]:.0%} ({len(config['noise_levels'])}‰∏™Á∫ßÂà´)")
    print(f"ËøêË°åÊ¨°Êï∞: {config['n_runs']}Ê¨° (ÈöèÊú∫ÁßçÂ≠ê: {config['base_random_seed']} - {config['base_random_seed'] + config['n_runs'] - 1})")
    print(f"Á®≥ÂÆöÊÄßÈÖçÁΩÆ: Â≠¶‰π†Áéá={config['learning_rate']}, Ëø≠‰ª£={config['max_iter']}, ËÄêÂøÉ={config['patience']}, ÂÆπÂøçÂ∫¶={config['tol']}")
    print(f"È™åËØÅÈõÜÊØî‰æã: {config['validation_fraction']}, Êó©ÂÅúËÄêÂøÉ: {config['n_iter_no_change']}")
    
    all_results = []
    
    # Â§öÊ¨°ËøêË°å
    for run_idx in range(config['n_runs']):
        # ‰∏∫ÊØèÊ¨°ËøêË°åËÆæÁΩÆ‰∏çÂêåÁöÑÈöèÊú∫ÁßçÂ≠ê
        run_config = config.copy()
        run_config['random_state'] = config['base_random_seed'] + run_idx
        
        result = run_single_classification_robustness_analysis(run_config, run_idx)
        all_results.append(result)
    
    # ËÅöÂêàÁªìÊûú
    print(f"\nüìä ËÅöÂêà {config['n_runs']} Ê¨°ËøêË°åÁöÑÁªìÊûú...")
    aggregated_results = aggregate_classification_results(all_results)
    
    # ÂàõÂª∫ÂèØËßÜÂåñÔºà‰ΩøÁî®ËÅöÂêàÂêéÁöÑÁªìÊûúÔºâ
    create_classification_robustness_plots(aggregated_results, config)
    
    # ‰øùÂ≠òÁªìÊûúÊï∞ÊçÆ
    if config.get('save_data', True):
        _ensure_output_dir(config['output_dir'])
        
        dataset_key = config['dataset']
        data_filename = f'classification_results_{dataset_key}_aggregated.npy'
        data_path = _get_output_path(config['output_dir'], data_filename)
        
        np.save(data_path, aggregated_results)
        print(f"üìä ËÅöÂêàÂàÜÁ±ªÁªìÊûúÂ∑≤‰øùÂ≠ò‰∏∫ {data_path}")
    
    print(f"\n‚úÖ Â§öÊ¨°ËøêË°åÂàÜÁ±ªÈ≤ÅÊ£íÊÄßÂàÜÊûêÂÆåÊàê!")
    print(f"üìä ÁªìÊûú‰øùÂ≠òÂú®: {config['output_dir']}")
    print(f"üéØ Êï∞ÊçÆÈõÜ: {REAL_DATASETS[config['dataset']]['name']}")
    print(f"üéØ Á®≥ÂÆöÊÄßÊèêÂçá: ÈÄöËøá {config['n_runs']} Ê¨°ËøêË°åÂèñÂπ≥ÂùáÔºåÈôç‰ΩéÈöèÊú∫Ê≥¢Âä®")
    
    return aggregated_results

# =============================================================================
# ÊâπÈáèÊµãËØïÂ§ö‰∏™Êï∞ÊçÆÈõÜ
# =============================================================================

def run_all_datasets_analysis():
    """ËøêË°åÊâÄÊúâÊï∞ÊçÆÈõÜÁöÑÂàÜÁ±ªÈ≤ÅÊ£íÊÄßÂàÜÊûêÔºàÂ§öÊ¨°ËøêË°åÁâàÊú¨Ôºâ"""
    print("üöÄ ÊâπÈáèÊµãËØïÊâÄÊúâÁúüÂÆûÊï∞ÊçÆÈõÜ (Â§öÊ¨°ËøêË°åÁâàÊú¨)")
    print("=" * 70)
    
    all_results = {}
    
    for dataset_name in REAL_DATASETS.keys():
        print(f"\nüîÑ ÂºÄÂßãÊµãËØïÊï∞ÊçÆÈõÜ: {dataset_name}")
        
        # ÂàõÂª∫ÁâπÂÆöÊï∞ÊçÆÈõÜÁöÑÈÖçÁΩÆ
        dataset_config = REAL_DATASETS_CONFIG.copy()
        dataset_config['dataset'] = dataset_name
        
        try:
            results = run_classification_robustness_analysis(dataset_config)
            all_results[dataset_name] = results
            print(f"‚úÖ Êï∞ÊçÆÈõÜ {dataset_name} ÊµãËØïÂÆåÊàê")
        except Exception as e:
            print(f"‚ùå Êï∞ÊçÆÈõÜ {dataset_name} ÊµãËØïÂ§±Ë¥•: {str(e)}")
    
    print(f"\nüéâ ÊâÄÊúâÊï∞ÊçÆÈõÜÊµãËØïÂÆåÊàê!")
    print(f"üìä ÊÄªËÆ°ÊµãËØï‰∫Ü {len(all_results)} ‰∏™Êï∞ÊçÆÈõÜ")
    print(f"üéØ ÊØè‰∏™Êï∞ÊçÆÈõÜËøêË°å‰∫Ü {REAL_DATASETS_CONFIG['n_runs']} Ê¨°ÂèñÂπ≥Âùá")
    
    return all_results

# =============================================================================
# ÂÖ•Âè£ÁÇπ
# =============================================================================

if __name__ == '__main__':
    # ÂèØ‰ª•ÈÄâÊã©ËøêË°åÂçï‰∏™Êï∞ÊçÆÈõÜÊàñÊâÄÊúâÊï∞ÊçÆÈõÜ
    
    # ËøêË°åÂçï‰∏™Êï∞ÊçÆÈõÜÂàÜÊûê
    results = run_classification_robustness_analysis()
    
    # ÂèñÊ∂àÊ≥®Èáä‰∏ãÈù¢ÁöÑË°åÊù•ËøêË°åÊâÄÊúâÊï∞ÊçÆÈõÜÁöÑÂàÜÊûê
    # all_results = run_all_datasets_analysis()