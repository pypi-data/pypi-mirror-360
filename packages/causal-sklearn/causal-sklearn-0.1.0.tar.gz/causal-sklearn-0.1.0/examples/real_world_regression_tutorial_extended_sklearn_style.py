#!/usr/bin/env python3
"""
ğŸ  æ‰©å±•ç‰ˆçœŸå®ä¸–ç•Œå›å½’æ•™ç¨‹ï¼šåŠ å·æˆ¿ä»·é¢„æµ‹ - Sklearn-Styleç‰ˆæœ¬
============================================================

è¿™ä¸ªæ•™ç¨‹æ¼”ç¤ºCausalEngineä¸å¤šç§å¼ºåŠ›ä¼ ç»Ÿæ–¹æ³•åœ¨çœŸå®ä¸–ç•Œå›å½’ä»»åŠ¡ä¸­çš„æ€§èƒ½å¯¹æ¯”ã€‚

æ•°æ®é›†ï¼šåŠ å·æˆ¿ä»·æ•°æ®é›†ï¼ˆCalifornia Housing Datasetï¼‰
- 20,640ä¸ªæ ·æœ¬
- 8ä¸ªç‰¹å¾ï¼ˆæˆ¿å±‹å¹´é¾„ã€æ”¶å…¥ã€äººå£ç­‰ï¼‰
- ç›®æ ‡ï¼šé¢„æµ‹æˆ¿ä»·ä¸­ä½æ•°

æˆ‘ä»¬å°†æ¯”è¾ƒ13ç§æ–¹æ³•ï¼š
1. sklearn MLPRegressorï¼ˆä¼ ç»Ÿç¥ç»ç½‘ç»œï¼‰
2. PyTorch MLPï¼ˆä¼ ç»Ÿæ·±åº¦å­¦ä¹ ï¼‰
3. MLP Huberï¼ˆHuberæŸå¤±ç¨³å¥å›å½’ï¼‰
4. MLP Pinballï¼ˆPinballæŸå¤±ç¨³å¥å›å½’ï¼‰
5. MLP Cauchyï¼ˆCauchyæŸå¤±ç¨³å¥å›å½’ï¼‰
6. Random Forestï¼ˆéšæœºæ£®æ—ï¼‰
7. XGBoostï¼ˆæ¢¯åº¦æå‡ï¼‰
8. LightGBMï¼ˆè½»é‡æ¢¯åº¦æå‡ï¼‰
9. CatBoostï¼ˆå¼ºåŠ›æ¢¯åº¦æå‡ï¼‰
10. CausalEngine - deterministicï¼ˆç¡®å®šæ€§ï¼‰
11. CausalEngine - exogenousï¼ˆå¤–ç”Ÿå™ªå£°ä¸»å¯¼ï¼‰
12. CausalEngine - endogenousï¼ˆå†…ç”Ÿä¸ç¡®å®šæ€§ä¸»å¯¼ï¼‰
13. CausalEngine - standardï¼ˆå†…ç”Ÿ+å¤–ç”Ÿæ··åˆï¼‰

å…³é”®äº®ç‚¹ï¼š
- çœŸå®ä¸–ç•Œæ•°æ®çš„é²æ£’æ€§æµ‹è¯•
- 6ç§å¼ºåŠ›ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•å¯¹æ¯”
- 3ç§ç¨³å¥ç¥ç»ç½‘ç»œå›å½’æ–¹æ³•ï¼ˆHuberã€Pinballã€Cauchyï¼‰
- 4ç§CausalEngineæ¨¡å¼å®Œæ•´å¯¹æ¯”
- ç»Ÿä¸€ç¥ç»ç½‘ç»œå‚æ•°é…ç½®ç¡®ä¿å…¬å¹³æ¯”è¾ƒ
- å› æœæ¨ç†vsä¼ ç»Ÿæ–¹æ³•çš„æ€§èƒ½å·®å¼‚åˆ†æ
- ä½¿ç”¨sklearn-style regressorå®ç°

å®éªŒè®¾è®¡è¯´æ˜
==================================================================
æœ¬è„šæœ¬åŒ…å«ä¸¤ç»„æ ¸å¿ƒå®éªŒï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°CausalEngineåœ¨çœŸå®å›å½’ä»»åŠ¡ä¸Šçš„æ€§èƒ½å’Œé²æ£’æ€§ã€‚
æ‰€æœ‰å®éªŒå‚æ•°å‡å¯åœ¨ä¸‹æ–¹çš„ `TutorialConfig` ç±»ä¸­è¿›è¡Œä¿®æ”¹ã€‚

å®éªŒä¸€ï¼šæ ¸å¿ƒæ€§èƒ½å¯¹æ¯” (åœ¨40%æ ‡ç­¾å™ªå£°ä¸‹)
--------------------------------------------------
- **ç›®æ ‡**: æ¯”è¾ƒCausalEngineå’Œ9ç§ä¼ ç»Ÿæ–¹æ³•åœ¨å«æœ‰å›ºå®šå™ªå£°æ•°æ®ä¸Šçš„é¢„æµ‹æ€§èƒ½ã€‚
- **è®¾ç½®**: é»˜è®¤è®¾ç½®40%çš„æ ‡ç­¾å™ªå£°ï¼ˆ`ANOMALY_RATIO = 0.4`ï¼‰ï¼Œæ¨¡æ‹ŸçœŸå®ä¸–ç•Œä¸­å¸¸è§çš„æ•°æ®è´¨é‡é—®é¢˜ã€‚
- **å¯¹æ¯”æ¨¡å‹**: 
  - ä¼ ç»Ÿæ–¹æ³•: sklearn MLP, PyTorch MLP, MLP Huber, MLP Pinball, MLP Cauchy, Random Forest, XGBoost, LightGBM, CatBoost
  - CausalEngine: deterministic, exogenous, endogenous, standardç­‰æ¨¡å¼

å®éªŒäºŒï¼šé²æ£’æ€§åˆ†æ (è·¨è¶Šä¸åŒå™ªå£°æ°´å¹³)
--------------------------------------------------
- **ç›®æ ‡**: æ¢ç©¶æ¨¡å‹æ€§èƒ½éšæ ‡ç­¾å™ªå£°æ°´å¹³å¢åŠ æ—¶çš„å˜åŒ–æƒ…å†µï¼Œè¯„ä¼°å…¶ç¨³å®šæ€§ã€‚
- **è®¾ç½®**: åœ¨ä¸€ç³»åˆ—å™ªå£°æ¯”ä¾‹ï¼ˆå¦‚0%, 10%, 20%, 30%, 40%, 50%ï¼‰ä¸‹åˆ†åˆ«è¿è¡Œæµ‹è¯•ã€‚
- **å¯¹æ¯”æ¨¡å‹**: æ‰€æœ‰13ç§æ–¹æ³•åœ¨ä¸åŒå™ªå£°æ°´å¹³ä¸‹çš„è¡¨ç°
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, median_absolute_error
from sklearn.neural_network import MLPRegressor
from sklearn.ensemble import RandomForestRegressor
import warnings
import os
import sys
import time

# è®¾ç½®matplotlibåç«¯ä¸ºéäº¤äº’å¼ï¼Œé¿å…å¼¹å‡ºçª—å£
plt.switch_backend('Agg')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥sklearn-styleå®ç°
from causal_sklearn.regressor import (
    MLPCausalRegressor, MLPPytorchRegressor, 
    MLPHuberRegressor, MLPPinballRegressor, MLPCauchyRegressor
)
from causal_sklearn.data_processing import inject_shuffle_noise

# å¯¼å…¥æ ‘æ¨¡å‹ - å¤„ç†å¯èƒ½çš„å¯¼å…¥é”™è¯¯
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False

try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False

try:
    import catboost as cb
    CATBOOST_AVAILABLE = True
except ImportError:
    CATBOOST_AVAILABLE = False

warnings.filterwarnings('ignore')


class TutorialConfig:
    """
    æ‰©å±•æ•™ç¨‹é…ç½®ç±» - æ–¹ä¾¿è°ƒæ•´å„ç§å‚æ•°
    
    ğŸ”§ åœ¨è¿™é‡Œä¿®æ”¹å‚æ•°æ¥è‡ªå®šä¹‰å®éªŒè®¾ç½®ï¼
    """
    
    # ğŸ¯ æ•°æ®åˆ†å‰²å‚æ•°
    TEST_SIZE = 0.2          # æµ‹è¯•é›†æ¯”ä¾‹
    VAL_SIZE = 0.2           # éªŒè¯é›†æ¯”ä¾‹ (ç›¸å¯¹äºè®­ç»ƒé›†)
    RANDOM_STATE = 42        # éšæœºç§å­
    
    # ğŸ§  ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½® - æ‰€æœ‰ç¥ç»ç½‘ç»œæ–¹æ³•ä½¿ç”¨ç›¸åŒå‚æ•°ç¡®ä¿å…¬å¹³æ¯”è¾ƒ
    # =========================================================================
    # ğŸ”§ åœ¨è¿™é‡Œä¿®æ”¹æ‰€æœ‰ç¥ç»ç½‘ç»œæ–¹æ³•çš„å…±åŒå‚æ•°ï¼
    NN_HIDDEN_SIZES = (128, 64, 32)                 # ç¥ç»ç½‘ç»œéšè—å±‚ç»“æ„
    NN_MAX_EPOCHS = 3000                            # æœ€å¤§è®­ç»ƒè½®æ•°
    NN_LEARNING_RATE = 0.01                         # å­¦ä¹ ç‡
    NN_PATIENCE = 200                               # æ—©åœpatience
    NN_TOLERANCE = 1e-4                             # æ—©åœtolerance
    # =========================================================================
    
    # ğŸ¤– CausalEngineå‚æ•° - ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    CAUSAL_MODES = ['deterministic', 'exogenous', 'endogenous', 'standard']  # å¯é€‰: ['deterministic', 'exogenous', 'endogenous', 'standard']
    CAUSAL_HIDDEN_SIZES = NN_HIDDEN_SIZES          # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    CAUSAL_MAX_EPOCHS = NN_MAX_EPOCHS              # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    CAUSAL_LR = NN_LEARNING_RATE                   # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    CAUSAL_PATIENCE = NN_PATIENCE                  # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    CAUSAL_TOL = NN_TOLERANCE                      # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    CAUSAL_GAMMA_INIT = 1.0                      # gammaåˆå§‹åŒ–
    CAUSAL_B_NOISE_INIT = 1.0                    # b_noiseåˆå§‹åŒ–
    CAUSAL_B_NOISE_TRAINABLE = True              # b_noiseæ˜¯å¦å¯è®­ç»ƒ
    
    # ğŸ§  ä¼ ç»Ÿç¥ç»ç½‘ç»œæ–¹æ³•å‚æ•° - ä½¿ç”¨ç»Ÿä¸€é…ç½®
    SKLEARN_HIDDEN_LAYERS = NN_HIDDEN_SIZES         # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    SKLEARN_MAX_ITER = NN_MAX_EPOCHS                # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    SKLEARN_LR = NN_LEARNING_RATE                   # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    
    PYTORCH_HIDDEN_SIZES = NN_HIDDEN_SIZES          # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    PYTORCH_EPOCHS = NN_MAX_EPOCHS                  # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    PYTORCH_LR = NN_LEARNING_RATE                   # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    PYTORCH_PATIENCE = NN_PATIENCE                  # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    
    # ğŸ¯ åŸºå‡†æ–¹æ³•é…ç½® - æ‰©å±•ç‰ˆåŒ…å«æ›´å¤šå¼ºåŠ›æ–¹æ³•
    BASELINE_METHODS = [
        'sklearn_mlp',       # sklearnç¥ç»ç½‘ç»œ  
        'pytorch_mlp',       # PyTorchç¥ç»ç½‘ç»œ
        'mlp_huber',         # HuberæŸå¤±MLPï¼ˆç¨³å¥å›å½’ï¼‰
        'mlp_pinball_median',# PinballæŸå¤±MLPï¼ˆç¨³å¥å›å½’ï¼‰
        'mlp_cauchy',        # CauchyæŸå¤±MLPï¼ˆç¨³å¥å›å½’ï¼‰
        'random_forest',     # éšæœºæ£®æ—
        'xgboost',           # XGBoost - å¼ºåŠ›æ¢¯åº¦æå‡
        'lightgbm',          # LightGBM - è½»é‡æ¢¯åº¦æå‡
        'catboost'           # CatBoost - å¼ºåŠ›æ¢¯åº¦æå‡
    ]
    
    # ğŸ›¡ï¸ ç¨³å¥å›å½’å™¨å‚æ•° - ä½¿ç”¨ç»Ÿä¸€é…ç½®
    ROBUST_HIDDEN_SIZES = NN_HIDDEN_SIZES           # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    ROBUST_MAX_EPOCHS = NN_MAX_EPOCHS               # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    ROBUST_LR = NN_LEARNING_RATE                    # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    ROBUST_PATIENCE = NN_PATIENCE                   # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    
    # ğŸŒ² æ ‘æ¨¡å‹å‚æ•°
    TREE_N_ESTIMATORS = 100                         # æ ‘çš„æ•°é‡
    TREE_RANDOM_STATE = RANDOM_STATE                # éšæœºç§å­
    TREE_MAX_DEPTH = None                           # æœ€å¤§æ·±åº¦ï¼ˆNoneè¡¨ç¤ºä¸é™åˆ¶ï¼‰
    
    # XGBoost å‚æ•°
    XGBOOST_N_ESTIMATORS = 100
    XGBOOST_LEARNING_RATE = 0.1
    XGBOOST_MAX_DEPTH = 6
    
    # LightGBM å‚æ•°
    LIGHTGBM_N_ESTIMATORS = 100
    LIGHTGBM_LEARNING_RATE = 0.1
    LIGHTGBM_MAX_DEPTH = -1
    
    # CatBoost å‚æ•°
    CATBOOST_ITERATIONS = 100
    CATBOOST_LEARNING_RATE = 0.1
    CATBOOST_DEPTH = 6
    
    # ğŸ“Š å®éªŒå‚æ•°
    ANOMALY_RATIO = 0.4                          # æ ‡ç­¾å¼‚å¸¸æ¯”ä¾‹ (æ ¸å¿ƒå®éªŒé»˜è®¤å€¼: 40%å™ªå£°æŒ‘æˆ˜)
    SAVE_PLOTS = True                            # æ˜¯å¦ä¿å­˜å›¾è¡¨
    VERBOSE = True                               # æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†è¾“å‡º
    
    # ğŸ›¡ï¸ é²æ£’æ€§æµ‹è¯•å‚æ•° - éªŒè¯"CausalEngineé²æ£’æ€§ä¼˜åŠ¿"çš„å‡è®¾
    ROBUSTNESS_ANOMALY_RATIOS = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]  # å™ªå£°æ°´å¹³
    RUN_ROBUSTNESS_TEST = True                      # æ˜¯å¦è¿è¡Œé²æ£’æ€§æµ‹è¯•
    
    # ğŸ“ˆ å¯è§†åŒ–å‚æ•°
    FIGURE_DPI = 300                             # å›¾è¡¨åˆ†è¾¨ç‡
    FIGURE_SIZE_ANALYSIS = (24, 20)              # æ•°æ®åˆ†æå›¾è¡¨å¤§å°
    FIGURE_SIZE_PERFORMANCE = (24, 20)           # æ€§èƒ½å¯¹æ¯”å›¾è¡¨å¤§å°
    FIGURE_SIZE_ROBUSTNESS = (24, 20)            # é²æ£’æ€§æµ‹è¯•å›¾è¡¨å¤§å°
    
    # ğŸ“ è¾“å‡ºç›®å½•å‚æ•°
    OUTPUT_DIR = "results/california_housing_regression_extended_sklearn_style"  # è¾“å‡ºç›®å½•åç§°


class ExtendedCaliforniaHousingTutorialSklearnStyle:
    """
    æ‰©å±•ç‰ˆåŠ å·æˆ¿ä»·å›å½’æ•™ç¨‹ç±» - Sklearn-Styleç‰ˆæœ¬
    
    æ¼”ç¤ºCausalEngineåœ¨çœŸå®ä¸–ç•Œå›å½’ä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§èƒ½
    """
    
    def __init__(self, config=None):
        self.config = config if config is not None else TutorialConfig()
        self.X = None
        self.y = None
        self.feature_names = None
        self.results = {}
        self._ensure_output_dir()
    
    def _ensure_output_dir(self):
        """ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨"""
        if not os.path.exists(self.config.OUTPUT_DIR):
            os.makedirs(self.config.OUTPUT_DIR)
            print(f"ğŸ“ åˆ›å»ºè¾“å‡ºç›®å½•: {self.config.OUTPUT_DIR}/")
    
    def _get_output_path(self, filename):
        """è·å–è¾“å‡ºæ–‡ä»¶çš„å®Œæ•´è·¯å¾„"""
        return os.path.join(self.config.OUTPUT_DIR, filename)
        
    def load_and_explore_data(self, verbose=True):
        """åŠ è½½å¹¶æ¢ç´¢åŠ å·æˆ¿ä»·æ•°æ®é›†"""
        if verbose:
            print("ğŸ  æ‰©å±•ç‰ˆåŠ å·æˆ¿ä»·é¢„æµ‹ - çœŸå®ä¸–ç•Œå›å½’æ•™ç¨‹ (Sklearn-Style)")
            print("=" * 80)
            print("ğŸ“Š æ­£åœ¨åŠ è½½åŠ å·æˆ¿ä»·æ•°æ®é›†...")
        
        # åŠ è½½æ•°æ®
        housing = fetch_california_housing()
        self.X, self.y = housing.data, housing.target
        self.feature_names = housing.feature_names
        
        if verbose:
            print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ")
            print(f"   - æ ·æœ¬æ•°é‡: {self.X.shape[0]:,}")
            print(f"   - ç‰¹å¾æ•°é‡: {self.X.shape[1]}")
            print(f"   - ç‰¹å¾åç§°: {', '.join(self.feature_names)}")
            print(f"   - ç›®æ ‡èŒƒå›´: ${self.y.min():.2f} - ${self.y.max():.2f} (ç™¾ä¸‡ç¾å…ƒ)")
            print(f"   - ç›®æ ‡å‡å€¼: ${self.y.mean():.2f}")
            print(f"   - ç›®æ ‡æ ‡å‡†å·®: ${self.y.std():.2f}")
            print(f"   - å°†æ¯”è¾ƒ {len(self.config.CAUSAL_MODES) + 9} ç§æ–¹æ³•")
        
        return self.X, self.y
    
    def visualize_data(self, save_plots=None):
        """æ•°æ®å¯è§†åŒ–åˆ†æ"""
        if save_plots is None:
            save_plots = self.config.SAVE_PLOTS
            
        print("\\nğŸ“ˆ æ•°æ®åˆ†å¸ƒåˆ†æ")
        print("-" * 30)
        
        # åˆ›å»ºå›¾å½¢
        fig, axes = plt.subplots(2, 2, figsize=self.config.FIGURE_SIZE_ANALYSIS)
        fig.suptitle('California Housing Dataset Analysis - Extended Regression Tutorial', fontsize=16, fontweight='bold')
        
        # 1. ç›®æ ‡å˜é‡åˆ†å¸ƒ
        axes[0, 0].hist(self.y, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
        axes[0, 0].set_title('House Price Distribution')
        axes[0, 0].set_xlabel('House Price ($100k)')
        axes[0, 0].set_ylabel('Frequency')
        axes[0, 0].axvline(self.y.mean(), color='red', linestyle='--', label=f'Mean: ${self.y.mean():.2f}')
        axes[0, 0].legend()
        
        # 2. ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾
        df = pd.DataFrame(self.X, columns=self.feature_names)
        df['MedHouseVal'] = self.y
        corr_matrix = df.corr()
        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, 
                   square=True, ax=axes[0, 1], cbar_kws={'shrink': 0.8})
        axes[0, 1].set_title('Feature Correlation Matrix')
        
        # 3. ç‰¹å¾åˆ†å¸ƒç®±çº¿å›¾
        df_features = pd.DataFrame(self.X, columns=self.feature_names)
        df_features_normalized = (df_features - df_features.mean()) / df_features.std()
        df_features_normalized.boxplot(ax=axes[1, 0])
        axes[1, 0].set_title('Feature Distribution (Standardized)')
        axes[1, 0].set_xlabel('Features')
        axes[1, 0].set_ylabel('Standardized Values')
        axes[1, 0].tick_params(axis='x', rotation=45)
        
        # 4. æœ€é‡è¦ç‰¹å¾ä¸ç›®æ ‡çš„æ•£ç‚¹å›¾
        most_corr_feature = corr_matrix['MedHouseVal'].abs().sort_values(ascending=False).index[1]
        most_corr_idx = list(self.feature_names).index(most_corr_feature)
        axes[1, 1].scatter(self.X[:, most_corr_idx], self.y, alpha=0.5, s=1)
        axes[1, 1].set_title(f'Most Correlated Feature: {most_corr_feature}')
        axes[1, 1].set_xlabel(most_corr_feature)
        axes[1, 1].set_ylabel('House Price ($100k)')
        
        plt.tight_layout()
        
        if save_plots:
            output_path = self._get_output_path('extended_data_analysis.png')
            plt.savefig(output_path, dpi=self.config.FIGURE_DPI, bbox_inches='tight')
            print(f"ğŸ“Š æ•°æ®åˆ†æå›¾è¡¨å·²ä¿å­˜ä¸º {output_path}")
        
        plt.close()  # å…³é—­å›¾å½¢ï¼Œé¿å…å†…å­˜æ³„æ¼
        
        # æ•°æ®ç»Ÿè®¡æ‘˜è¦
        print("\\nğŸ“‹ æ•°æ®ç»Ÿè®¡æ‘˜è¦:")
        print(f"  - æœ€ç›¸å…³ç‰¹å¾: {most_corr_feature} (ç›¸å…³ç³»æ•°: {corr_matrix.loc[most_corr_feature, 'MedHouseVal']:.3f})")
        print(f"  - å¼‚å¸¸å€¼æ£€æµ‹: {np.sum(np.abs(self.y - self.y.mean()) > 3 * self.y.std())} ä¸ªæ½œåœ¨å¼‚å¸¸å€¼")
        print(f"  - æ•°æ®å®Œæ•´æ€§: æ— ç¼ºå¤±å€¼" if not np.any(np.isnan(self.X)) else "  - è­¦å‘Š: å­˜åœ¨ç¼ºå¤±å€¼")
    
    def _prepare_data(self, test_size, val_size, anomaly_ratio, random_state):
        """å‡†å¤‡è®­ç»ƒå’Œæµ‹è¯•æ•°æ®"""
        # æ•°æ®åˆ†å‰²
        X_temp, X_test, y_temp, y_test = train_test_split(
            self.X, self.y, test_size=test_size, random_state=random_state
        )
        
        X_train, X_val, y_train, y_val = train_test_split(
            X_temp, y_temp, test_size=val_size, random_state=random_state
        )
        
        # æ³¨å…¥å™ªå£°
        if anomaly_ratio > 0:
            y_train_noisy, noise_indices = inject_shuffle_noise(
                y_train, noise_ratio=anomaly_ratio, random_state=random_state
            )
            y_train = y_train_noisy
            if self.config.VERBOSE:
                print(f"   å¼‚å¸¸æ³¨å…¥: {anomaly_ratio:.1%} ({len(noise_indices)}/{len(y_train)} æ ·æœ¬å—å½±å“)")
        
        # æ ‡å‡†åŒ–
        scaler_X = StandardScaler()
        scaler_y = StandardScaler()
        
        X_train_scaled = scaler_X.fit_transform(X_train)
        X_val_scaled = scaler_X.transform(X_val)
        X_test_scaled = scaler_X.transform(X_test)
        
        y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()
        y_val_scaled = scaler_y.transform(y_val.reshape(-1, 1)).flatten()
        
        return {
            'X_train': X_train_scaled, 'X_val': X_val_scaled, 'X_test': X_test_scaled,
            'y_train': y_train_scaled, 'y_val': y_val_scaled, 'y_test': y_test,
            'scaler_X': scaler_X, 'scaler_y': scaler_y
        }
    
    def _train_sklearn_model(self, data):
        """è®­ç»ƒsklearnæ¨¡å‹"""
        print("ğŸ”§ è®­ç»ƒ sklearn MLPRegressor...")
        
        model = MLPRegressor(
            hidden_layer_sizes=self.config.SKLEARN_HIDDEN_LAYERS,
            max_iter=self.config.SKLEARN_MAX_ITER,
            learning_rate_init=self.config.SKLEARN_LR,
            early_stopping=True,
            validation_fraction=self.config.VAL_SIZE,  # ä¸BaselineBenchmarkä¸€è‡´
            n_iter_no_change=self.config.NN_PATIENCE,
            tol=self.config.NN_TOLERANCE,
            random_state=self.config.RANDOM_STATE
        )
        
        model.fit(data['X_train'], data['y_train'])
        
        if self.config.VERBOSE:
            print(f"   è®­ç»ƒå®Œæˆ: {model.n_iter_} epochs")
        
        return model
    
    def _train_pytorch_model(self, data):
        """è®­ç»ƒPyTorchæ¨¡å‹"""
        print("ğŸ”§ è®­ç»ƒ PyTorch MLPRegressor...")
        
        model = MLPPytorchRegressor(
            hidden_layer_sizes=self.config.PYTORCH_HIDDEN_SIZES,
            max_iter=self.config.PYTORCH_EPOCHS,
            learning_rate=self.config.PYTORCH_LR,
            early_stopping=True,
            validation_fraction=self.config.VAL_SIZE,  # ä¸BaselineBenchmarkä¸€è‡´
            n_iter_no_change=self.config.PYTORCH_PATIENCE,
            tol=self.config.NN_TOLERANCE,
            alpha=0.0,  # ç»Ÿä¸€æ— æ­£åˆ™åŒ–
            batch_size=None,  # å…¨æ‰¹æ¬¡è®­ç»ƒï¼Œä¸BaselineBenchmarkä¸€è‡´
            random_state=self.config.RANDOM_STATE,
            verbose=False
        )
        
        model.fit(data['X_train'], data['y_train'])
        
        print(f"   è®­ç»ƒå®Œæˆ: {model.n_iter_} epochs")
        return model
    
    def _train_robust_regressor(self, data, robust_type):
        """è®­ç»ƒç¨³å¥å›å½’å™¨"""
        print(f"ğŸ”§ è®­ç»ƒ {robust_type} Regressor...")
        
        # é€‰æ‹©ç¨³å¥å›å½’å™¨ç±»å‹
        if robust_type == 'huber':
            model_class = MLPHuberRegressor
        elif robust_type == 'pinball':
            model_class = MLPPinballRegressor
        elif robust_type == 'cauchy':
            model_class = MLPCauchyRegressor
        else:
            raise ValueError(f"Unknown robust type: {robust_type}")
        
        model = model_class(
            hidden_layer_sizes=self.config.ROBUST_HIDDEN_SIZES,
            max_iter=self.config.ROBUST_MAX_EPOCHS,
            learning_rate=self.config.ROBUST_LR,
            early_stopping=True,
            validation_fraction=self.config.VAL_SIZE,  # ä¸BaselineBenchmarkä¸€è‡´
            n_iter_no_change=self.config.ROBUST_PATIENCE,
            tol=self.config.NN_TOLERANCE,
            alpha=0.0,  # ç¨³å¥å›å½’å™¨é€šå¸¸ä¸éœ€è¦é¢å¤–æ­£åˆ™åŒ–
            batch_size=None,  # å…¨æ‰¹æ¬¡è®­ç»ƒï¼Œä¸BaselineBenchmarkä¸€è‡´
            random_state=self.config.RANDOM_STATE,
            verbose=False
        )
        
        model.fit(data['X_train'], data['y_train'])
        
        if hasattr(model, 'n_iter_'):
            print(f"   è®­ç»ƒå®Œæˆ: {model.n_iter_} epochs")
        else:
            print(f"   è®­ç»ƒå®Œæˆ")
        
        return model
    
    def _train_tree_model(self, data, tree_type):
        """è®­ç»ƒæ ‘æ¨¡å‹"""
        print(f"ğŸ”§ è®­ç»ƒ {tree_type}...")
        
        # ä½¿ç”¨åŸå§‹æ•°æ®ï¼ˆæœªæ ‡å‡†åŒ–çš„ç‰¹å¾ï¼‰è¿›è¡Œæ ‘æ¨¡å‹è®­ç»ƒ
        X_train_original = data['scaler_X'].inverse_transform(data['X_train'])
        y_train_original = data['scaler_y'].inverse_transform(data['y_train'].reshape(-1, 1)).flatten()
        
        if tree_type == 'random_forest':
            model = RandomForestRegressor(
                n_estimators=self.config.TREE_N_ESTIMATORS,
                max_depth=self.config.TREE_MAX_DEPTH,
                random_state=self.config.TREE_RANDOM_STATE,
                n_jobs=-1
            )
            
        elif tree_type == 'xgboost':
            if not XGBOOST_AVAILABLE:
                print("   âš ï¸ XGBoost ä¸å¯ç”¨ï¼Œè·³è¿‡...")
                return None
            model = xgb.XGBRegressor(
                n_estimators=self.config.XGBOOST_N_ESTIMATORS,
                learning_rate=self.config.XGBOOST_LEARNING_RATE,
                max_depth=self.config.XGBOOST_MAX_DEPTH,
                random_state=self.config.RANDOM_STATE,
                n_jobs=-1,
                verbosity=0
            )
            
        elif tree_type == 'lightgbm':
            if not LIGHTGBM_AVAILABLE:
                print("   âš ï¸ LightGBM ä¸å¯ç”¨ï¼Œè·³è¿‡...")
                return None
            model = lgb.LGBMRegressor(
                n_estimators=self.config.LIGHTGBM_N_ESTIMATORS,
                learning_rate=self.config.LIGHTGBM_LEARNING_RATE,
                max_depth=self.config.LIGHTGBM_MAX_DEPTH,
                random_state=self.config.RANDOM_STATE,
                n_jobs=-1,
                verbosity=-1
            )
            
        elif tree_type == 'catboost':
            if not CATBOOST_AVAILABLE:
                print("   âš ï¸ CatBoost ä¸å¯ç”¨ï¼Œè·³è¿‡...")
                return None
            model = cb.CatBoostRegressor(
                iterations=self.config.CATBOOST_ITERATIONS,
                learning_rate=self.config.CATBOOST_LEARNING_RATE,
                depth=self.config.CATBOOST_DEPTH,
                random_state=self.config.RANDOM_STATE,
                verbose=False
            )
            
        else:
            raise ValueError(f"Unknown tree type: {tree_type}")
        
        model.fit(X_train_original, y_train_original)
        print(f"   è®­ç»ƒå®Œæˆ")
        
        return model
    
    def _train_causal_model(self, data, mode):
        """è®­ç»ƒCausalEngineæ¨¡å‹"""
        print(f"ğŸ”§ è®­ç»ƒ CausalEngine ({mode})...")
        
        model = MLPCausalRegressor(
            perception_hidden_layers=self.config.CAUSAL_HIDDEN_SIZES,
            mode=mode,
            max_iter=self.config.CAUSAL_MAX_EPOCHS,
            learning_rate=self.config.CAUSAL_LR,
            early_stopping=True,
            validation_fraction=self.config.VAL_SIZE,  # ä¸BaselineBenchmarkä¸€è‡´
            n_iter_no_change=self.config.CAUSAL_PATIENCE,
            tol=self.config.CAUSAL_TOL,
            gamma_init=self.config.CAUSAL_GAMMA_INIT,
            b_noise_init=self.config.CAUSAL_B_NOISE_INIT,
            b_noise_trainable=self.config.CAUSAL_B_NOISE_TRAINABLE,
            alpha=0.0,  # ä¸BaselineBenchmarkä¸€è‡´çš„æ­£åˆ™åŒ–
            batch_size=None,  # å…¨æ‰¹æ¬¡è®­ç»ƒï¼Œä¸BaselineBenchmarkä¸€è‡´
            random_state=self.config.RANDOM_STATE,
            verbose=False
        )
        
        model.fit(data['X_train'], data['y_train'])
        
        if self.config.VERBOSE:
            print(f"   è®­ç»ƒå®Œæˆ: {model.n_iter_} epochs")
        
        return model
    
    def _evaluate_model(self, model, data, model_name):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        is_tree_model = model_name in ['random_forest', 'xgboost', 'lightgbm', 'catboost']
        
        if is_tree_model:
            # æ ‘æ¨¡å‹ä½¿ç”¨åŸå§‹ç‰¹å¾è¿›è¡Œé¢„æµ‹
            X_val_for_pred = data['scaler_X'].inverse_transform(data['X_val'])
            X_test_for_pred = data['scaler_X'].inverse_transform(data['X_test'])
            
            val_pred = model.predict(X_val_for_pred)
            test_pred = model.predict(X_test_for_pred)
            
            # è·å–éªŒè¯é›†çš„åŸå§‹ç›®æ ‡å€¼
            y_val_original = data['scaler_y'].inverse_transform(data['y_val'].reshape(-1, 1)).flatten()
        else:
            # ç¥ç»ç½‘ç»œæ¨¡å‹ä½¿ç”¨æ ‡å‡†åŒ–ç‰¹å¾è¿›è¡Œé¢„æµ‹
            val_pred_scaled = model.predict(data['X_val'])
            test_pred_scaled = model.predict(data['X_test'])
            
            # è½¬æ¢å›åŸå§‹å°ºåº¦
            val_pred = data['scaler_y'].inverse_transform(val_pred_scaled.reshape(-1, 1)).flatten()
            test_pred = data['scaler_y'].inverse_transform(test_pred_scaled.reshape(-1, 1)).flatten()
            y_val_original = data['scaler_y'].inverse_transform(data['y_val'].reshape(-1, 1)).flatten()
        
        # è®¡ç®—æŒ‡æ ‡
        val_metrics = {
            'MAE': mean_absolute_error(y_val_original, val_pred),
            'MdAE': median_absolute_error(y_val_original, val_pred),
            'RMSE': np.sqrt(mean_squared_error(y_val_original, val_pred)),
            'RÂ²': r2_score(y_val_original, val_pred)
        }
        
        test_metrics = {
            'MAE': mean_absolute_error(data['y_test'], test_pred),
            'MdAE': median_absolute_error(data['y_test'], test_pred),
            'RMSE': np.sqrt(mean_squared_error(data['y_test'], test_pred)),
            'RÂ²': r2_score(data['y_test'], test_pred)
        }
        
        return {'val': val_metrics, 'test': test_metrics}
    
    def run_comprehensive_benchmark(self, test_size=None, val_size=None, anomaly_ratio=None, verbose=None):
        """è¿è¡Œå…¨é¢çš„åŸºå‡†æµ‹è¯•"""
        # ä½¿ç”¨é…ç½®å‚æ•°ä½œä¸ºé»˜è®¤å€¼
        if test_size is None:
            test_size = self.config.TEST_SIZE
        if val_size is None:
            val_size = self.config.VAL_SIZE
        if anomaly_ratio is None:
            anomaly_ratio = self.config.ANOMALY_RATIO
        if verbose is None:
            verbose = self.config.VERBOSE
            
        if verbose:
            print("\\nğŸš€ å¼€å§‹ç»¼åˆåŸºå‡†æµ‹è¯• (Extended Sklearn-Style)")
            print("=" * 80)
            print(f"ğŸ”§ å®éªŒé…ç½®:")
            print(f"   - æµ‹è¯•é›†æ¯”ä¾‹: {test_size:.1%}")
            print(f"   - éªŒè¯é›†æ¯”ä¾‹: {val_size:.1%}")
            print(f"   - å¼‚å¸¸æ ‡ç­¾æ¯”ä¾‹: {anomaly_ratio:.1%}")
            print(f"   - éšæœºç§å­: {self.config.RANDOM_STATE}")
            print(f"   - CausalEngineæ¨¡å¼: {', '.join(self.config.CAUSAL_MODES)}")
            print(f"   - ç½‘ç»œæ¶æ„: {self.config.CAUSAL_HIDDEN_SIZES}")
            print(f"   - æœ€å¤§è®­ç»ƒè½®æ•°: {self.config.CAUSAL_MAX_EPOCHS}")
            print(f"   - æ—©åœpatience: {self.config.CAUSAL_PATIENCE}")
        
        # åŠ è½½æ•°æ®
        if self.X is None or self.y is None:
            self.load_and_explore_data(verbose=verbose)
        
        # å‡†å¤‡æ•°æ®
        data = self._prepare_data(test_size, val_size, anomaly_ratio, self.config.RANDOM_STATE)
        
        # è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹
        self.results = {}
        
        # 1. sklearnæ¨¡å‹
        sklearn_model = self._train_sklearn_model(data)
        self.results['sklearn_mlp'] = self._evaluate_model(sklearn_model, data, 'sklearn_mlp')
        
        # 2. PyTorchæ¨¡å‹
        pytorch_model = self._train_pytorch_model(data)
        self.results['pytorch_mlp'] = self._evaluate_model(pytorch_model, data, 'pytorch_mlp')
        
        # 3. ç¨³å¥å›å½’å™¨
        for robust_type in ['huber', 'pinball', 'cauchy']:
            robust_model = self._train_robust_regressor(data, robust_type)
            if robust_model is not None:
                # ä½¿ç”¨ä¸Legacyç‰ˆæœ¬ä¸€è‡´çš„é”®å
                result_key = f'mlp_{robust_type}_median' if robust_type == 'pinball' else f'mlp_{robust_type}'
                self.results[result_key] = self._evaluate_model(robust_model, data, result_key)
        
        # 4. æ ‘æ¨¡å‹
        for tree_type in ['random_forest', 'xgboost', 'lightgbm', 'catboost']:
            tree_model = self._train_tree_model(data, tree_type)
            if tree_model is not None:
                self.results[tree_type] = self._evaluate_model(tree_model, data, tree_type)
        
        # 5. CausalEngineæ¨¡å‹
        for mode in self.config.CAUSAL_MODES:
            causal_model = self._train_causal_model(data, mode)
            self.results[mode] = self._evaluate_model(causal_model, data, mode)
        
        if verbose:
            self._print_results(anomaly_ratio)
        
        return self.results
    
    def _print_results(self, anomaly_ratio):
        """æ‰“å°ç»“æœ"""
        print(f"\\nğŸ“Š åŸºå‡†æµ‹è¯•ç»“æœ (å¼‚å¸¸æ¯”ä¾‹: {anomaly_ratio:.0%})")
        print("=" * 140)
        print(f"{'æ–¹æ³•':<20} {'éªŒè¯é›†':<50} {'æµ‹è¯•é›†':<50}")
        print(f"{'':20} {'MAE':<10} {'MdAE':<10} {'RMSE':<10} {'RÂ²':<10} {'MAE':<10} {'MdAE':<10} {'RMSE':<10} {'RÂ²':<10}")
        print("-" * 140)
        
        for method, metrics in self.results.items():
            val_m = metrics['val']
            test_m = metrics['test']
            print(f"{method:<20} {val_m['MAE']:<10.4f} {val_m['MdAE']:<10.4f} {val_m['RMSE']:<10.4f} {val_m['RÂ²']:<10.4f} "
                  f"{test_m['MAE']:<10.4f} {test_m['MdAE']:<10.4f} {test_m['RMSE']:<10.4f} {test_m['RÂ²']:<10.4f}")
        
        print("=" * 140)
    
    def analyze_performance(self, verbose=True):
        """åˆ†ææ€§èƒ½ç»“æœ"""
        if not self.results:
            print("âŒ è¯·å…ˆè¿è¡ŒåŸºå‡†æµ‹è¯•")
            return
        
        if verbose:
            print("\\nğŸ” æ€§èƒ½åˆ†æ")
            print("=" * 60)
        
        # æå–æµ‹è¯•é›†RÂ²åˆ†æ•°
        test_r2_scores = {}
        for method, metrics in self.results.items():
            test_r2_scores[method] = metrics['test']['RÂ²']
        
        # æ‰¾åˆ°æœ€ä½³æ–¹æ³•
        best_method = max(test_r2_scores.keys(), key=lambda x: test_r2_scores[x])
        best_r2 = test_r2_scores[best_method]
        
        if verbose:
            print(f"ğŸ† æœ€ä½³æ–¹æ³•: {best_method}")
            print(f"   RÂ² = {best_r2:.4f}")
            print()
            print("ğŸ“Š æ€§èƒ½æ’å (æŒ‰RÂ²åˆ†æ•°):")
            
            sorted_methods = sorted(test_r2_scores.items(), key=lambda x: x[1], reverse=True)
            for i, (method, r2) in enumerate(sorted_methods, 1):
                improvement = ((r2 - sorted_methods[-1][1]) / abs(sorted_methods[-1][1])) * 100
                print(f"   {i}. {method:<15} RÂ² = {r2:.4f} (+ {improvement:+.1f}%)")
        
        # CausalEngineæ€§èƒ½åˆ†æ
        causal_methods = [m for m in self.results.keys() if m in ['deterministic', 'standard', 'sampling', 'exogenous', 'endogenous']]
        if causal_methods:
            best_causal = max(causal_methods, key=lambda x: test_r2_scores[x])
            traditional_methods = [m for m in self.results.keys() if m not in causal_methods]
            
            if traditional_methods and verbose:
                best_traditional = max(traditional_methods, key=lambda x: test_r2_scores[x])
                causal_improvement = ((test_r2_scores[best_causal] - test_r2_scores[best_traditional]) 
                                    / abs(test_r2_scores[best_traditional])) * 100
                
                print(f"\\nğŸ¯ CausalEngineä¼˜åŠ¿åˆ†æ:")
                print(f"   æœ€ä½³CausalEngineæ¨¡å¼: {best_causal} (RÂ² = {test_r2_scores[best_causal]:.4f})")
                print(f"   æœ€ä½³ä¼ ç»Ÿæ–¹æ³•: {best_traditional} (RÂ² = {test_r2_scores[best_traditional]:.4f})")
                print(f"   æ€§èƒ½æå‡: {causal_improvement:+.2f}%")
                
                if causal_improvement > 0:
                    print(f"   âœ… CausalEngineæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼")
                else:
                    print(f"   âš ï¸ åœ¨æ­¤æ•°æ®é›†ä¸Šä¼ ç»Ÿæ–¹æ³•è¡¨ç°æ›´å¥½")
        
        return test_r2_scores
    
    def create_performance_visualization(self, save_plot=None):
        """åˆ›å»ºæ€§èƒ½å¯è§†åŒ–å›¾è¡¨"""
        if save_plot is None:
            save_plot = self.config.SAVE_PLOTS
            
        if not self.results:
            print("âŒ è¯·å…ˆè¿è¡ŒåŸºå‡†æµ‹è¯•")
            return
        
        print("\\nğŸ“Š åˆ›å»ºæ€§èƒ½å¯è§†åŒ–å›¾è¡¨")
        print("-" * 30)
        
        # å‡†å¤‡æ•°æ®
        methods = list(self.results.keys())
        metrics = ['MAE', 'MdAE', 'RMSE', 'RÂ²']
        
        # åˆ›å»ºå­å›¾
        fig, axes = plt.subplots(2, 2, figsize=self.config.FIGURE_SIZE_PERFORMANCE)
        fig.suptitle('Extended California Housing Test Set Performance\\nNoise Level: 40.0%', 
                    fontsize=16, fontweight='bold')
        axes = axes.flatten()  # å±•å¹³ä¸ºä¸€ç»´æ•°ç»„ä¾¿äºè®¿é—®
        
        # è®¾ç½®é¢œè‰²æ–¹æ¡ˆ
        def get_method_color(method):
            if method in ['deterministic', 'exogenous', 'endogenous', 'standard']:
                return 'gold'  # CausalEngineç”¨é‡‘è‰²
            elif any(robust in method for robust in ['huber', 'pinball', 'cauchy']):
                return 'lightgreen'  # ç¨³å¥æ–¹æ³•ç”¨æµ…ç»¿
            elif method in ['random_forest', 'xgboost', 'lightgbm', 'catboost']:
                return 'sandybrown'  # æ ‘æ¨¡å‹ç”¨æ£•è‰²
            else:
                return 'lightblue'  # ç¥ç»ç½‘ç»œç”¨æµ…è“
        
        colors = [get_method_color(method) for method in methods]
        
        for i, metric in enumerate(metrics):
            values = [self.results[method]['test'][metric] for method in methods]
            
            bars = axes[i].bar(methods, values, color=colors, alpha=0.8, edgecolor='black')
            axes[i].set_title(f'{metric} (Test Set)', fontweight='bold')
            axes[i].set_ylabel(metric)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, value in zip(bars, values):
                height = bar.get_height()
                axes[i].text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                           f'{value:.3f}', ha='center', va='bottom', fontsize=8, fontweight='bold')
            
            # é«˜äº®æœ€ä½³ç»“æœ
            if metric == 'RÂ²':
                best_idx = values.index(max(values))
            else:
                best_idx = values.index(min(values))
            bars[best_idx].set_color('red')
            bars[best_idx].set_edgecolor('darkred')
            bars[best_idx].set_linewidth(3)
            
            axes[i].tick_params(axis='x', rotation=45)
            axes[i].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_plot:
            output_path = self._get_output_path('core_performance_comparison.png')
            plt.savefig(output_path, dpi=self.config.FIGURE_DPI, bbox_inches='tight')
            print(f"ğŸ“Š æ€§èƒ½å›¾è¡¨å·²ä¿å­˜ä¸º {output_path}")
        
        plt.close()  # å…³é—­å›¾å½¢ï¼Œé¿å…å†…å­˜æ³„æ¼
    
    def run_robustness_analysis(self, noise_levels=None, verbose=None):
        """è¿è¡Œé²æ£’æ€§åˆ†æ"""
        if noise_levels is None:
            noise_levels = self.config.ROBUSTNESS_ANOMALY_RATIOS
        if verbose is None:
            verbose = self.config.VERBOSE
            
        if verbose:
            print("\nğŸ”¬ å¼€å§‹é²æ£’æ€§åˆ†æ")
            print("=" * 60)
            print(f"ğŸ¯ æµ‹è¯•å™ªå£°æ°´å¹³: {[f'{level:.0%}' for level in noise_levels]}")
            print(f"   æ¯”è¾ƒæ–¹æ³•: é€‰å–ä¸»è¦æ–¹æ³•è¿›è¡Œé²æ£’æ€§æµ‹è¯•")
        
        robustness_results = {}
        
        for noise_level in noise_levels:
            if verbose:
                print(f"\nğŸ“Š æµ‹è¯•å™ªå£°æ°´å¹³: {noise_level:.0%}")
                print("-" * 30)
            
            # ä¸´æ—¶ä¿®æ”¹é…ç½®ä»¥æµ‹è¯•ç‰¹å®šå™ªå£°æ°´å¹³
            original_config = self.config.ANOMALY_RATIO
            self.config.ANOMALY_RATIO = noise_level
            
            # ä½¿ç”¨æ‰€æœ‰æ–¹æ³•è¿›è¡Œé²æ£’æ€§æµ‹è¯•ï¼ˆä¸æ ¸å¿ƒæ€§èƒ½æµ‹è¯•ä¸€è‡´ï¼‰
            try:
                # åŠ è½½æ•°æ®ï¼ˆå¦‚æœå°šæœªåŠ è½½ï¼‰
                if self.X is None or self.y is None:
                    self.load_and_explore_data(verbose=False)
                
                # å‡†å¤‡æ•°æ®
                data = self._prepare_data(
                    self.config.TEST_SIZE, 
                    self.config.VAL_SIZE, 
                    noise_level, 
                    self.config.RANDOM_STATE
                )
                
                noise_results = {}
                
                # 1. sklearnæ¨¡å‹
                sklearn_model = self._train_sklearn_model(data)
                noise_results['sklearn_mlp'] = self._evaluate_model(sklearn_model, data, 'sklearn_mlp')
                
                # 2. PyTorchæ¨¡å‹
                pytorch_model = self._train_pytorch_model(data)
                noise_results['pytorch_mlp'] = self._evaluate_model(pytorch_model, data, 'pytorch_mlp')
                
                # 3. ç¨³å¥å›å½’å™¨
                for robust_type in ['huber', 'pinball', 'cauchy']:
                    robust_model = self._train_robust_regressor(data, robust_type)
                    if robust_model is not None:
                        # ä½¿ç”¨ä¸æ ¸å¿ƒæµ‹è¯•ä¸€è‡´çš„é”®å
                        result_key = f'mlp_{robust_type}_median' if robust_type == 'pinball' else f'mlp_{robust_type}'
                        noise_results[result_key] = self._evaluate_model(robust_model, data, result_key)
                
                # 4. æ ‘æ¨¡å‹
                for tree_type in ['random_forest', 'xgboost', 'lightgbm', 'catboost']:
                    tree_model = self._train_tree_model(data, tree_type)
                    if tree_model is not None:
                        noise_results[tree_type] = self._evaluate_model(tree_model, data, tree_type)
                
                # 5. CausalEngineæ¨¡å‹
                for mode in self.config.CAUSAL_MODES:
                    causal_model = self._train_causal_model(data, mode)
                    noise_results[mode] = self._evaluate_model(causal_model, data, mode)
                
                robustness_results[noise_level] = noise_results
                
            finally:
                # æ¢å¤åŸå§‹é…ç½®
                self.config.ANOMALY_RATIO = original_config
        
        if verbose:
            self._print_robustness_results(robustness_results, noise_levels)
        
        return robustness_results
    
    def _print_robustness_results(self, robustness_results, noise_levels):
        """æ‰“å°é²æ£’æ€§åˆ†æç»“æœ"""
        print("\nğŸ“Š é²æ£’æ€§åˆ†æç»“æœ")
        print("=" * 100)
        
        methods = list(robustness_results[noise_levels[0]].keys())
        
        # æ‰“å°è¡¨å¤´
        header = f"{'æ–¹æ³•':<20}"
        for noise_level in noise_levels:
            header += f"{'å™ªå£°' + f'{noise_level:.0%}':<15}"
        print(header)
        print("-" * len(header))
        
        # æ‰“å°æ¯ä¸ªæ–¹æ³•çš„ç»“æœï¼ˆä½¿ç”¨RÂ²ä½œä¸ºä¸»è¦æŒ‡æ ‡ï¼‰
        for method in methods:
            row = f"{method:<20}"
            for noise_level in noise_levels:
                if method in robustness_results[noise_level]:
                    r2 = robustness_results[noise_level][method]['test']['RÂ²']
                    row += f"{r2:<15.4f}"
                else:
                    row += f"{'N/A':<15}"
            print(row)
        
        print("=" * 100)
        
        # åˆ†ææœ€ç¨³å®šçš„æ–¹æ³•
        print("\nğŸ¯ ç¨³å®šæ€§åˆ†æ:")
        stability_scores = {}
        
        for method in methods:
            r2_scores = []
            for noise_level in noise_levels:
                if method in robustness_results[noise_level]:
                    r2_scores.append(robustness_results[noise_level][method]['test']['RÂ²'])
            
            if r2_scores:
                # è®¡ç®—æ ‡å‡†å·®ä½œä¸ºç¨³å®šæ€§æŒ‡æ ‡ï¼ˆè¶Šå°è¶Šç¨³å®šï¼‰
                stability_scores[method] = np.std(r2_scores)
        
        # æ‰¾åˆ°æœ€ç¨³å®šçš„æ–¹æ³•
        most_stable = min(stability_scores.keys(), key=lambda x: stability_scores[x])
        print(f"   æœ€ç¨³å®šæ–¹æ³•: {most_stable} (æ ‡å‡†å·®: {stability_scores[most_stable]:.4f})")
        
        # æŒ‰ç¨³å®šæ€§æ’åº
        print("   ç¨³å®šæ€§æ’å:")
        sorted_stability = sorted(stability_scores.items(), key=lambda x: x[1])
        for i, (method, std) in enumerate(sorted_stability, 1):
            print(f"   {i}. {method:<15} æ ‡å‡†å·®: {std:.4f}")
    
    def create_robustness_visualization(self, robustness_results, save_plot=None):
        """åˆ›å»ºé²æ£’æ€§å¯è§†åŒ–å›¾è¡¨ - 4ä¸ªæŒ‡æ ‡çš„2x2å­å›¾å¸ƒå±€"""
        if save_plot is None:
            save_plot = self.config.SAVE_PLOTS
            
        print("\nğŸ“Š åˆ›å»ºé²æ£’æ€§å¯è§†åŒ–å›¾è¡¨")
        print("-" * 30)
        
        noise_levels = list(robustness_results.keys())
        methods = list(robustness_results[noise_levels[0]].keys())
        
        # åˆ›å»º2x2å­å›¾å¸ƒå±€
        fig, axes = plt.subplots(2, 2, figsize=self.config.FIGURE_SIZE_ROBUSTNESS)
        fig.suptitle('Extended Robustness Analysis: Performance vs Noise Level', fontsize=16, fontweight='bold')
        
        # 4ä¸ªå›å½’æŒ‡æ ‡
        metrics = ['MAE', 'MdAE', 'RMSE', 'RÂ²']
        metric_labels = ['Mean Absolute Error (MAE)', 'Median Absolute Error (MdAE)', 'Root Mean Squared Error (RMSE)', 'R-squared Score (RÂ²)']
        
        # è®¾ç½®é¢œè‰²å’Œçº¿å‹
        method_styles = {}
        causal_methods = [m for m in methods if m in ['deterministic', 'exogenous', 'endogenous', 'standard']]
        
        colors = ['#1f77b4', '#ff7f0e', '#d62728', '#2ca02c', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']
        markers = ['o', 's', 'v', '^', 'D', 'P', 'X', 'h', '+', '*']
        
        for i, method in enumerate(methods):
            if method in causal_methods:
                method_styles[method] = {'color': '#d62728', 'linestyle': '-', 'linewidth': 3, 'marker': 'o', 'markersize': 8}
            else:
                method_styles[method] = {'color': colors[i % len(colors)], 'linestyle': '--', 'linewidth': 2, 'marker': markers[i % len(markers)], 'markersize': 6}
        
        # ä¸ºæ¯ä¸ªæŒ‡æ ‡åˆ›å»ºå­å›¾
        for idx, (metric, metric_label) in enumerate(zip(metrics, metric_labels)):
            row, col = idx // 2, idx % 2
            ax = axes[row, col]
            
            # ä¸ºæ¯ä¸ªæ–¹æ³•ç»˜åˆ¶çº¿å›¾
            for method in methods:
                scores = []
                valid_noise_levels = []
                
                for noise_level in noise_levels:
                    if method in robustness_results[noise_level]:
                        scores.append(robustness_results[noise_level][method]['test'][metric])
                        valid_noise_levels.append(noise_level * 100)  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
                
                if scores:
                    ax.plot(valid_noise_levels, scores, 
                           label=method, 
                           **method_styles[method])
            
            ax.set_xlabel('Label Noise Ratio (%)', fontsize=11)
            ax.set_ylabel(metric, fontsize=11)
            ax.set_title(metric_label, fontsize=12, fontweight='bold')
            ax.legend(fontsize=9)
            ax.grid(True, alpha=0.3)
            
            # ä¸ºRÂ²æ·»åŠ ç‰¹æ®Šå¤„ç†ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰ï¼Œå…¶ä»–æŒ‡æ ‡è¶Šä½è¶Šå¥½
            if metric == 'RÂ²':
                ax.set_ylim(bottom=0)  # RÂ²ä»0å¼€å§‹æ˜¾ç¤º
            else:
                ax.set_ylim(bottom=0)  # è¯¯å·®æŒ‡æ ‡ä»0å¼€å§‹æ˜¾ç¤º
        
        plt.tight_layout()
        
        if save_plot:
            output_path = self._get_output_path('extended_robustness_analysis.png')
            plt.savefig(output_path, dpi=self.config.FIGURE_DPI, bbox_inches='tight')
            print(f"ğŸ“Š é²æ£’æ€§å›¾è¡¨å·²ä¿å­˜ä¸º {output_path}")
        
        plt.close()
    
    def generate_summary_report(self):
        """ç”Ÿæˆå®éªŒæ€»ç»“æŠ¥å‘Š"""
        if self.config.VERBOSE:
            print("\\nğŸ“‹ ç”Ÿæˆå®éªŒæ€»ç»“æŠ¥å‘Š...")
        
        report_lines = []
        report_lines.append("# æ‰©å±•ç‰ˆåŠ å·æˆ¿ä»·å›å½’å®éªŒæ€»ç»“æŠ¥å‘Š (Sklearn-Style)")
        report_lines.append("")
        report_lines.append("ğŸ  **California Housing Dataset Regression Analysis - Sklearn-Style Implementation**")
        report_lines.append("")
        report_lines.append("---")
        report_lines.append("")
        
        # å®éªŒé…ç½®
        report_lines.append("## ğŸ“Š å®éªŒé…ç½®")
        report_lines.append("")
        report_lines.append(f"- **æ•°æ®é›†**: åŠ å·æˆ¿ä»·æ•°æ®é›†")
        report_lines.append(f"  - æ ·æœ¬æ•°: {self.X.shape[0]:,}")
        report_lines.append(f"  - ç‰¹å¾æ•°: {self.X.shape[1]}")
        report_lines.append(f"  - æˆ¿ä»·èŒƒå›´: ${self.y.min():.2f} - ${self.y.max():.2f} (10ä¸‡ç¾å…ƒ)")
        report_lines.append("")
        report_lines.append(f"- **æ•°æ®åˆ†å‰²**:")
        report_lines.append(f"  - æµ‹è¯•é›†æ¯”ä¾‹: {self.config.TEST_SIZE:.1%}")
        report_lines.append(f"  - éªŒè¯é›†æ¯”ä¾‹: {self.config.VAL_SIZE:.1%}")
        report_lines.append(f"  - éšæœºç§å­: {self.config.RANDOM_STATE}")
        report_lines.append("")
        report_lines.append(f"- **ç¥ç»ç½‘ç»œç»Ÿä¸€é…ç½®**:")
        report_lines.append(f"  - ç½‘ç»œç»“æ„: {self.config.NN_HIDDEN_SIZES}")
        report_lines.append(f"  - æœ€å¤§è½®æ•°: {self.config.NN_MAX_EPOCHS}")
        report_lines.append(f"  - å­¦ä¹ ç‡: {self.config.NN_LEARNING_RATE}")
        report_lines.append(f"  - æ—©åœpatience: {self.config.NN_PATIENCE}")
        report_lines.append("")
        report_lines.append(f"- **å®éªŒæ–¹æ³•**: {len(self.config.BASELINE_METHODS) + len(self.config.CAUSAL_MODES)} ç§")
        report_lines.append(f"  - ä¼ ç»Ÿæ–¹æ³• ({len(self.config.BASELINE_METHODS)}ç§): {', '.join(self.config.BASELINE_METHODS)}")
        report_lines.append(f"  - CausalEngine ({len(self.config.CAUSAL_MODES)}ç§): {', '.join(self.config.CAUSAL_MODES)}")
        report_lines.append("")
        
        # æ ¸å¿ƒæ€§èƒ½æµ‹è¯•ç»“æœ
        if self.results:
            results = self.results
            report_lines.append("## ğŸ¯ æ ¸å¿ƒæ€§èƒ½æµ‹è¯•ç»“æœ")
            report_lines.append("")
            report_lines.append(f"**å™ªå£°æ°´å¹³**: {self.config.ANOMALY_RATIO:.1%}")
            report_lines.append("")
            
            # åˆ›å»ºæ€§èƒ½è¡¨æ ¼ - æŒ‰MdAEæ’åº
            methods_by_mdae = sorted(results.keys(), key=lambda x: results[x]['test']['MdAE'])
            
            report_lines.append("### ğŸ“ˆ æµ‹è¯•é›†æ€§èƒ½æ’å (æŒ‰MdAEå‡åº)")
            report_lines.append("")
            
            # è¡¨æ ¼å¤´
            report_lines.append("| æ’å | æ–¹æ³• | MAE | MdAE | RMSE | RÂ² | æ–¹æ³•ç±»å‹ |")
            report_lines.append("|:----:|------|----:|-----:|-----:|---:|----------|")
            
            for i, method in enumerate(methods_by_mdae, 1):
                test_metrics = results[method]['test']
                
                # åˆ¤æ–­æ–¹æ³•ç±»å‹
                if any(mode in method for mode in ['deterministic', 'standard', 'exogenous', 'endogenous']):
                    method_type = "ğŸ¤– CausalEngine"
                elif any(robust in method.lower() for robust in ['huber', 'cauchy', 'pinball']):
                    method_type = "ğŸ›¡ï¸ ç¨³å¥å›å½’"
                elif method.lower() in ['catboost', 'random_forest', 'xgboost', 'lightgbm']:
                    method_type = "ğŸŒ² é›†æˆå­¦ä¹ "
                else:
                    method_type = "ğŸ§  ç¥ç»ç½‘ç»œ"
                
                report_lines.append(f"| {i} | **{method}** | "
                                  f"{test_metrics['MAE']:.4f} | "
                                  f"**{test_metrics['MdAE']:.4f}** | "
                                  f"{test_metrics['RMSE']:.4f} | "
                                  f"{test_metrics['RÂ²']:.4f} | "
                                  f"{method_type} |")
            
            report_lines.append("")
            
            # éªŒè¯é›†vsæµ‹è¯•é›†å¯¹æ¯”ï¼ˆå±•ç¤ºå™ªå£°å½±å“ï¼‰
            report_lines.append("### ğŸ” éªŒè¯é›† vs æµ‹è¯•é›†æ€§èƒ½å¯¹æ¯”")
            report_lines.append("")
            report_lines.append("*éªŒè¯é›†åŒ…å«å™ªå£°ï¼Œæµ‹è¯•é›†ä¸ºçº¯å‡€æ•°æ®*")
            report_lines.append("")
            
            report_lines.append("| æ–¹æ³• | éªŒè¯é›†MdAE | æµ‹è¯•é›†MdAE | æ€§èƒ½æå‡ |")
            report_lines.append("|------|----------:|----------:|--------:|")
            
            for method in methods_by_mdae:
                val_mdae = results[method]['val']['MdAE']
                test_mdae = results[method]['test']['MdAE']
                improvement = ((val_mdae - test_mdae) / val_mdae) * 100
                
                report_lines.append(f"| {method} | "
                                  f"{val_mdae:.4f} | "
                                  f"{test_mdae:.4f} | "
                                  f"{improvement:+.1f}% |")
            
            report_lines.append("")
            
            # å…³é”®å‘ç°
            best_mdae_method = methods_by_mdae[0]
            best_mdae_score = results[best_mdae_method]['test']['MdAE']
            
            # è¯†åˆ«CausalEngineæ–¹æ³•
            causal_methods = [m for m in results.keys() if any(mode in m for mode in ['deterministic', 'standard', 'exogenous', 'endogenous'])]
            
            report_lines.append("### ğŸ† å…³é”®å‘ç°")
            report_lines.append("")
            report_lines.append(f"- **ğŸ¥‡ æœ€ä½³æ•´ä½“æ€§èƒ½**: `{best_mdae_method}` (MdAE: {best_mdae_score:.4f})")
            
            if causal_methods:
                best_causal = min(causal_methods, key=lambda x: results[x]['test']['MdAE'])
                causal_rank = methods_by_mdae.index(best_causal) + 1
                causal_score = results[best_causal]['test']['MdAE']
                report_lines.append(f"- **ğŸ¤– æœ€ä½³CausalEngine**: `{best_causal}` (æ’å: {causal_rank}/{len(methods_by_mdae)}, MdAE: {causal_score:.4f})")
                
                # CausalEngineæ¨¡å¼å¯¹æ¯”
                if len(causal_methods) > 1:
                    report_lines.append("")
                    report_lines.append("**CausalEngineæ¨¡å¼å¯¹æ¯”**:")
                    for causal_method in sorted(causal_methods, key=lambda x: results[x]['test']['MdAE']):
                        rank = methods_by_mdae.index(causal_method) + 1
                        score = results[causal_method]['test']['MdAE']
                        report_lines.append(f"  - `{causal_method}`: æ’å {rank}, MdAE {score:.4f}")
            
            # ä¼ ç»Ÿæ–¹æ³•åˆ†æ
            traditional_methods = [m for m in results.keys() if m not in causal_methods]
            if traditional_methods:
                best_traditional = min(traditional_methods, key=lambda x: results[x]['test']['MdAE'])
                traditional_rank = methods_by_mdae.index(best_traditional) + 1
                traditional_score = results[best_traditional]['test']['MdAE']
                report_lines.append(f"- **ğŸ… æœ€ä½³ä¼ ç»Ÿæ–¹æ³•**: `{best_traditional}` (æ’å: {traditional_rank}/{len(methods_by_mdae)}, MdAE: {traditional_score:.4f})")
            
            report_lines.append("")
        
        # æ·»åŠ è„šæ³¨
        report_lines.append("---")
        report_lines.append("")
        report_lines.append("## ğŸ“ è¯´æ˜")
        report_lines.append("")
        report_lines.append("- **MdAE**: Median Absolute Error (ä¸­ä½æ•°ç»å¯¹è¯¯å·®) - ä¸»è¦è¯„ä¼°æŒ‡æ ‡")
        report_lines.append("- **MAE**: Mean Absolute Error (å¹³å‡ç»å¯¹è¯¯å·®)")
        report_lines.append("- **RMSE**: Root Mean Square Error (å‡æ–¹æ ¹è¯¯å·®)")
        report_lines.append("- **RÂ²**: å†³å®šç³»æ•° (è¶Šæ¥è¿‘1è¶Šå¥½)")
        report_lines.append("- **å™ªå£°è®¾ç½®**: éªŒè¯é›†åŒ…å«äººå·¥å™ªå£°ï¼Œæµ‹è¯•é›†ä¸ºçº¯å‡€æ•°æ®")
        report_lines.append("- **ç»Ÿä¸€é…ç½®**: æ‰€æœ‰ç¥ç»ç½‘ç»œæ–¹æ³•ä½¿ç”¨ç›¸åŒçš„è¶…å‚æ•°ç¡®ä¿å…¬å¹³æ¯”è¾ƒ")
        report_lines.append("- **å®ç°æ–¹å¼**: ä½¿ç”¨sklearn-style regressorå®ç°")
        report_lines.append("")
        report_lines.append(f"ğŸ“Š **ç”Ÿæˆæ—¶é—´**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = self._get_output_path('extended_experiment_summary.md')
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write('\\n'.join(report_lines))
        
        if self.config.VERBOSE:
            print(f"ğŸ“‹ å®éªŒæ€»ç»“æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        
        return report_lines


def main():
    """ä¸»å‡½æ•° - è¿è¡Œæ‰©å±•ç‰ˆæ•™ç¨‹ (Sklearn-Styleç‰ˆæœ¬)"""
    print("ğŸš€ æ‰©å±•ç‰ˆåŠ å·æˆ¿ä»·å›å½’æ•™ç¨‹")
    print("=" * 60)
    
    # åˆ›å»ºæ•™ç¨‹å®ä¾‹
    tutorial = ExtendedCaliforniaHousingTutorialSklearnStyle()
    
    # 1. åŠ è½½å’Œåˆ†ææ•°æ®
    tutorial.load_and_explore_data()
    
    # 2. åˆ›å»ºæ•°æ®åˆ†æå¯è§†åŒ–
    tutorial.visualize_data()
    
    # 3. è¿è¡Œæ ¸å¿ƒæ€§èƒ½æµ‹è¯•
    core_results = tutorial.run_comprehensive_benchmark()
    
    # 4. åˆ†ææ€§èƒ½ç»“æœ
    tutorial.analyze_performance()
    
    # 5. åˆ›å»ºæ€§èƒ½å¯¹æ¯”å¯è§†åŒ–
    tutorial.create_performance_visualization()
    
    # 6. è¿è¡Œé²æ£’æ€§æµ‹è¯•
    if tutorial.config.RUN_ROBUSTNESS_TEST:
        robustness_results = tutorial.run_robustness_analysis()
        
        # åˆ›å»ºé²æ£’æ€§å¯è§†åŒ–
        tutorial.create_robustness_visualization(robustness_results)
    
    # 7. ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
    tutorial.generate_summary_report()
    
    if tutorial.config.VERBOSE:
        print("\nğŸ‰ æ‰©å±•ç‰ˆæ•™ç¨‹è¿è¡Œå®Œæˆï¼")
        print(f"ğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {tutorial.config.OUTPUT_DIR}")
        print("\nä¸»è¦è¾“å‡ºæ–‡ä»¶:")
        print("- extended_data_analysis.png: æ•°æ®åˆ†æå›¾è¡¨")
        print("- core_performance_comparison.png: æ ¸å¿ƒæ€§èƒ½å¯¹æ¯”å›¾è¡¨")
        if tutorial.config.RUN_ROBUSTNESS_TEST:
            print("- extended_robustness_analysis.png: é²æ£’æ€§åˆ†æå›¾è¡¨")
        print("- extended_experiment_summary.md: å®éªŒæ€»ç»“æŠ¥å‘Š")


if __name__ == "__main__":
    main()