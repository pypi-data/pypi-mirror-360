#!/usr/bin/env python3
"""
ğŸ  å…¨é¢CausalEngineæ¨¡å¼æ•™ç¨‹ï¼šåŠ å·æˆ¿ä»·é¢„æµ‹ - Sklearn-Styleç‰ˆæœ¬
================================================================

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                    Sklearn-Style CausalEngine å®éªŒæµç¨‹æ¶æ„å›¾                                        â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                                                                    â•‘
â•‘  ğŸ“Š æ•°æ®å¤„ç†ç®¡é“ (Data Processing Pipeline)                                                                         â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘
â•‘  â”‚                                                                                                                â”‚  â•‘
â•‘  â”‚  1. åŸå§‹æ•°æ®åŠ è½½ â†’ 2. æ•°æ®åˆ†å‰² â†’ 3. æ ‡å‡†åŒ–(ä»…è®­ç»ƒé›†fit) â†’ 4. å™ªå£°æ³¨å…¥(ä»…è®­ç»ƒé›†) â†’ 5. æ¨¡å‹è®­ç»ƒ                      â”‚  â•‘
â•‘  â”‚                                                                                                                â”‚  â•‘
â•‘  â”‚  California Housing    Train/Test     X & Y         40% Label     13ç§æ–¹æ³•                                      â”‚  â•‘
â•‘  â”‚  (20,640 samples)  â†’   Split      â†’   Scaling   â†’   Noise     â†’   è®­ç»ƒå¯¹æ¯”                                       â”‚  â•‘
â•‘  â”‚  8 features            (80/20)       (åŸºäºå¹²å‡€æ•°æ®)   (è®­ç»ƒé›†Only)   Performance                                  â”‚  â•‘
â•‘  â”‚                                      ğŸ“Œæ— æ•°æ®æ³„éœ²    ğŸ“Œæµ‹è¯•é›†ä¿æŒçº¯å‡€                                             â”‚  â•‘
â•‘  â”‚                                                                                                                â”‚  â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘
â•‘                                                                                                                    â•‘
â•‘  ğŸ”§ ç»Ÿä¸€å‚æ•°ä¼ é€’æœºåˆ¶ (Unified Parameter Passing)                                                                     â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘
â•‘  â”‚                                                                                                                â”‚  â•‘
â•‘  â”‚  SklearnStyleTutorialConfig                                                                                    â”‚  â•‘
â•‘  â”‚  â”œâ”€ ğŸ§  ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®                                                                                          â”‚  â•‘
â•‘  â”‚  â”‚   â”œâ”€ NN_HIDDEN_SIZES = (128, 64, 32)     # æ‰€æœ‰ç¥ç»ç½‘ç»œä½¿ç”¨ç›¸åŒæ¶æ„                                          â”‚  â•‘
â•‘  â”‚  â”‚   â”œâ”€ NN_MAX_EPOCHS = 3000                # ç»Ÿä¸€æœ€å¤§è®­ç»ƒè½®æ•°                                                  â”‚  â•‘
â•‘  â”‚  â”‚   â”œâ”€ NN_LEARNING_RATE = 0.01             # ç»Ÿä¸€å­¦ä¹ ç‡                                                      â”‚  â•‘
â•‘  â”‚  â”‚   â”œâ”€ NN_PATIENCE = 200                   # ç»Ÿä¸€æ—©åœpatience                                                â”‚  â•‘
â•‘  â”‚  â”‚   â””â”€ NN_BATCH_SIZE = 200                 # ç»Ÿä¸€æ‰¹å¤„ç†å¤§å°                                                   â”‚  â•‘
â•‘  â”‚  â”‚                                                                                                            â”‚  â•‘
â•‘  â”‚  â”œâ”€ ğŸ¯ æ–¹æ³•ç‰¹å®šå‚æ•°                                                                                              â”‚  â•‘
â•‘  â”‚  â”‚   â”œâ”€ CausalEngine: gamma_init, b_noise_init, modes=['deterministic', 'exogenous', 'endogenous', 'standard'] â”‚  â•‘
â•‘  â”‚  â”‚   â”œâ”€ Robust MLP: delta (Huber), quantile (Pinball), Cauchy loss                                           â”‚  â•‘
â•‘  â”‚  â”‚   â””â”€ Tree Methods: n_estimators, max_depth, learning_rate                                                  â”‚  â•‘
â•‘  â”‚  â”‚                                                                                                            â”‚  â•‘
â•‘  â”‚  â””â”€ ğŸ“Š å®éªŒæ§åˆ¶å‚æ•°                                                                                              â”‚  â•‘
â•‘  â”‚      â”œâ”€ ANOMALY_RATIO = 0.4                 # 40%æ ‡ç­¾å™ªå£°                                                      â”‚  â•‘
â•‘  â”‚      â”œâ”€ TEST_SIZE = 0.2                     # æµ‹è¯•é›†æ¯”ä¾‹                                                       â”‚  â•‘
â•‘  â”‚      â””â”€ RANDOM_STATE = 42                   # éšæœºç§å­                                                        â”‚  â•‘
â•‘  â”‚                                                                                                                â”‚  â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘
â•‘                                                                                                                    â•‘
â•‘  ğŸ”„ 13ç§æ–¹æ³•å¯¹æ¯”æ¶æ„ (13-Method Comparison Framework)                                                               â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘
â•‘  â”‚                                                                                                                â”‚  â•‘
â•‘  â”‚  ä¼ ç»Ÿç¥ç»ç½‘ç»œ (2ç§)          ç¨³å¥å›å½’ (3ç§)              æ ‘æ¨¡å‹ (4ç§)              å› æœæ¨ç† (4ç§)                    â”‚  â•‘
â•‘  â”‚  â”œâ”€ sklearn MLP             â”œâ”€ Huber MLP              â”œâ”€ Random Forest          â”œâ”€ deterministic              â”‚  â•‘
â•‘  â”‚  â””â”€ PyTorch MLP             â”œâ”€ Pinball MLP            â”œâ”€ XGBoost                â”œâ”€ exogenous                  â”‚  â•‘
â•‘  â”‚                             â””â”€ Cauchy MLP             â”œâ”€ LightGBM               â”œâ”€ endogenous                 â”‚  â•‘
â•‘  â”‚                                                       â””â”€ CatBoost               â””â”€ standard                   â”‚  â•‘
â•‘  â”‚                                                                                                                â”‚  â•‘
â•‘  â”‚  ğŸ’¡ å…³é”®è®¾è®¡äº®ç‚¹:                                                                                                â”‚  â•‘
â•‘  â”‚  â€¢ ç§‘å­¦å®éªŒè®¾è®¡: å…ˆåŸºäºå¹²å‡€æ•°æ®æ ‡å‡†åŒ–ï¼Œå†åœ¨è®­ç»ƒé›†æ³¨å…¥40%å™ªå£°ï¼Œæµ‹è¯•é›†ä¿æŒçº¯å‡€                                      â”‚  â•‘
â•‘  â”‚  â€¢ æ— æ•°æ®æ³„éœ²: æ ‡å‡†åŒ–å™¨åªåœ¨å¹²å‡€è®­ç»ƒæ•°æ®ä¸Šfitï¼Œé¿å…å™ªå£°æ±¡æŸ“ç»Ÿè®¡é‡                                                  â”‚  â•‘
â•‘  â”‚  â€¢ ç»Ÿä¸€æ•°æ®æ ‡å‡†åŒ–ç­–ç•¥: æ‰€æœ‰æ–¹æ³•åœ¨æ ‡å‡†åŒ–ç©ºé—´è®­ç»ƒï¼Œç¡®ä¿å®éªŒå…¬å¹³æ€§                                                     â”‚  â•‘
â•‘  â”‚  â€¢ å‚æ•°å…¬å¹³æ€§: æ‰€æœ‰ç¥ç»ç½‘ç»œä½¿ç”¨ç»Ÿä¸€é…ç½®ï¼Œç¡®ä¿å…¬å¹³æ¯”è¾ƒ                                                             â”‚  â•‘
â•‘  â”‚  â€¢ è¯„ä¼°ä¸€è‡´æ€§: æ‰€æœ‰æ–¹æ³•åœ¨åŸå§‹å°ºåº¦ä¸‹è¯„ä¼°ï¼Œä¾¿äºç»“æœè§£é‡Š                                                             â”‚  â•‘
â•‘  â”‚                                                                                                                â”‚  â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘
â•‘                                                                                                                    â•‘
â•‘  ğŸ“ˆ è¾“å‡ºåˆ†æ (Analysis & Visualization)                                                                             â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘
â•‘  â”‚                                                                                                                â”‚  â•‘
â•‘  â”‚  1. æ•°æ®æ¢ç´¢åˆ†æå›¾    2. æ ‡å‡†ç‰ˆæ€§èƒ½å¯¹æ¯”    3. æ‰©å±•ç‰ˆæ€§èƒ½å¯¹æ¯”    4. CausalEngineä¸“é¡¹å¯¹æ¯”                             â”‚  â•‘
â•‘  â”‚     (ç‰¹å¾åˆ†å¸ƒ)           (9ç§æ ¸å¿ƒæ–¹æ³•)       (13ç§å…¨éƒ¨æ–¹æ³•)       (4ç§æ¨¡å¼è¯¦ç»†)                                    â”‚  â•‘
â•‘  â”‚                                                                                                                â”‚  â•‘
â•‘  â”‚  ğŸ“Š è¯„ä¼°æŒ‡æ ‡: MAE, MdAE, RMSE, RÂ² (æ‰€æœ‰æ–¹æ³•åœ¨åŸå§‹æˆ¿ä»·å°ºåº¦ä¸‹ç»Ÿä¸€è¯„ä¼°)                                               â”‚  â•‘
â•‘  â”‚                                                                                                                â”‚  â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

è¿™ä¸ªæ•™ç¨‹æ¼”ç¤ºæ‰€æœ‰CausalEngineæ¨ç†æ¨¡å¼åœ¨çœŸå®ä¸–ç•Œå›å½’ä»»åŠ¡ä¸­çš„æ€§èƒ½è¡¨ç°ï¼Œä½¿ç”¨sklearn-styleå®ç°ã€‚

æ•°æ®é›†ï¼šåŠ å·æˆ¿ä»·æ•°æ®é›†ï¼ˆCalifornia Housing Datasetï¼‰
- 20,640ä¸ªæ ·æœ¬
- 8ä¸ªç‰¹å¾ï¼ˆæˆ¿å±‹å¹´é¾„ã€æ”¶å…¥ã€äººå£ç­‰ï¼‰
- ç›®æ ‡ï¼šé¢„æµ‹æˆ¿ä»·ä¸­ä½æ•°

æˆ‘ä»¬å°†æ¯”è¾ƒæ‰€æœ‰æ–¹æ³•ï¼š
**æ ‡å‡†ç‰ˆæ¯”è¾ƒå›¾ï¼ˆ9ç§æ ¸å¿ƒæ–¹æ³•ï¼‰ï¼š**
1. sklearn MLPRegressorï¼ˆä¼ ç»Ÿç¥ç»ç½‘ç»œï¼‰
2. PyTorch MLPï¼ˆä¼ ç»Ÿæ·±åº¦å­¦ä¹ ï¼‰
3. Random Forestï¼ˆéšæœºæ£®æ—ï¼‰
4. XGBoostï¼ˆæ¢¯åº¦æå‡ï¼‰
5. LightGBMï¼ˆè½»é‡æ¢¯åº¦æå‡ï¼‰
6. CatBoostï¼ˆå¼ºåŠ›æ¢¯åº¦æå‡ï¼‰
7. CausalEngine - exogenousï¼ˆå¤–ç”Ÿå™ªå£°ä¸»å¯¼ï¼‰
8. CausalEngine - endogenousï¼ˆå†…ç”Ÿä¸ç¡®å®šæ€§ä¸»å¯¼ï¼‰
9. CausalEngine - standardï¼ˆå†…ç”Ÿ+å¤–ç”Ÿæ··åˆï¼‰

**æ‰©å±•ç‰ˆæ¯”è¾ƒå›¾ï¼ˆåŒ…å«æ‰€æœ‰13ç§æ–¹æ³•ï¼‰ï¼š**
- ä¸Šè¿°9ç§æ ¸å¿ƒæ–¹æ³• + 4ç§é¢å¤–æ–¹æ³•ï¼š
10. CausalEngine - deterministicï¼ˆç¡®å®šæ€§æ¨ç†ï¼‰
11. MLP Huberï¼ˆHuberæŸå¤±ç¨³å¥å›å½’ï¼‰
12. MLP Pinball Medianï¼ˆä¸­ä½æ•°å›å½’ï¼‰
13. MLP Cauchyï¼ˆCauchyæŸå¤±ç¨³å¥å›å½’ï¼‰

å…³é”®äº®ç‚¹ï¼š
- 4ç§CausalEngineæ¨ç†æ¨¡å¼çš„å…¨é¢å¯¹æ¯”
- 9ç§å¼ºåŠ›ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•ï¼ˆåŒ…å«2ç§ç¥ç»ç½‘ç»œ+3ç§æ¢¯åº¦æå‡+1ç§éšæœºæ£®æ—+3ç§ç¨³å¥å›å½’ï¼‰
- çœŸå®ä¸–ç•Œæ•°æ®çš„é²æ£’æ€§æµ‹è¯•
- å› æœæ¨ç†vsä¼ ç»Ÿæ–¹æ³•çš„æ€§èƒ½å·®å¼‚åˆ†æ
- æ ‡å‡†ç‰ˆ(9ç§æ ¸å¿ƒ)ä¸æ‰©å±•ç‰ˆ(13ç§å…¨éƒ¨)åŒé‡å¯è§†åŒ–
- ä½¿ç”¨sklearn-style regressorå®ç°ï¼Œä¸Legacyç‰ˆæœ¬å½¢æˆå¯¹æ¯”

å®éªŒè®¾è®¡è¯´æ˜
==================================================================
æœ¬è„šæœ¬ä½¿ç”¨sklearn-styleå®ç°ä¸“æ³¨äºå…¨é¢è¯„ä¼°CausalEngineçš„4ç§æ¨ç†æ¨¡å¼ï¼Œ
æ—¨åœ¨æ­ç¤ºä¸åŒå› æœæ¨ç†ç­–ç•¥åœ¨çœŸå®å›å½’ä»»åŠ¡ä¸Šçš„æ€§èƒ½ç‰¹ç‚¹å’Œé€‚ç”¨åœºæ™¯ã€‚

æ ¸å¿ƒå®éªŒï¼šå…¨æ¨¡å¼æ€§èƒ½å¯¹æ¯” (åœ¨40%æ ‡ç­¾å™ªå£°ä¸‹)
--------------------------------------------------
- **ç›®æ ‡**: æ¯”è¾ƒæ‰€æœ‰4ç§CausalEngineæ¨¡å¼å’Œ9ç§ä¼ ç»Ÿæ–¹æ³•çš„é¢„æµ‹æ€§èƒ½ï¼ˆæ ‡å‡†ç‰ˆ9ç§æ ¸å¿ƒæ–¹æ³•ï¼Œæ‰©å±•ç‰ˆ13ç§æ€»æ–¹æ³•ï¼‰
- **è®¾ç½®**: 40%æ ‡ç­¾å™ªå£°ï¼Œæ¨¡æ‹ŸçœŸå®ä¸–ç•Œæ•°æ®è´¨é‡æŒ‘æˆ˜
- **å¯¹æ¯”æ¨¡å‹**: 
  - ä¼ ç»Ÿæ–¹æ³•ï¼ˆæ ¸å¿ƒ6ç§ï¼‰: sklearn MLP, PyTorch MLP, Random Forest, XGBoost, LightGBM, CatBoost
  - ç¨³å¥å›å½’ï¼ˆé¢å¤–3ç§ï¼‰: Huber MLP, Pinball MLP, Cauchy MLP
  - CausalEngineï¼ˆ4ç§æ¨¡å¼ï¼‰: deterministic, exogenous, endogenous, standard
- **åˆ†æé‡ç‚¹**: 
  - å“ªç§å› æœæ¨ç†æ¨¡å¼è¡¨ç°æœ€ä¼˜ï¼Ÿ
  - ä¸åŒæ¨¡å¼çš„æ€§èƒ½ç‰¹ç‚¹å’Œå·®å¼‚
  - å› æœæ¨ç†ç›¸å¯¹ä¼ ç»Ÿæ–¹æ³•çš„ä¼˜åŠ¿
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, median_absolute_error
import warnings
import os
import sys
import time

# è®¾ç½®matplotlibåç«¯ä¸ºéäº¤äº’å¼ï¼Œé¿å…å¼¹å‡ºçª—å£
plt.switch_backend('Agg')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥sklearn-styleå®ç°
from causal_sklearn.regressor import (
    MLPCausalRegressor, MLPPytorchRegressor, 
    MLPHuberRegressor, MLPPinballRegressor, MLPCauchyRegressor
)
from causal_sklearn.data_processing import inject_shuffle_noise
from sklearn.neural_network import MLPRegressor
from sklearn.ensemble import RandomForestRegressor
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False
try:
    import catboost as cb
    CATBOOST_AVAILABLE = True
except ImportError:
    CATBOOST_AVAILABLE = False

warnings.filterwarnings('ignore')


class SklearnStyleTutorialConfig:
    """
    Sklearn-Styleæ•™ç¨‹é…ç½®ç±» - æµ‹è¯•æ‰€æœ‰CausalEngineæ¨¡å¼
    
    ğŸ”§ åœ¨è¿™é‡Œä¿®æ”¹å‚æ•°æ¥è‡ªå®šä¹‰å®éªŒè®¾ç½®ï¼
    """
    
    # ğŸ¯ æ•°æ®åˆ†å‰²å‚æ•°
    TEST_SIZE = 0.2          # æµ‹è¯•é›†æ¯”ä¾‹
    VAL_SIZE = 0.2           # éªŒè¯é›†æ¯”ä¾‹ (ç›¸å¯¹äºè®­ç»ƒé›†)
    RANDOM_STATE = 42        # éšæœºç§å­
    
    # ğŸ§  ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½® - æ‰€æœ‰ç¥ç»ç½‘ç»œæ–¹æ³•ä½¿ç”¨ç›¸åŒå‚æ•°ç¡®ä¿å…¬å¹³æ¯”è¾ƒ
    # =========================================================================
    # ğŸ”§ åœ¨è¿™é‡Œä¿®æ”¹æ‰€æœ‰ç¥ç»ç½‘ç»œæ–¹æ³•çš„å…±åŒå‚æ•°ï¼
    NN_HIDDEN_SIZES = (128, 64, 32)                  # ç¥ç»ç½‘ç»œéšè—å±‚ç»“æ„
    NN_MAX_EPOCHS = 3000                         # æœ€å¤§è®­ç»ƒè½®æ•°
    NN_LEARNING_RATE = 0.01                      # å­¦ä¹ ç‡
    NN_PATIENCE = 50                            # æ—©åœpatience
    NN_TOLERANCE = 1e-4                          # æ—©åœtolerance
    NN_BATCH_SIZE = 200                          # æ‰¹å¤„ç†å¤§å°
    # =========================================================================
    
    # ğŸ¤– CausalEngineå‚æ•° - æµ‹è¯•4ç§æœ‰æ•ˆæ¨¡å¼
    CAUSAL_MODES = ['deterministic', 'exogenous', 'endogenous', 'standard']
    CAUSAL_HIDDEN_SIZES = NN_HIDDEN_SIZES        # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    CAUSAL_MAX_EPOCHS = NN_MAX_EPOCHS            # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    CAUSAL_LR = NN_LEARNING_RATE                 # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    CAUSAL_PATIENCE = NN_PATIENCE                # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    CAUSAL_TOL = NN_TOLERANCE                    # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    CAUSAL_GAMMA_INIT = 1.0                      # gammaåˆå§‹åŒ–
    CAUSAL_B_NOISE_INIT = 1.0                    # b_noiseåˆå§‹åŒ–
    CAUSAL_B_NOISE_TRAINABLE = True              # b_noiseæ˜¯å¦å¯è®­ç»ƒ
    CAUSAL_ALPHA = 0.0                           # CausalEngine L2æ­£åˆ™åŒ–
    
    # ğŸ§  ä¼ ç»Ÿç¥ç»ç½‘ç»œæ–¹æ³•å‚æ•° - ä½¿ç”¨ç»Ÿä¸€é…ç½®
    SKLEARN_HIDDEN_LAYERS = NN_HIDDEN_SIZES      # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    SKLEARN_MAX_ITER = NN_MAX_EPOCHS             # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    SKLEARN_LR = NN_LEARNING_RATE                # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    SKLEARN_ALPHA = 0.0                          # sklearn MLP L2æ­£åˆ™åŒ–
    
    PYTORCH_EPOCHS = NN_MAX_EPOCHS               # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    PYTORCH_LR = NN_LEARNING_RATE                # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    PYTORCH_PATIENCE = NN_PATIENCE               # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    PYTORCH_ALPHA = 0.0                          # PyTorch MLP L2æ­£åˆ™åŒ–
    
    # ğŸ“Š å®éªŒå‚æ•°
    ANOMALY_RATIO = 0.3                          # æ ‡ç­¾å¼‚å¸¸æ¯”ä¾‹ (æ ¸å¿ƒå®éªŒé»˜è®¤å€¼: 40%å™ªå£°æŒ‘æˆ˜)
    SAVE_PLOTS = True                            # æ˜¯å¦ä¿å­˜å›¾è¡¨
    VERBOSE = True                               # æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†è¾“å‡º
    
    # ğŸ¯ è¦æµ‹è¯•çš„æ–¹æ³•åˆ—è¡¨
    METHODS_TO_TEST = {
        'sklearn_mlp': True,         # sklearn MLPRegressor
        'pytorch_mlp': True,         # PyTorch MLP
        'mlp_huber': True,           # HuberæŸå¤±MLP
        'mlp_pinball': True,         # PinballæŸå¤±MLP  
        'mlp_cauchy': True,          # CauchyæŸå¤±MLP
        'random_forest': True,       # Random Forest
        'xgboost': True,            # XGBoost (å¦‚æœå¯ç”¨)
        'lightgbm': True,           # LightGBM (å¦‚æœå¯ç”¨)
        'catboost': True,           # CatBoost (å¦‚æœå¯ç”¨)
        'causal_deterministic': True,  # CausalEngine deterministic
        'causal_exogenous': True,      # CausalEngine exogenous
        'causal_endogenous': True,     # CausalEngine endogenous
        'causal_standard': True,       # CausalEngine standard
    }
    
    # ğŸŒ² ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•å‚æ•°
    RANDOM_FOREST_N_ESTIMATORS = 100
    RANDOM_FOREST_MAX_DEPTH = 10
    RANDOM_FOREST_MIN_SAMPLES_SPLIT = 5
    
    XGBOOST_N_ESTIMATORS = 100
    XGBOOST_MAX_DEPTH = 6
    XGBOOST_LEARNING_RATE = 0.1
    
    LIGHTGBM_N_ESTIMATORS = 100
    LIGHTGBM_MAX_DEPTH = 6
    LIGHTGBM_LEARNING_RATE = 0.1
    
    CATBOOST_ITERATIONS = 100
    CATBOOST_DEPTH = 6
    CATBOOST_LEARNING_RATE = 0.1
    
    # ğŸ›‘ æ ‘æ–¹æ³•æ—©åœé…ç½®ï¼ˆä»…XGBoost, LightGBM, CatBoostæ”¯æŒï¼‰
    TREE_EARLY_STOPPING_ROUNDS = 10     # æ—©åœpatienceï¼Œä¸ç¥ç»ç½‘ç»œNN_PATIENCE/5ä¿æŒåˆç†æ¯”ä¾‹
    
    # ğŸ“ˆ å¯è§†åŒ–å‚æ•°
    FIGURE_DPI = 300                             # å›¾è¡¨åˆ†è¾¨ç‡
    FIGURE_SIZE_ANALYSIS = (16, 12)              # æ•°æ®åˆ†æå›¾è¡¨å¤§å°
    FIGURE_SIZE_PERFORMANCE = (24, 20)           # æ€§èƒ½å¯¹æ¯”å›¾è¡¨å¤§å°ï¼ˆæ›´å¤§ä»¥å®¹çº³13ä¸ªæ–¹æ³•ï¼‰
    FIGURE_SIZE_MODES_COMPARISON = (18, 12)      # CausalEngineæ¨¡å¼å¯¹æ¯”å›¾è¡¨å¤§å°
    
    # ğŸ“ è¾“å‡ºç›®å½•å‚æ•°
    OUTPUT_DIR = "results/comprehensive_causal_modes_sklearn_style"


class SklearnStyleCausalModesTutorial:
    """
    å…¨é¢CausalEngineæ¨¡å¼æ•™ç¨‹ç±» - Sklearn-Styleç‰ˆæœ¬
    
    æ¼”ç¤ºæ‰€æœ‰CausalEngineæ¨ç†æ¨¡å¼åœ¨çœŸå®ä¸–ç•Œå›å½’ä»»åŠ¡ä¸­çš„æ€§èƒ½ç‰¹ç‚¹
    """
    
    def __init__(self, config=None):
        self.config = config if config is not None else SklearnStyleTutorialConfig()
        self.X = None
        self.y = None
        self.feature_names = None
        self.results = {}
        self._ensure_output_dir()
    
    def _ensure_output_dir(self):
        """ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨"""
        if not os.path.exists(self.config.OUTPUT_DIR):
            os.makedirs(self.config.OUTPUT_DIR)
            print(f"ğŸ“ åˆ›å»ºè¾“å‡ºç›®å½•: {self.config.OUTPUT_DIR}/")
    
    def _get_output_path(self, filename):
        """è·å–è¾“å‡ºæ–‡ä»¶çš„å®Œæ•´è·¯å¾„"""
        return os.path.join(self.config.OUTPUT_DIR, filename)
        
    def load_and_explore_data(self, verbose=True):
        """åŠ è½½å¹¶æ¢ç´¢åŠ å·æˆ¿ä»·æ•°æ®é›†"""
        if verbose:
            print("ğŸ  å…¨é¢CausalEngineæ¨¡å¼æ•™ç¨‹ - åŠ å·æˆ¿ä»·é¢„æµ‹ (Sklearn-Styleç‰ˆæœ¬)")
            print("=" * 80)
            print("ğŸ“Š æ­£åœ¨åŠ è½½åŠ å·æˆ¿ä»·æ•°æ®é›†...")
        
        # åŠ è½½æ•°æ®
        housing = fetch_california_housing()
        self.X, self.y = housing.data, housing.target
        self.feature_names = housing.feature_names
        
        if verbose:
            print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ")
            print(f"   - æ ·æœ¬æ•°é‡: {self.X.shape[0]:,}")
            print(f"   - ç‰¹å¾æ•°é‡: {self.X.shape[1]}")
            print(f"   - ç‰¹å¾åç§°: {', '.join(self.feature_names)}")
            print(f"   - ç›®æ ‡èŒƒå›´: ${self.y.min():.2f} - ${self.y.max():.2f} (ç™¾ä¸‡ç¾å…ƒ)")
            print(f"   - ç›®æ ‡å‡å€¼: ${self.y.mean():.2f}")
            print(f"   - ç›®æ ‡æ ‡å‡†å·®: ${self.y.std():.2f}")
        
        return self.X, self.y
    
    def visualize_data(self, save_plots=None):
        """æ•°æ®å¯è§†åŒ–åˆ†æ"""
        if save_plots is None:
            save_plots = self.config.SAVE_PLOTS
            
        print("\nğŸ“ˆ æ•°æ®åˆ†å¸ƒåˆ†æ")
        print("-" * 30)
        
        # åˆ›å»ºå›¾å½¢
        fig, axes = plt.subplots(2, 2, figsize=self.config.FIGURE_SIZE_ANALYSIS)
        fig.suptitle('California Housing Dataset Analysis - Sklearn-Style CausalEngine Tutorial', fontsize=16, fontweight='bold')
        
        # 1. ç›®æ ‡å˜é‡åˆ†å¸ƒ
        axes[0, 0].hist(self.y, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
        axes[0, 0].set_title('House Price Distribution')
        axes[0, 0].set_xlabel('House Price ($100k)')
        axes[0, 0].set_ylabel('Frequency')
        axes[0, 0].axvline(self.y.mean(), color='red', linestyle='--', label=f'Mean: ${self.y.mean():.2f}')
        axes[0, 0].legend()
        
        # 2. ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾
        df = pd.DataFrame(self.X, columns=self.feature_names)
        df['MedHouseVal'] = self.y
        corr_matrix = df.corr()
        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, 
                   square=True, ax=axes[0, 1], cbar_kws={'shrink': 0.8})
        axes[0, 1].set_title('Feature Correlation Matrix')
        
        # 3. ç‰¹å¾åˆ†å¸ƒç®±çº¿å›¾
        df_features = pd.DataFrame(self.X, columns=self.feature_names)
        df_features_normalized = (df_features - df_features.mean()) / df_features.std()
        df_features_normalized.boxplot(ax=axes[1, 0])
        axes[1, 0].set_title('Feature Distribution (Standardized)')
        axes[1, 0].set_xlabel('Features')
        axes[1, 0].set_ylabel('Standardized Values')
        axes[1, 0].tick_params(axis='x', rotation=45)
        
        # 4. æœ€é‡è¦ç‰¹å¾ä¸ç›®æ ‡çš„æ•£ç‚¹å›¾
        most_corr_feature = corr_matrix['MedHouseVal'].abs().sort_values(ascending=False).index[1]
        most_corr_idx = list(self.feature_names).index(most_corr_feature)
        axes[1, 1].scatter(self.X[:, most_corr_idx], self.y, alpha=0.5, s=1)
        axes[1, 1].set_title(f'Most Correlated Feature: {most_corr_feature}')
        axes[1, 1].set_xlabel(most_corr_feature)
        axes[1, 1].set_ylabel('House Price ($100k)')
        
        plt.tight_layout()
        
        if save_plots:
            output_path = self._get_output_path('sklearn_style_data_analysis.png')
            plt.savefig(output_path, dpi=self.config.FIGURE_DPI, bbox_inches='tight')
            print(f"ğŸ“Š æ•°æ®åˆ†æå›¾è¡¨å·²ä¿å­˜ä¸º {output_path}")
        
        plt.close()  # å…³é—­å›¾å½¢ï¼Œé¿å…å†…å­˜æ³„æ¼
        
        # æ•°æ®ç»Ÿè®¡æ‘˜è¦
        print("\nğŸ“‹ æ•°æ®ç»Ÿè®¡æ‘˜è¦:")
        print(f"  - æœ€ç›¸å…³ç‰¹å¾: {most_corr_feature} (ç›¸å…³ç³»æ•°: {corr_matrix.loc[most_corr_feature, 'MedHouseVal']:.3f})")
        print(f"  - å¼‚å¸¸å€¼æ£€æµ‹: {np.sum(np.abs(self.y - self.y.mean()) > 3 * self.y.std())} ä¸ªæ½œåœ¨å¼‚å¸¸å€¼")
        print(f"  - æ•°æ®å®Œæ•´æ€§: æ— ç¼ºå¤±å€¼" if not np.any(np.isnan(self.X)) else "  - è­¦å‘Š: å­˜åœ¨ç¼ºå¤±å€¼")
    
    def prepare_data(self, verbose=True):
        """å‡†å¤‡æ•°æ® - ç§‘å­¦ä¸¥è°¨çš„æ ‡å‡†åŒ–ç­–ç•¥"""
        if verbose:
            print("\nğŸ“Š æ•°æ®å‡†å¤‡ - ç§‘å­¦ä¸¥è°¨çš„æ ‡å‡†åŒ–ç­–ç•¥")
            print("=" * 60)
        
        # 1. æ•°æ®åˆ†å‰²
        X_train, X_test, y_train, y_test = train_test_split(
            self.X, self.y,
            test_size=self.config.TEST_SIZE,
            random_state=self.config.RANDOM_STATE
        )
        
        if verbose:
            print(f"âœ… æ•°æ®åˆ†å‰²å®Œæˆ: è®­ç»ƒé›† {len(X_train)} | æµ‹è¯•é›† {len(X_test)}")
        
        # 2. ğŸ¯ æ ‡å‡†åŒ–ç­–ç•¥ï¼ˆåŸºäºå¹²å‡€çš„è®­ç»ƒæ•°æ®ï¼‰
        if verbose:
            print("\nğŸ¯ æ ‡å‡†åŒ–ç­–ç•¥ - åŸºäºå¹²å‡€è®­ç»ƒæ•°æ®å­¦ä¹ ç»Ÿè®¡é‡:")
        
        # ç‰¹å¾æ ‡å‡†åŒ– - åªåœ¨è®­ç»ƒé›†ä¸Šfit
        scaler_X = StandardScaler()
        X_train_scaled = scaler_X.fit_transform(X_train)
        X_test_scaled = scaler_X.transform(X_test)
        
        # ç›®æ ‡æ ‡å‡†åŒ– - å…³é”®ï¼šåœ¨å¹²å‡€çš„è®­ç»ƒé›†ä¸Šfit
        scaler_y = StandardScaler()
        y_train_clean_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()
        y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).flatten()
        
        if verbose:
            print(f"   - ç‰¹å¾æ ‡å‡†åŒ–å™¨åŸºäºè®­ç»ƒé›†å­¦ä¹ : mean={scaler_X.mean_[:3]}, std={scaler_X.scale_[:3]}")
            print(f"   - ç›®æ ‡æ ‡å‡†åŒ–å™¨åŸºäºå¹²å‡€è®­ç»ƒç›®æ ‡å­¦ä¹ : mean={scaler_y.mean_[0]:.3f}, std={scaler_y.scale_[0]:.3f}")
        
        # 3. å™ªå£°æ³¨å…¥ï¼ˆåœ¨æ ‡å‡†åŒ–åè¿›è¡Œï¼‰
        if self.config.ANOMALY_RATIO > 0:
            y_train_scaled_noisy, noise_indices = inject_shuffle_noise(
                y_train_clean_scaled,
                noise_ratio=self.config.ANOMALY_RATIO,
                random_state=self.config.RANDOM_STATE
            )
            y_train_scaled = y_train_scaled_noisy
            
            # åŒæ—¶å¯¹åŸå§‹å°ºåº¦çš„è®­ç»ƒç›®æ ‡åº”ç”¨ç›¸åŒçš„å™ªå£°ï¼ˆç”¨äºè¯„ä¼°ï¼‰
            y_train_noisy, _ = inject_shuffle_noise(
                y_train,
                noise_ratio=self.config.ANOMALY_RATIO,
                random_state=self.config.RANDOM_STATE
            )
            y_train_original = y_train_noisy
            
            if verbose:
                print(f"\nâœ… å™ªå£°æ³¨å…¥å®Œæˆ: {self.config.ANOMALY_RATIO:.1%} ({len(noise_indices)}/{len(y_train_scaled)} æ ·æœ¬å—å½±å“)")
                print(f"   - å™ªå£°åœ¨æ ‡å‡†åŒ–åæ³¨å…¥ï¼Œä¿è¯æ ‡å‡†åŒ–å™¨åŸºäºå¹²å‡€æ•°æ®")
        else:
            y_train_scaled = y_train_clean_scaled
            y_train_original = y_train
            if verbose:
                print("\nâœ… æ— å™ªå£°æ³¨å…¥: çº¯å‡€ç¯å¢ƒ")
        
        if verbose:
            print(f"\nğŸ“Š æœ€ç»ˆæ•°æ®çŠ¶æ€:")
            print(f"   - è®­ç»ƒé›†: Xæ ‡å‡†åŒ– + yæ ‡å‡†åŒ– + {self.config.ANOMALY_RATIO:.0%}å™ªå£°")
            print(f"   - æµ‹è¯•é›†: Xæ ‡å‡†åŒ– + yæ ‡å‡†åŒ– + çº¯å‡€æ— å™ªå£°")
            print(f"   - æ ‡å‡†åŒ–å™¨åŸºäºå¹²å‡€è®­ç»ƒæ•°æ®ï¼Œç¡®ä¿æ— æ³„éœ²")
        
        return {
            # åŸå§‹æ•°æ®ï¼ˆç”¨äºæœ€ç»ˆè¯„ä¼°ï¼‰
            'X_train_original': X_train, 'X_test_original': X_test,
            'y_train_original': y_train_original, 'y_test_original': y_test,
            
            # æ ‡å‡†åŒ–æ•°æ®ï¼ˆç”¨äºæ¨¡å‹è®­ç»ƒï¼‰
            'X_train': X_train_scaled, 'X_test': X_test_scaled,
            'y_train': y_train_scaled, 'y_test': y_test_scaled,
            
            # æ ‡å‡†åŒ–å™¨ï¼ˆç”¨äºé€†å˜æ¢ï¼‰
            'scaler_X': scaler_X, 'scaler_y': scaler_y
        }
    
    def train_sklearn_mlp(self, data, verbose=True):
        """è®­ç»ƒsklearn MLPRegressor"""
        if not self.config.METHODS_TO_TEST.get('sklearn_mlp'):
            return None
            
        if verbose:
            print("ğŸ”§ è®­ç»ƒ sklearn MLPRegressor...")
        
        start_time = time.time()
        model = MLPRegressor(
            hidden_layer_sizes=self.config.SKLEARN_HIDDEN_LAYERS,
            max_iter=self.config.SKLEARN_MAX_ITER,
            learning_rate_init=self.config.SKLEARN_LR,
            early_stopping=True,
            validation_fraction=self.config.VAL_SIZE,
            n_iter_no_change=self.config.NN_PATIENCE,
            tol=self.config.NN_TOLERANCE,
            batch_size=self.config.NN_BATCH_SIZE,
            alpha=self.config.SKLEARN_ALPHA,
            random_state=self.config.RANDOM_STATE,
            verbose=False
        )
        
        model.fit(data['X_train'], data['y_train'])
        training_time = time.time() - start_time
        
        if verbose:
            print(f"   è®­ç»ƒå®Œæˆ: {model.n_iter_} epochs (ç”¨æ—¶: {training_time:.2f}s)")
        
        return model, training_time
    
    def train_pytorch_mlp(self, data, verbose=True):
        """è®­ç»ƒPyTorch MLPRegressor"""
        if not self.config.METHODS_TO_TEST.get('pytorch_mlp'):
            return None
            
        if verbose:
            print("ğŸ”§ è®­ç»ƒ PyTorch MLPRegressor...")
        
        start_time = time.time()
        model = MLPPytorchRegressor(
            hidden_layer_sizes=self.config.SKLEARN_HIDDEN_LAYERS,
            max_iter=self.config.PYTORCH_EPOCHS,
            learning_rate=self.config.PYTORCH_LR,
            early_stopping=True,
            validation_fraction=self.config.VAL_SIZE,
            n_iter_no_change=self.config.NN_PATIENCE,
            tol=self.config.NN_TOLERANCE,
            batch_size=self.config.NN_BATCH_SIZE,
            alpha=self.config.PYTORCH_ALPHA,
            random_state=self.config.RANDOM_STATE,
            verbose=False
        )
        
        model.fit(data['X_train'], data['y_train'])
        training_time = time.time() - start_time
        
        if verbose:
            print(f"   è®­ç»ƒå®Œæˆ: {model.n_iter_} epochs (ç”¨æ—¶: {training_time:.2f}s)")
        
        return model, training_time
    
    def train_robust_mlp(self, data, method_name, regressor_class, verbose=True):
        """è®­ç»ƒç¨³å¥å›å½’å™¨ï¼ˆHuber, Pinball, Cauchyï¼‰"""
        if not self.config.METHODS_TO_TEST.get(method_name):
            return None
            
        if verbose:
            print(f"ğŸ”§ è®­ç»ƒ {regressor_class.__name__}...")
        
        start_time = time.time()
        model = regressor_class(
            hidden_layer_sizes=self.config.SKLEARN_HIDDEN_LAYERS,
            max_iter=self.config.SKLEARN_MAX_ITER,
            learning_rate=self.config.SKLEARN_LR,
            early_stopping=True,
            validation_fraction=self.config.VAL_SIZE,
            n_iter_no_change=self.config.NN_PATIENCE,
            tol=self.config.NN_TOLERANCE,
            batch_size=self.config.NN_BATCH_SIZE,
            alpha=self.config.PYTORCH_ALPHA,  # ä½¿ç”¨ä¸PyTorchç›¸åŒçš„alpha
            random_state=self.config.RANDOM_STATE,
            verbose=False
        )
        
        model.fit(data['X_train'], data['y_train'])
        training_time = time.time() - start_time
        
        if verbose:
            print(f"   è®­ç»ƒå®Œæˆ: {model.n_iter_} epochs (ç”¨æ—¶: {training_time:.2f}s)")
        
        return model, training_time
    
    def train_causal_regressor(self, data, mode, verbose=True):
        """è®­ç»ƒCausalEngineå›å½’å™¨"""
        method_key = f'causal_{mode}'
        if not self.config.METHODS_TO_TEST.get(method_key):
            return None
            
        if verbose:
            print(f"ğŸ”§ è®­ç»ƒ CausalEngine ({mode})...")
        
        start_time = time.time()
        model = MLPCausalRegressor(
            perception_hidden_layers=self.config.CAUSAL_HIDDEN_SIZES,
            mode=mode,
            max_iter=self.config.CAUSAL_MAX_EPOCHS,
            learning_rate=self.config.CAUSAL_LR,
            early_stopping=True,
            validation_fraction=self.config.VAL_SIZE,
            n_iter_no_change=self.config.CAUSAL_PATIENCE,
            tol=self.config.CAUSAL_TOL,
            batch_size=self.config.NN_BATCH_SIZE,
            alpha=self.config.CAUSAL_ALPHA,
            gamma_init=self.config.CAUSAL_GAMMA_INIT,
            b_noise_init=self.config.CAUSAL_B_NOISE_INIT,
            b_noise_trainable=self.config.CAUSAL_B_NOISE_TRAINABLE,
            random_state=self.config.RANDOM_STATE,
            verbose=False
        )
        
        model.fit(data['X_train'], data['y_train'])
        training_time = time.time() - start_time
        
        if verbose:
            print(f"   è®­ç»ƒå®Œæˆ: {model.n_iter_} epochs (ç”¨æ—¶: {training_time:.2f}s)")
        
        return model, training_time
    
    def train_random_forest(self, data, verbose=True):
        """è®­ç»ƒRandom Forest"""
        if not self.config.METHODS_TO_TEST.get('random_forest'):
            return None
            
        if verbose:
            print("ğŸŒ² è®­ç»ƒ Random Forest...")
        
        start_time = time.time()
        model = RandomForestRegressor(
            n_estimators=self.config.RANDOM_FOREST_N_ESTIMATORS,
            max_depth=self.config.RANDOM_FOREST_MAX_DEPTH,
            min_samples_split=self.config.RANDOM_FOREST_MIN_SAMPLES_SPLIT,
            random_state=self.config.RANDOM_STATE,
            n_jobs=-1
        )
        
        model.fit(data['X_train'], data['y_train'])
        training_time = time.time() - start_time
        
        if verbose:
            print(f"   è®­ç»ƒå®Œæˆ (ç”¨æ—¶: {training_time:.2f}s)")
        
        return model, training_time
    
    def train_xgboost(self, data, verbose=True):
        """è®­ç»ƒXGBoost"""
        if not self.config.METHODS_TO_TEST.get('xgboost') or not XGBOOST_AVAILABLE:
            return None
            
        if verbose:
            print("ğŸš€ è®­ç»ƒ XGBoost...")
        
        start_time = time.time()
        
        # å‡†å¤‡éªŒè¯é›†ç”¨äºæ—©åœï¼ˆä½¿ç”¨æ ‡å‡†åŒ–æ•°æ®ï¼‰
        X_train_std = data['X_train']
        y_train_std = data['y_train']
        X_train_val, X_val, y_train_val, y_val = train_test_split(
            X_train_std, y_train_std,
            test_size=self.config.VAL_SIZE,
            random_state=self.config.RANDOM_STATE
        )
        
        model = xgb.XGBRegressor(
            n_estimators=self.config.XGBOOST_N_ESTIMATORS,
            max_depth=self.config.XGBOOST_MAX_DEPTH,
            learning_rate=self.config.XGBOOST_LEARNING_RATE,
            random_state=self.config.RANDOM_STATE,
            n_jobs=-1,
            verbosity=0,
            early_stopping_rounds=self.config.TREE_EARLY_STOPPING_ROUNDS
        )
        
        # ä½¿ç”¨æ—©åœè®­ç»ƒï¼ˆç»Ÿä¸€ä½¿ç”¨æ ‡å‡†åŒ–æ•°æ®ï¼‰
        model.fit(
            X_train_val, y_train_val,
            eval_set=[(X_val, y_val)],
            verbose=False
        )
        
        training_time = time.time() - start_time
        
        if verbose:
            print(f"   è®­ç»ƒå®Œæˆ: {model.best_iteration} è½® (æ—©åœ) (ç”¨æ—¶: {training_time:.2f}s)")
        
        return model, training_time
    
    def train_lightgbm(self, data, verbose=True):
        """è®­ç»ƒLightGBM"""
        if not self.config.METHODS_TO_TEST.get('lightgbm') or not LIGHTGBM_AVAILABLE:
            return None
            
        if verbose:
            print("âš¡ è®­ç»ƒ LightGBM...")
        
        start_time = time.time()
        model = lgb.LGBMRegressor(
            n_estimators=self.config.LIGHTGBM_N_ESTIMATORS,
            max_depth=self.config.LIGHTGBM_MAX_DEPTH,
            learning_rate=self.config.LIGHTGBM_LEARNING_RATE,
            random_state=self.config.RANDOM_STATE,
            n_jobs=-1,
            verbosity=-1
        )
        
        model.fit(data['X_train'], data['y_train'])
        training_time = time.time() - start_time
        
        if verbose:
            print(f"   è®­ç»ƒå®Œæˆ (ç”¨æ—¶: {training_time:.2f}s)")
        
        return model, training_time
    
    def train_catboost(self, data, verbose=True):
        """è®­ç»ƒCatBoost"""
        if not self.config.METHODS_TO_TEST.get('catboost') or not CATBOOST_AVAILABLE:
            return None
            
        if verbose:
            print("ğŸ± è®­ç»ƒ CatBoost...")
        
        start_time = time.time()
        model = cb.CatBoostRegressor(
            iterations=self.config.CATBOOST_ITERATIONS,
            depth=self.config.CATBOOST_DEPTH,
            learning_rate=self.config.CATBOOST_LEARNING_RATE,
            random_seed=self.config.RANDOM_STATE,
            thread_count=-1,
            verbose=False
        )
        
        model.fit(data['X_train'], data['y_train'])
        training_time = time.time() - start_time
        
        if verbose:
            print(f"   è®­ç»ƒå®Œæˆ (ç”¨æ—¶: {training_time:.2f}s)")
        
        return model, training_time
    
    def evaluate_model(self, model, data, model_name):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½ - ç»Ÿä¸€é€†å˜æ¢é€»è¾‘"""
        # ğŸ¯ ç»Ÿä¸€ç­–ç•¥ï¼šæ‰€æœ‰æ–¹æ³•åœ¨æ ‡å‡†åŒ–ç©ºé—´é¢„æµ‹ï¼Œç„¶åé€†å˜æ¢åˆ°åŸå§‹å°ºåº¦è¯„ä¼°
        test_pred_scaled = model.predict(data['X_test'])
        
        # å°†é¢„æµ‹ç»“æœè½¬æ¢å›åŸå§‹å°ºåº¦è¿›è¡Œè¯„ä¼°
        test_pred_original = data['scaler_y'].inverse_transform(test_pred_scaled.reshape(-1, 1)).flatten()
        
        # åœ¨åŸå§‹å°ºåº¦ä¸‹è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        results = {
            'test': {
                'MAE': mean_absolute_error(data['y_test_original'], test_pred_original),
                'MdAE': median_absolute_error(data['y_test_original'], test_pred_original),
                'RMSE': np.sqrt(mean_squared_error(data['y_test_original'], test_pred_original)),
                'RÂ²': r2_score(data['y_test_original'], test_pred_original)
            }
        }
        
        return results
    
    def run_comprehensive_benchmark(self, verbose=None):
        """è¿è¡Œå…¨é¢çš„åŸºå‡†æµ‹è¯• - åŒ…å«æ‰€æœ‰CausalEngineæ¨¡å¼"""
        if verbose is None:
            verbose = self.config.VERBOSE
            
        if verbose:
            print("\nğŸš€ å¼€å§‹å…¨é¢åŸºå‡†æµ‹è¯• - Sklearn-Styleå®ç°")
            print("=" * 80)
            print(f"ğŸ”§ å®éªŒé…ç½®:")
            print(f"   - æµ‹è¯•é›†æ¯”ä¾‹: {self.config.TEST_SIZE:.1%}")
            print(f"   - éªŒè¯é›†æ¯”ä¾‹: {self.config.VAL_SIZE:.1%}")
            print(f"   - å¼‚å¸¸æ ‡ç­¾æ¯”ä¾‹: {self.config.ANOMALY_RATIO:.1%}")
            print(f"   - éšæœºç§å­: {self.config.RANDOM_STATE}")
            print(f"   - CausalEngineæ¨¡å¼: {', '.join(self.config.CAUSAL_MODES)}")
            print(f"   - ç½‘ç»œç»“æ„: {self.config.CAUSAL_HIDDEN_SIZES}")
            print(f"   - æœ€å¤§è®­ç»ƒè½®æ•°: {self.config.CAUSAL_MAX_EPOCHS}")
            print(f"   - æ—©åœpatience: {self.config.CAUSAL_PATIENCE}")
            enabled_methods = [k for k, v in self.config.METHODS_TO_TEST.items() if v]
            print(f"   - æ€»è®¡å¯¹æ¯”æ–¹æ³•: {len(enabled_methods)} ç§")
            print(f"   - æ–¹æ³•åˆ—è¡¨: {', '.join(enabled_methods)}")
        
        # å‡†å¤‡æ•°æ®
        data = self.prepare_data(verbose=verbose)
        self.results = {}
        training_times = {}
        
        # 1. è®­ç»ƒsklearn MLPRegressor
        result = self.train_sklearn_mlp(data, verbose=verbose)
        if result:
            model, train_time = result
            self.results['sklearn'] = self.evaluate_model(model, data, 'sklearn')
            training_times['sklearn'] = train_time
        
        # 2. è®­ç»ƒPyTorch MLPRegressor
        result = self.train_pytorch_mlp(data, verbose=verbose)
        if result:
            model, train_time = result
            self.results['pytorch'] = self.evaluate_model(model, data, 'pytorch')
            training_times['pytorch'] = train_time
        
        # 3. è®­ç»ƒç¨³å¥å›å½’å™¨
        robust_methods = [
            ('mlp_huber', MLPHuberRegressor),
            ('mlp_pinball', MLPPinballRegressor),
            ('mlp_cauchy', MLPCauchyRegressor)
        ]
        
        for method_name, regressor_class in robust_methods:
            result = self.train_robust_mlp(data, method_name, regressor_class, verbose=verbose)
            if result:
                model, train_time = result
                # æ˜ å°„åˆ°ç»“æœé”®å
                result_key = method_name.replace('mlp_', '')
                if method_name == 'mlp_pinball':
                    result_key = 'pinball'
                self.results[result_key] = self.evaluate_model(model, data, result_key)
                training_times[result_key] = train_time
        
        # 4. è®­ç»ƒä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•
        traditional_ml_methods = [
            ('random_forest', self.train_random_forest),
            ('xgboost', self.train_xgboost),
            ('lightgbm', self.train_lightgbm),
            ('catboost', self.train_catboost)
        ]
        
        for method_name, train_func in traditional_ml_methods:
            result = train_func(data, verbose=verbose)
            if result:
                model, train_time = result
                self.results[method_name] = self.evaluate_model(model, data, method_name)
                training_times[method_name] = train_time
        
        # 5. è®­ç»ƒCausalEngineæ¨¡å¼
        for mode in self.config.CAUSAL_MODES:
            result = self.train_causal_regressor(data, mode, verbose=verbose)
            if result:
                model, train_time = result
                self.results[mode] = self.evaluate_model(model, data, mode)
                training_times[mode] = train_time
        
        if verbose:
            print(f"\nğŸ“Š å…¨é¢åŸºå‡†æµ‹è¯•ç»“æœ (å¼‚å¸¸æ¯”ä¾‹: {self.config.ANOMALY_RATIO:.0%})")
            self.print_results(training_times)
        
        return self.results
    
    def print_results(self, training_times=None):
        """æ‰“å°æµ‹è¯•ç»“æœ"""
        if not self.results:
            print("âŒ æ²¡æœ‰å¯æ˜¾ç¤ºçš„ç»“æœ")
            return
        
        print("\n" + "=" * 120)
        print(f"{'æ–¹æ³•':<20} {'MAE':<10} {'MdAE':<10} {'RMSE':<10} {'RÂ²':<10} {'è®­ç»ƒæ—¶é—´(s)':<12}")
        print("-" * 120)
        
        # æŒ‰MdAEæ’åºï¼ˆè¶Šå°è¶Šå¥½ï¼‰
        sorted_results = sorted(self.results.items(), key=lambda x: x[1]['test']['MdAE'])
        
        for method, metrics in sorted_results:
            test_m = metrics['test']
            train_time = training_times.get(method, 0.0) if training_times else 0.0
            
            # æ–¹æ³•åæ˜¾ç¤ºä¼˜åŒ–
            if method in self.config.CAUSAL_MODES:
                display_name = f"CausalEngine ({method})"
            elif method == 'random_forest':
                display_name = "Random Forest"
            elif method == 'xgboost':
                display_name = "XGBoost"
            elif method == 'lightgbm':
                display_name = "LightGBM"
            elif method == 'catboost':
                display_name = "CatBoost"
            else:
                display_name = method.replace('_', ' ').title()
            
            print(f"{display_name:<20} {test_m['MAE']:<10.4f} {test_m['MdAE']:<10.4f} "
                  f"{test_m['RMSE']:<10.4f} {test_m['RÂ²']:<10.4f} {train_time:<12.2f}")
        
        print("=" * 120)
    
    def analyze_causal_modes_performance(self, verbose=True):
        """ä¸“é—¨åˆ†æCausalEngineä¸åŒæ¨¡å¼çš„æ€§èƒ½ç‰¹ç‚¹"""
        if not self.results:
            print("âŒ è¯·å…ˆè¿è¡ŒåŸºå‡†æµ‹è¯•")
            return
        
        if verbose:
            print("\nğŸ”¬ CausalEngineæ¨¡å¼æ·±åº¦åˆ†æ")
            print("=" * 70)
        
        # æå–CausalEngineæ¨¡å¼ç»“æœ
        causal_results = {}
        traditional_results = {}
        
        for method, metrics in self.results.items():
            if method in self.config.CAUSAL_MODES:
                causal_results[method] = metrics
            else:
                traditional_results[method] = metrics
        
        if verbose:
            print(f"ğŸ¯ CausalEngineæ¨¡å¼æ€§èƒ½å¯¹æ¯” (å…±{len(causal_results)}ç§æ¨¡å¼):")
            print("-" * 50)
            
            # æŒ‰MdAEåˆ†æ•°æ’åºï¼ˆè¶Šå°è¶Šå¥½ï¼‰
            causal_mdae_scores = {mode: metrics['test']['MdAE'] for mode, metrics in causal_results.items()}
            sorted_causal = sorted(causal_mdae_scores.items(), key=lambda x: x[1])
            
            for i, (mode, mdae) in enumerate(sorted_causal, 1):
                mae = causal_results[mode]['test']['MAE']
                r2 = causal_results[mode]['test']['RÂ²']
                print(f"   {i}. {mode:<12} - MdAE: {mdae:.3f}, MAE: {mae:.3f}, RÂ²: {r2:.4f}")
            
            # æ¨¡å¼ç‰¹ç‚¹åˆ†æ
            print(f"\nğŸ“Š æ¨¡å¼ç‰¹ç‚¹åˆ†æ:")
            print("-" * 30)
            
            best_mode = sorted_causal[0][0]
            worst_mode = sorted_causal[-1][0]
            performance_gap = sorted_causal[-1][1] - sorted_causal[0][1]
            
            print(f"   ğŸ† æœ€ä½³æ¨¡å¼: {best_mode} (MdAE = {sorted_causal[0][1]:.3f})")
            print(f"   ğŸ“‰ æœ€å¼±æ¨¡å¼: {worst_mode} (MdAE = {sorted_causal[-1][1]:.3f})")
            print(f"   ğŸ“ æ€§èƒ½å·®è·: {performance_gap:.3f} ({performance_gap/sorted_causal[0][1]*100:.1f}%)")
            
            # ä¸ä¼ ç»Ÿæ–¹æ³•æ¯”è¾ƒ
            if traditional_results:
                print(f"\nğŸ†š CausalEngine vs ä¼ ç»Ÿæ–¹æ³•:")
                print("-" * 40)
                
                traditional_mdae_scores = {method: metrics['test']['MdAE'] for method, metrics in traditional_results.items()}
                best_traditional = min(traditional_mdae_scores.keys(), key=lambda x: traditional_mdae_scores[x])
                best_traditional_mdae = traditional_mdae_scores[best_traditional]
                
                print(f"   æœ€ä½³ä¼ ç»Ÿæ–¹æ³•: {best_traditional} (MdAE = {best_traditional_mdae:.3f})")
                print(f"   æœ€ä½³CausalEngine: {best_mode} (MdAE = {sorted_causal[0][1]:.3f})")
                
                improvement = (best_traditional_mdae - sorted_causal[0][1]) / best_traditional_mdae * 100
                print(f"   æ€§èƒ½æå‡: {improvement:+.2f}%")
                
                better_modes = sum(1 for _, mdae in sorted_causal if mdae < best_traditional_mdae)
                print(f"   ä¼˜äºä¼ ç»Ÿæ–¹æ³•çš„CausalEngineæ¨¡å¼: {better_modes}/{len(sorted_causal)}")
        
        return causal_results, traditional_results
    
    def create_comprehensive_performance_visualization(self, save_plot=None, extended=False):
        """åˆ›å»ºå…¨é¢çš„æ€§èƒ½å¯è§†åŒ–å›¾è¡¨ - æ”¯æŒæ ‡å‡†ç‰ˆå’Œæ‰©å±•ç‰ˆ"""
        if save_plot is None:
            save_plot = self.config.SAVE_PLOTS
            
        if not self.results:
            print("âŒ è¯·å…ˆè¿è¡ŒåŸºå‡†æµ‹è¯•")
            return
        
        chart_type = "æ‰©å±•ç‰ˆ" if extended else "æ ‡å‡†ç‰ˆ"
        print(f"\nğŸ“Š åˆ›å»ºå…¨é¢æ€§èƒ½å¯è§†åŒ–å›¾è¡¨ ({chart_type})")
        print("-" * 40)
        
        # å‡†å¤‡æ•°æ® - æ ¹æ®æ‰©å±•æ ‡å¿—å†³å®šåŒ…å«çš„æ–¹æ³•
        if extended:
            # æ‰©å±•ç‰ˆï¼šåŒ…å«æ‰€æœ‰å¯ç”¨æ–¹æ³•
            all_available_methods = list(self.results.keys())
            # æŒ‰ç±»å‹æ’åºï¼šå…ˆä¼ ç»Ÿæ–¹æ³•ï¼ŒåCausalEngine
            traditional_methods = [m for m in all_available_methods if m not in self.config.CAUSAL_MODES]
            causal_methods = [m for m in self.config.CAUSAL_MODES if m in self.results]
            methods = traditional_methods + causal_methods
        else:
            # æ ‡å‡†ç‰ˆï¼šåŒ…å«9ç§æ ¸å¿ƒæ–¹æ³•ï¼ˆé™¤äº†3ç§robust MLPï¼‰
            robust_mlp_methods = ['huber', 'pinball', 'cauchy']  # æ’é™¤çš„robust MLPæ–¹æ³•
            standard_traditional = [m for m in self.results.keys() 
                                  if m not in self.config.CAUSAL_MODES and m not in robust_mlp_methods]
            causal_methods = [m for m in self.config.CAUSAL_MODES if m in self.results]
            methods = standard_traditional + causal_methods
        
        # ä¸ºä¸åŒç±»å‹çš„æ–¹æ³•è®¾ç½®é¢œè‰²
        colors = []
        for method in methods:
            if method in self.config.CAUSAL_MODES:
                colors.append('#2ca02c')  # ç»¿è‰²ç³» - CausalEngine
            else:
                colors.append('#1f77b4')  # è“è‰²ç³» - ä¼ ç»Ÿæ–¹æ³•
        
        metrics = ['MAE', 'MdAE', 'RMSE', 'RÂ²']
        
        # åˆ›å»ºå­å›¾
        fig, axes = plt.subplots(2, 2, figsize=self.config.FIGURE_SIZE_PERFORMANCE)
        title_suffix = " (Extended with All Methods)" if extended else ""
        fig.suptitle(f'Sklearn-Style CausalEngine Modes vs Traditional Methods{title_suffix}\nCalifornia Housing Performance (40% Label Noise)', 
                     fontsize=16, fontweight='bold')
        axes = axes.flatten()
        
        for i, metric in enumerate(metrics):
            values = [self.results[method]['test'][metric] for method in methods]
            
            bars = axes[i].bar(range(len(methods)), values, color=colors, alpha=0.8, edgecolor='black')
            axes[i].set_title(f'{metric} (Test Set)', fontweight='bold')
            axes[i].set_ylabel(metric)
            
            # è®¾ç½®Xè½´æ ‡ç­¾
            method_labels = []
            for method in methods:
                if method in self.config.CAUSAL_MODES:
                    method_labels.append(f'CausalEngine\n({method})')
                elif method == 'sklearn':
                    method_labels.append('sklearn\nMLP')
                elif method == 'pytorch':
                    method_labels.append('PyTorch\nMLP')
                else:
                    display_name = method.replace('_', ' ').title()
                    if len(display_name) > 12:
                        words = display_name.split()
                        if len(words) > 1:
                            display_name = f"{words[0]}\n{' '.join(words[1:])}"
                    method_labels.append(display_name)
            
            axes[i].set_xticks(range(len(methods)))
            axes[i].set_xticklabels(method_labels, rotation=45, ha='right', fontsize=8)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, value in zip(bars, values):
                height = bar.get_height()
                axes[i].text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                           f'{value:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=8)
            
            # é«˜äº®æœ€ä½³ç»“æœ
            if metric == 'RÂ²':
                best_idx = values.index(max(values))
            else:
                best_idx = values.index(min(values))
            bars[best_idx].set_color('gold')
            bars[best_idx].set_edgecolor('red')
            bars[best_idx].set_linewidth(3)
        
        plt.tight_layout()
        
        if save_plot:
            if extended:
                filename = 'sklearn_style_performance_comparison_extended.png'
            else:
                filename = 'sklearn_style_performance_comparison.png'
            output_path = self._get_output_path(filename)
            plt.savefig(output_path, dpi=self.config.FIGURE_DPI, bbox_inches='tight')
            print(f"ğŸ“Š {chart_type}æ€§èƒ½å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜ä¸º {output_path}")
        
        plt.close()
    
    def create_causal_modes_comparison(self, save_plot=None):
        """åˆ›å»ºä¸“é—¨çš„CausalEngineæ¨¡å¼å¯¹æ¯”å›¾è¡¨"""
        if save_plot is None:
            save_plot = self.config.SAVE_PLOTS
            
        if not self.results:
            print("âŒ è¯·å…ˆè¿è¡ŒåŸºå‡†æµ‹è¯•")
            return
        
        print("\nğŸ“Š åˆ›å»ºCausalEngineæ¨¡å¼ä¸“é¡¹å¯¹æ¯”å›¾è¡¨")
        print("-" * 45)
        
        # æå–CausalEngineæ¨¡å¼ç»“æœ
        causal_methods = [m for m in self.config.CAUSAL_MODES if m in self.results]
        
        if len(causal_methods) < 2:
            print("âŒ éœ€è¦è‡³å°‘2ç§CausalEngineæ¨¡å¼æ¥è¿›è¡Œå¯¹æ¯”")
            return
        
        # åˆ›å»ºé›·è¾¾å›¾æ˜¾ç¤ºCausalEngineæ¨¡å¼çš„å¤šç»´æ€§èƒ½
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=self.config.FIGURE_SIZE_MODES_COMPARISON)
        fig.suptitle('Sklearn-Style CausalEngine Modes Detailed Comparison', fontsize=16, fontweight='bold')
        
        # å·¦å›¾ï¼šæ€§èƒ½æ¡å½¢å›¾
        metrics = ['MAE', 'MdAE', 'RMSE', 'RÂ²']
        colors = plt.cm.Set3(np.linspace(0, 1, len(causal_methods)))
        
        x = np.arange(len(metrics))
        width = 0.15
        
        for i, method in enumerate(causal_methods):
            values = [self.results[method]['test'][metric] for metric in metrics]
            ax1.bar(x + i * width, values, width, label=f'{method}', color=colors[i], alpha=0.8)
        
        ax1.set_xlabel('Metrics')
        ax1.set_ylabel('Values')
        ax1.set_title('CausalEngine Modes Performance Comparison')
        ax1.set_xticks(x + width * (len(causal_methods) - 1) / 2)
        ax1.set_xticklabels(metrics)
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # å³å›¾ï¼šMdAEæ€§èƒ½æ’åï¼ˆè¶Šå°è¶Šå¥½ï¼‰
        mdae_scores = [(method, self.results[method]['test']['MdAE']) for method in causal_methods]
        mdae_scores.sort(key=lambda x: x[1])  # æŒ‰å‡åºæ’åˆ—ï¼Œå› ä¸ºMdAEè¶Šå°è¶Šå¥½
        
        methods_sorted = [item[0] for item in mdae_scores]
        mdae_values = [item[1] for item in mdae_scores]
        
        bars = ax2.bar(range(len(methods_sorted)), mdae_values, color=colors[:len(methods_sorted)], alpha=0.8)
        ax2.set_xlabel('CausalEngine Modes')
        ax2.set_ylabel('MdAE (Median Absolute Error)')
        ax2.set_title('CausalEngine Modes MdAE Performance Ranking')
        ax2.set_xticks(range(len(methods_sorted)))
        ax2.set_xticklabels(methods_sorted, rotation=45)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, value in zip(bars, mdae_values):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height + height*0.02,
                    f'{value:.3f}', ha='center', va='bottom', fontweight='bold')
        
        # é«˜äº®æœ€ä½³æ¨¡å¼ï¼ˆMdAEæœ€å°çš„ï¼‰
        bars[0].set_color('gold')
        bars[0].set_edgecolor('red')
        bars[0].set_linewidth(3)
        
        plt.tight_layout()
        
        if save_plot:
            output_path = self._get_output_path('sklearn_style_causal_modes_comparison.png')
            plt.savefig(output_path, dpi=self.config.FIGURE_DPI, bbox_inches='tight')
            print(f"ğŸ“Š CausalEngineæ¨¡å¼å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜ä¸º {output_path}")
        
        plt.close()
    
    def print_comprehensive_summary(self):
        """æ‰“å°å…¨é¢çš„æ€»ç»“æŠ¥å‘Š"""
        if not self.results:
            print("âŒ è¯·å…ˆè¿è¡ŒåŸºå‡†æµ‹è¯•")
            return
        
        print("\nğŸ“‹ å…¨é¢å®éªŒæ€»ç»“æŠ¥å‘Š - Sklearn-Styleç‰ˆæœ¬")
        print("=" * 80)
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_methods = len(self.results)
        causal_methods = len([m for m in self.results if m in self.config.CAUSAL_MODES])
        traditional_methods = total_methods - causal_methods
        
        print(f"ğŸ”¢ å®éªŒè§„æ¨¡:")
        print(f"   - æ€»è®¡æµ‹è¯•æ–¹æ³•: {total_methods}")
        print(f"   - CausalEngineæ¨¡å¼: {causal_methods}")
        print(f"   - ä¼ ç»Ÿæ–¹æ³•: {traditional_methods}")
        print(f"   - æ•°æ®é›†å¤§å°: {self.X.shape[0]:,} æ ·æœ¬ Ã— {self.X.shape[1]} ç‰¹å¾")
        print(f"   - å¼‚å¸¸æ ‡ç­¾æ¯”ä¾‹: {self.config.ANOMALY_RATIO:.1%}")
        
        # æ€§èƒ½æ’å
        print(f"\nğŸ† æ€»ä½“æ€§èƒ½æ’å (æŒ‰MdAEåˆ†æ•°):")
        print("-" * 50)
        
        all_mdae_scores = [(method, metrics['test']['MdAE']) for method, metrics in self.results.items()]
        all_mdae_scores.sort(key=lambda x: x[1])
        
        for i, (method, mdae) in enumerate(all_mdae_scores, 1):
            method_type = "CausalEngine" if method in self.config.CAUSAL_MODES else "Traditional"
            r2 = self.results[method]['test']['RÂ²']
            print(f"   {i:2d}. {method:<15} ({method_type:<12}) - MdAE: {mdae:.3f}, RÂ²: {r2:.4f}")
        
        # CausalEngineä¼˜åŠ¿åˆ†æ
        print(f"\nğŸ¯ CausalEngineæ¨¡å¼åˆ†æ:")
        print("-" * 40)
        
        causal_results = [(method, metrics['test']['MdAE']) for method, metrics in self.results.items() 
                         if method in self.config.CAUSAL_MODES]
        traditional_results = [(method, metrics['test']['MdAE']) for method, metrics in self.results.items() 
                              if method not in self.config.CAUSAL_MODES]
        
        if causal_results and traditional_results:
            best_causal = min(causal_results, key=lambda x: x[1])
            best_traditional = min(traditional_results, key=lambda x: x[1])
            
            print(f"   æœ€ä½³CausalEngineæ¨¡å¼: {best_causal[0]} (MdAE = {best_causal[1]:.3f})")
            print(f"   æœ€ä½³ä¼ ç»Ÿæ–¹æ³•: {best_traditional[0]} (MdAE = {best_traditional[1]:.3f})")
            
            improvement = (best_traditional[1] - best_causal[1]) / best_traditional[1] * 100
            print(f"   æ€§èƒ½æå‡: {improvement:+.2f}%")
            
            better_causal_count = sum(1 for _, mdae in causal_results if mdae < best_traditional[1])
            print(f"   ä¼˜äºæœ€ä½³ä¼ ç»Ÿæ–¹æ³•çš„CausalEngineæ¨¡å¼: {better_causal_count}/{len(causal_results)}")
        
        # å…³é”®å‘ç°
        print(f"\nğŸ’¡ å…³é”®å‘ç°:")
        print("-" * 20)
        
        if len(all_mdae_scores) > 0:
            top_method = all_mdae_scores[0]
            if top_method[0] in self.config.CAUSAL_MODES:
                print(f"   âœ… CausalEngineæ¨¡å¼ '{top_method[0]}' åœ¨MdAEæŒ‡æ ‡ä¸Šå–å¾—æœ€ä½³æ€§èƒ½")
                print(f"   âœ… å› æœæ¨ç†åœ¨ç¨³å¥æ€§æ–¹é¢æ˜¾ç¤ºå‡ºæ˜æ˜¾ä¼˜åŠ¿")
            else:
                print(f"   âš ï¸ ä¼ ç»Ÿæ–¹æ³• '{top_method[0]}' åœ¨MdAEæŒ‡æ ‡ä¸Šè¡¨ç°æœ€ä¼˜")
                print(f"   âš ï¸ å»ºè®®è¿›ä¸€æ­¥è°ƒä¼˜CausalEngineå‚æ•°")


def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œå®Œæ•´çš„å…¨é¢CausalEngineæ¨¡å¼æ•™ç¨‹ - Sklearn-Styleç‰ˆæœ¬"""
    print("ğŸ  å…¨é¢CausalEngineæ¨¡å¼æ•™ç¨‹ - Sklearn-Styleç‰ˆæœ¬")
    print("ğŸ¯ ç›®æ ‡ï¼šæµ‹è¯•æ‰€æœ‰CausalEngineæ¨ç†æ¨¡å¼åœ¨çœŸå®ä¸–ç•Œå›å½’ä»»åŠ¡ä¸­çš„è¡¨ç°")
    print("=" * 90)
    
    # æ£€æŸ¥åŒ…å¯ç”¨æ€§
    print("ğŸ“¦ æ£€æŸ¥ä¾èµ–åŒ…å¯ç”¨æ€§:")
    available_packages = []
    if XGBOOST_AVAILABLE:
        available_packages.append("XGBoost")
    if LIGHTGBM_AVAILABLE:
        available_packages.append("LightGBM")
    if CATBOOST_AVAILABLE:
        available_packages.append("CatBoost")
    
    print(f"   âœ… å¯ç”¨çš„ä¼ ç»Ÿæœºå™¨å­¦ä¹ åŒ…: {', '.join(available_packages) if available_packages else 'æ— '}")
    
    missing_packages = []
    if not XGBOOST_AVAILABLE:
        missing_packages.append("xgboost")
    if not LIGHTGBM_AVAILABLE:
        missing_packages.append("lightgbm")
    if not CATBOOST_AVAILABLE:
        missing_packages.append("catboost")
    
    if missing_packages:
        print(f"   âš ï¸ ç¼ºå¤±çš„åŒ…: {', '.join(missing_packages)}")
        print(f"   ğŸ’¡ æç¤º: pip install {' '.join(missing_packages)}")
    print()
    
    # åˆ›å»ºé…ç½®å®ä¾‹
    config = SklearnStyleTutorialConfig()
    
    print(f"ğŸ”§ å½“å‰é…ç½®:")
    print(f"   - CausalEngineæ¨¡å¼: {', '.join(config.CAUSAL_MODES)} (å…±{len(config.CAUSAL_MODES)}ç§)")
    print(f"   - ç½‘ç»œæ¶æ„: {config.CAUSAL_HIDDEN_SIZES}")
    print(f"   - æœ€å¤§è½®æ•°: {config.CAUSAL_MAX_EPOCHS}")
    print(f"   - æ—©åœpatience: {config.CAUSAL_PATIENCE}")
    print(f"   - å¼‚å¸¸æ¯”ä¾‹: {config.ANOMALY_RATIO:.1%}")
    enabled_methods = [k for k, v in config.METHODS_TO_TEST.items() if v]
    print(f"   - æ€»è®¡å¯¹æ¯”æ–¹æ³•: {len(enabled_methods)} ç§")
    print(f"   - è¾“å‡ºç›®å½•: {config.OUTPUT_DIR}/")
    print()
    
    # åˆ›å»ºæ•™ç¨‹å®ä¾‹
    tutorial = SklearnStyleCausalModesTutorial(config)
    
    # 1. åŠ è½½å’Œæ¢ç´¢æ•°æ®
    tutorial.load_and_explore_data()
    
    # 2. æ•°æ®å¯è§†åŒ–
    tutorial.visualize_data()
    
    # 3. è¿è¡Œå…¨é¢åŸºå‡†æµ‹è¯•
    tutorial.run_comprehensive_benchmark()
    
    # 4. ä¸“é—¨åˆ†æCausalEngineæ¨¡å¼æ€§èƒ½
    tutorial.analyze_causal_modes_performance()
    
    # 5. åˆ›å»ºå…¨é¢æ€§èƒ½å¯è§†åŒ– - ç”Ÿæˆæ ‡å‡†ç‰ˆå’Œæ‰©å±•ç‰ˆ
    tutorial.create_comprehensive_performance_visualization(extended=False)  # æ ‡å‡†ç‰ˆ
    tutorial.create_comprehensive_performance_visualization(extended=True)   # æ‰©å±•ç‰ˆ
    
    # 6. åˆ›å»ºCausalEngineæ¨¡å¼ä¸“é¡¹å¯¹æ¯”
    tutorial.create_causal_modes_comparison()
    
    # 7. æ‰“å°å…¨é¢æ€»ç»“æŠ¥å‘Š
    tutorial.print_comprehensive_summary()
    
    print("\nğŸ‰ å…¨é¢CausalEngineæ¨¡å¼æ•™ç¨‹å®Œæˆï¼(Sklearn-Styleç‰ˆæœ¬)")
    print("ğŸ“‹ å®éªŒæ€»ç»“:")
    print(f"   - ä½¿ç”¨äº†çœŸå®ä¸–ç•Œçš„åŠ å·æˆ¿ä»·æ•°æ®é›† ({tutorial.X.shape[0]:,} æ ·æœ¬)")
    print(f"   - æµ‹è¯•äº†æ‰€æœ‰ {len(config.CAUSAL_MODES)} ç§CausalEngineæ¨ç†æ¨¡å¼")
    print(f"   - ä¸ä¼ ç»Ÿæ–¹æ³•è¿›è¡Œäº†å…¨é¢å¯¹æ¯”")
    print(f"   - åœ¨ {config.ANOMALY_RATIO:.0%} æ ‡ç­¾å™ªå£°ç¯å¢ƒä¸‹éªŒè¯äº†é²æ£’æ€§")
    print(f"   - ä½¿ç”¨sklearn-styleå®ç°ï¼Œä¸Legacyç‰ˆæœ¬å½¢æˆå¯¹æ¯”")
    
    print("\nğŸ“Š ç”Ÿæˆçš„æ–‡ä»¶:")
    if config.SAVE_PLOTS:
        print(f"   - {config.OUTPUT_DIR}/sklearn_style_data_analysis.png                   (æ•°æ®åˆ†æå›¾)")
        print(f"   - {config.OUTPUT_DIR}/sklearn_style_performance_comparison.png          (æ ‡å‡†æ€§èƒ½å¯¹æ¯”å›¾)")
        print(f"   - {config.OUTPUT_DIR}/sklearn_style_performance_comparison_extended.png (æ‰©å±•æ€§èƒ½å¯¹æ¯”å›¾)")
        print(f"   - {config.OUTPUT_DIR}/sklearn_style_causal_modes_comparison.png         (CausalEngineæ¨¡å¼ä¸“é¡¹å¯¹æ¯”å›¾)")
    
    print("\nğŸ’¡ æç¤ºï¼šé€šè¿‡ä¿®æ”¹SklearnStyleTutorialConfigç±»æ¥è‡ªå®šä¹‰å®éªŒå‚æ•°ï¼")
    print("ğŸ”¬ å¯¹æ¯”å»ºè®®ï¼šè¿è¡ŒLegacyç‰ˆæœ¬çš„comprehensive_causal_modes_tutorial.pyè¿›è¡Œæ€§èƒ½å¯¹æ¯”")


if __name__ == "__main__":
    main()