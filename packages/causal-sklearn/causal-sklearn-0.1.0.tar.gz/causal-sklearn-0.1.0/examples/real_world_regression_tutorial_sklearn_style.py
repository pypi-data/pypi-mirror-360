#!/usr/bin/env python3
"""
ğŸ  çœŸå®ä¸–ç•Œå›å½’æ•™ç¨‹ï¼šåŠ å·æˆ¿ä»·é¢„æµ‹ - Sklearn-Styleç‰ˆæœ¬
====================================================

è¿™ä¸ªæ•™ç¨‹æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨CausalEngineåœ¨çœŸå®ä¸–ç•Œå›å½’ä»»åŠ¡ä¸­å–å¾—ä¼˜äºä¼ ç»ŸMLæ–¹æ³•çš„æ€§èƒ½ã€‚

æ•°æ®é›†ï¼šåŠ å·æˆ¿ä»·æ•°æ®é›†ï¼ˆCalifornia Housing Datasetï¼‰
- 20,640ä¸ªæ ·æœ¬
- 8ä¸ªç‰¹å¾ï¼ˆæˆ¿å±‹å¹´é¾„ã€æ”¶å…¥ã€äººå£ç­‰ï¼‰
- ç›®æ ‡ï¼šé¢„æµ‹æˆ¿ä»·ä¸­ä½æ•°

æˆ‘ä»¬å°†æ¯”è¾ƒï¼š
1. sklearn MLPRegressorï¼ˆä¼ ç»Ÿç¥ç»ç½‘ç»œï¼‰
2. PyTorch MLPï¼ˆä¼ ç»Ÿæ·±åº¦å­¦ä¹ ï¼‰
3. CausalEngineï¼ˆå› æœæ¨ç†æ–¹æ³•ï¼‰

å…³é”®äº®ç‚¹ï¼š
- çœŸå®ä¸–ç•Œæ•°æ®çš„é²æ£’æ€§
- å¤„ç†å¼‚å¸¸å€¼çš„èƒ½åŠ›
- å› æœæ¨ç†å¸¦æ¥çš„æ€§èƒ½æå‡
- ä½¿ç”¨sklearn-style regressorå®ç°

å®éªŒè®¾è®¡è¯´æ˜
==================================================================
æœ¬è„šæœ¬åŒ…å«ä¸¤ç»„æ ¸å¿ƒå®éªŒï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°CausalEngineåœ¨çœŸå®å›å½’ä»»åŠ¡ä¸Šçš„æ€§èƒ½å’Œé²æ£’æ€§ã€‚
æ‰€æœ‰å®éªŒå‚æ•°å‡å¯åœ¨ä¸‹æ–¹çš„ `TutorialConfig` ç±»ä¸­è¿›è¡Œä¿®æ”¹ã€‚

å®éªŒä¸€ï¼šæ ¸å¿ƒæ€§èƒ½å¯¹æ¯” (åœ¨40%æ ‡ç­¾å™ªå£°ä¸‹)
--------------------------------------------------
- **ç›®æ ‡**: æ¯”è¾ƒCausalEngineå’Œä¼ ç»Ÿæ–¹æ³•åœ¨å«æœ‰å›ºå®šå™ªå£°æ•°æ®ä¸Šçš„é¢„æµ‹æ€§èƒ½ã€‚
- **è®¾ç½®**: é»˜è®¤è®¾ç½®40%çš„æ ‡ç­¾å™ªå£°ï¼ˆ`ANOMALY_RATIO = 0.4`ï¼‰ï¼Œæ¨¡æ‹ŸçœŸå®ä¸–ç•Œä¸­å¸¸è§çš„æ•°æ®è´¨é‡é—®é¢˜ã€‚
- **å¯¹æ¯”æ¨¡å‹**: 
  - CausalEngine (ä¸åŒæ¨¡å¼, å¦‚ 'deterministic', 'standard')
  - Sklearn MLPRegressor
  - PyTorch MLP

å®éªŒäºŒï¼šé²æ£’æ€§åˆ†æ (è·¨è¶Šä¸åŒå™ªå£°æ°´å¹³)
--------------------------------------------------
- **ç›®æ ‡**: æ¢ç©¶æ¨¡å‹æ€§èƒ½éšæ ‡ç­¾å™ªå£°æ°´å¹³å¢åŠ æ—¶çš„å˜åŒ–æƒ…å†µï¼Œè¯„ä¼°å…¶ç¨³å®šæ€§ã€‚
- **è®¾ç½®**: åœ¨ä¸€ç³»åˆ—å™ªå£°æ¯”ä¾‹ï¼ˆå¦‚0%, 10%, 20%, 30%ï¼‰ä¸‹åˆ†åˆ«è¿è¡Œæµ‹è¯•ã€‚
- **å¯¹æ¯”æ¨¡å‹**:
  - CausalEngine ('standard'æ¨¡å¼)
  - Sklearn MLPRegressor
  - PyTorch MLP
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, median_absolute_error
from sklearn.neural_network import MLPRegressor
import warnings
import os
import sys
import time

# è®¾ç½®matplotlibåç«¯ä¸ºéäº¤äº’å¼ï¼Œé¿å…å¼¹å‡ºçª—å£
plt.switch_backend('Agg')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥sklearn-styleå®ç°
from causal_sklearn.regressor import MLPCausalRegressor, MLPPytorchRegressor
from causal_sklearn.data_processing import inject_shuffle_noise

warnings.filterwarnings('ignore')


class TutorialConfig:
    """
    æ•™ç¨‹é…ç½®ç±» - æ–¹ä¾¿è°ƒæ•´å„ç§å‚æ•°
    
    ğŸ”§ åœ¨è¿™é‡Œä¿®æ”¹å‚æ•°æ¥è‡ªå®šä¹‰å®éªŒè®¾ç½®ï¼
    """
    
    # ğŸ¯ æ•°æ®åˆ†å‰²å‚æ•°
    TEST_SIZE = 0.2          # æµ‹è¯•é›†æ¯”ä¾‹
    VAL_SIZE = 0.2           # éªŒè¯é›†æ¯”ä¾‹ (ç›¸å¯¹äºè®­ç»ƒé›†)
    RANDOM_STATE = 42        # éšæœºç§å­
    
    # ğŸ§  ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½® - æ‰€æœ‰ç¥ç»ç½‘ç»œæ–¹æ³•ä½¿ç”¨ç›¸åŒå‚æ•°ç¡®ä¿å…¬å¹³æ¯”è¾ƒ
    # =========================================================================
    # ğŸ”§ åœ¨è¿™é‡Œä¿®æ”¹æ‰€æœ‰ç¥ç»ç½‘ç»œæ–¹æ³•çš„å…±åŒå‚æ•°ï¼
    NN_HIDDEN_SIZES = (128, 64, 32)                 # ç¥ç»ç½‘ç»œéšè—å±‚ç»“æ„
    NN_MAX_EPOCHS = 3000                            # æœ€å¤§è®­ç»ƒè½®æ•°
    NN_LEARNING_RATE = 0.01                         # å­¦ä¹ ç‡
    NN_PATIENCE = 200                               # æ—©åœpatience
    NN_TOLERANCE = 1e-4                             # æ—©åœtolerance
    # =========================================================================
    
    # ğŸ¤– CausalEngineå‚æ•° - ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    CAUSAL_MODES = ['deterministic', 'standard']  # å¯é€‰: ['deterministic', 'exogenous', 'endogenous', 'standard', 'sampling']
    CAUSAL_HIDDEN_SIZES = NN_HIDDEN_SIZES          # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    CAUSAL_MAX_EPOCHS = NN_MAX_EPOCHS              # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    CAUSAL_LR = NN_LEARNING_RATE                   # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    CAUSAL_PATIENCE = NN_PATIENCE                  # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    CAUSAL_TOL = NN_TOLERANCE                      # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    CAUSAL_GAMMA_INIT = 1.0                      # gammaåˆå§‹åŒ–
    CAUSAL_B_NOISE_INIT = 1.0                    # b_noiseåˆå§‹åŒ–
    CAUSAL_B_NOISE_TRAINABLE = True              # b_noiseæ˜¯å¦å¯è®­ç»ƒ
    
    # ğŸ§  ä¼ ç»Ÿæ–¹æ³•å‚æ•° - ä½¿ç”¨ç»Ÿä¸€é…ç½®
    SKLEARN_HIDDEN_LAYERS = NN_HIDDEN_SIZES         # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    SKLEARN_MAX_ITER = NN_MAX_EPOCHS                # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    SKLEARN_LR = NN_LEARNING_RATE                   # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    
    PYTORCH_HIDDEN_SIZES = NN_HIDDEN_SIZES          # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    PYTORCH_EPOCHS = NN_MAX_EPOCHS                  # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    PYTORCH_LR = NN_LEARNING_RATE                   # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    PYTORCH_PATIENCE = NN_PATIENCE                  # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    
    # ğŸ“Š å®éªŒå‚æ•°
    ANOMALY_RATIO = 0.4                          # æ ‡ç­¾å¼‚å¸¸æ¯”ä¾‹ (æ ¸å¿ƒå®éªŒé»˜è®¤å€¼: 40%å™ªå£°æŒ‘æˆ˜)
    SAVE_PLOTS = True                            # æ˜¯å¦ä¿å­˜å›¾è¡¨
    VERBOSE = True                               # æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†è¾“å‡º
    
    # ğŸ›¡ï¸ é²æ£’æ€§æµ‹è¯•å‚æ•° - è®¾è®¡ä¸ºéªŒè¯"CausalEngineé²æ£’æ€§ä¼˜åŠ¿"çš„å‡è®¾
    ROBUSTNESS_ANOMALY_RATIOS = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]  # 6ä¸ªå…³é”®å™ªå£°æ°´å¹³å°±è¶³å¤Ÿ
    RUN_ROBUSTNESS_TEST = True                   # æ˜¯å¦è¿è¡Œé²æ£’æ€§æµ‹è¯•
    
    # ğŸ“ˆ å¯è§†åŒ–å‚æ•°
    FIGURE_DPI = 300                             # å›¾è¡¨åˆ†è¾¨ç‡
    FIGURE_SIZE_ANALYSIS = (24, 20)              # æ•°æ®åˆ†æå›¾è¡¨å¤§å°
    FIGURE_SIZE_PERFORMANCE = (24, 20)           # æ€§èƒ½å¯¹æ¯”å›¾è¡¨å¤§å°
    FIGURE_SIZE_ROBUSTNESS = (24, 20)            # é²æ£’æ€§æµ‹è¯•å›¾è¡¨å¤§å° (4ä¸ªå­å›¾)
    
    # ğŸ“ è¾“å‡ºç›®å½•å‚æ•°
    OUTPUT_DIR = "results/california_housing_regression_sklearn_style"  # è¾“å‡ºç›®å½•åç§°


class CaliforniaHousingTutorialSklearnStyle:
    """
    åŠ å·æˆ¿ä»·å›å½’æ•™ç¨‹ç±» - Sklearn-Styleç‰ˆæœ¬
    
    æ¼”ç¤ºCausalEngineåœ¨çœŸå®ä¸–ç•Œå›å½’ä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§èƒ½
    """
    
    def __init__(self, config=None):
        self.config = config if config is not None else TutorialConfig()
        self.X = None
        self.y = None
        self.feature_names = None
        self.results = {}
        self._ensure_output_dir()
    
    def _ensure_output_dir(self):
        """ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨"""
        if not os.path.exists(self.config.OUTPUT_DIR):
            os.makedirs(self.config.OUTPUT_DIR)
            print(f"ğŸ“ åˆ›å»ºè¾“å‡ºç›®å½•: {self.config.OUTPUT_DIR}/")
    
    def _get_output_path(self, filename):
        """è·å–è¾“å‡ºæ–‡ä»¶çš„å®Œæ•´è·¯å¾„"""
        return os.path.join(self.config.OUTPUT_DIR, filename)
        
    def load_and_explore_data(self, verbose=True):
        """åŠ è½½å¹¶æ¢ç´¢åŠ å·æˆ¿ä»·æ•°æ®é›†"""
        if verbose:
            print("ğŸ  åŠ å·æˆ¿ä»·é¢„æµ‹ - çœŸå®ä¸–ç•Œå›å½’æ•™ç¨‹ (Sklearn-Style)")
            print("=" * 70)
            print("ğŸ“Š æ­£åœ¨åŠ è½½åŠ å·æˆ¿ä»·æ•°æ®é›†...")
        
        # åŠ è½½æ•°æ®
        housing = fetch_california_housing()
        self.X, self.y = housing.data, housing.target
        self.feature_names = housing.feature_names
        
        if verbose:
            print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ")
            print(f"   - æ ·æœ¬æ•°é‡: {self.X.shape[0]:,}")
            print(f"   - ç‰¹å¾æ•°é‡: {self.X.shape[1]}")
            print(f"   - ç‰¹å¾åç§°: {', '.join(self.feature_names)}")
            print(f"   - ç›®æ ‡èŒƒå›´: ${self.y.min():.2f} - ${self.y.max():.2f} (ç™¾ä¸‡ç¾å…ƒ)")
            print(f"   - ç›®æ ‡å‡å€¼: ${self.y.mean():.2f}")
            print(f"   - ç›®æ ‡æ ‡å‡†å·®: ${self.y.std():.2f}")
        
        return self.X, self.y
    
    def visualize_data(self, save_plots=None):
        """æ•°æ®å¯è§†åŒ–åˆ†æ"""
        if save_plots is None:
            save_plots = self.config.SAVE_PLOTS
            
        print("\\nğŸ“ˆ æ•°æ®åˆ†å¸ƒåˆ†æ")
        print("-" * 30)
        
        # åˆ›å»ºå›¾å½¢
        fig, axes = plt.subplots(2, 2, figsize=self.config.FIGURE_SIZE_ANALYSIS)
        fig.suptitle('California Housing Dataset Analysis - Sklearn-Style Tutorial', fontsize=16, fontweight='bold')
        
        # 1. ç›®æ ‡å˜é‡åˆ†å¸ƒ
        axes[0, 0].hist(self.y, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
        axes[0, 0].set_title('House Price Distribution')
        axes[0, 0].set_xlabel('House Price ($100k)')
        axes[0, 0].set_ylabel('Frequency')
        axes[0, 0].axvline(self.y.mean(), color='red', linestyle='--', label=f'Mean: ${self.y.mean():.2f}')
        axes[0, 0].legend()
        
        # 2. ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾
        df = pd.DataFrame(self.X, columns=self.feature_names)
        df['MedHouseVal'] = self.y
        corr_matrix = df.corr()
        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, 
                   square=True, ax=axes[0, 1], cbar_kws={'shrink': 0.8})
        axes[0, 1].set_title('Feature Correlation Matrix')
        
        # 3. ç‰¹å¾åˆ†å¸ƒç®±çº¿å›¾
        df_features = pd.DataFrame(self.X, columns=self.feature_names)
        df_features_normalized = (df_features - df_features.mean()) / df_features.std()
        df_features_normalized.boxplot(ax=axes[1, 0])
        axes[1, 0].set_title('Feature Distribution (Standardized)')
        axes[1, 0].set_xlabel('Features')
        axes[1, 0].set_ylabel('Standardized Values')
        axes[1, 0].tick_params(axis='x', rotation=45)
        
        # 4. æœ€é‡è¦ç‰¹å¾ä¸ç›®æ ‡çš„æ•£ç‚¹å›¾
        most_corr_feature = corr_matrix['MedHouseVal'].abs().sort_values(ascending=False).index[1]
        most_corr_idx = list(self.feature_names).index(most_corr_feature)
        axes[1, 1].scatter(self.X[:, most_corr_idx], self.y, alpha=0.5, s=1)
        axes[1, 1].set_title(f'Most Correlated Feature: {most_corr_feature}')
        axes[1, 1].set_xlabel(most_corr_feature)
        axes[1, 1].set_ylabel('House Price ($100k)')
        
        plt.tight_layout()
        
        if save_plots:
            output_path = self._get_output_path('california_housing_analysis_sklearn_style.png')
            plt.savefig(output_path, dpi=self.config.FIGURE_DPI, bbox_inches='tight')
            print(f"ğŸ“Š æ•°æ®åˆ†æå›¾è¡¨å·²ä¿å­˜ä¸º {output_path}")
        
        plt.close()  # å…³é—­å›¾å½¢ï¼Œé¿å…å†…å­˜æ³„æ¼
        
        # æ•°æ®ç»Ÿè®¡æ‘˜è¦
        print("\\nğŸ“‹ æ•°æ®ç»Ÿè®¡æ‘˜è¦:")
        print(f"  - æœ€ç›¸å…³ç‰¹å¾: {most_corr_feature} (ç›¸å…³ç³»æ•°: {corr_matrix.loc[most_corr_feature, 'MedHouseVal']:.3f})")
        print(f"  - å¼‚å¸¸å€¼æ£€æµ‹: {np.sum(np.abs(self.y - self.y.mean()) > 3 * self.y.std())} ä¸ªæ½œåœ¨å¼‚å¸¸å€¼")
        print(f"  - æ•°æ®å®Œæ•´æ€§: æ— ç¼ºå¤±å€¼" if not np.any(np.isnan(self.X)) else "  - è­¦å‘Š: å­˜åœ¨ç¼ºå¤±å€¼")
    
    def _prepare_data(self, test_size, val_size, anomaly_ratio, random_state):
        """å‡†å¤‡è®­ç»ƒå’Œæµ‹è¯•æ•°æ®"""
        # æ•°æ®åˆ†å‰²
        X_temp, X_test, y_temp, y_test = train_test_split(
            self.X, self.y, test_size=test_size, random_state=random_state
        )
        
        X_train, X_val, y_train, y_val = train_test_split(
            X_temp, y_temp, test_size=val_size, random_state=random_state
        )
        
        # æ³¨å…¥å™ªå£°
        if anomaly_ratio > 0:
            y_train_noisy, noise_indices = inject_shuffle_noise(
                y_train, noise_ratio=anomaly_ratio, random_state=random_state
            )
            y_train = y_train_noisy
            if self.config.VERBOSE:
                print(f"   å¼‚å¸¸æ³¨å…¥: {anomaly_ratio:.1%} ({len(noise_indices)}/{len(y_train)} æ ·æœ¬å—å½±å“)")
        
        # æ ‡å‡†åŒ–
        scaler_X = StandardScaler()
        scaler_y = StandardScaler()
        
        X_train_scaled = scaler_X.fit_transform(X_train)
        X_val_scaled = scaler_X.transform(X_val)
        X_test_scaled = scaler_X.transform(X_test)
        
        y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()
        y_val_scaled = scaler_y.transform(y_val.reshape(-1, 1)).flatten()
        
        return {
            'X_train': X_train_scaled, 'X_val': X_val_scaled, 'X_test': X_test_scaled,
            'y_train': y_train_scaled, 'y_val': y_val_scaled, 'y_test': y_test,
            'scaler_X': scaler_X, 'scaler_y': scaler_y
        }
    
    def _train_sklearn_model(self, data):
        """è®­ç»ƒsklearnæ¨¡å‹"""
        print("ğŸ”§ è®­ç»ƒ sklearn MLPRegressor...")
        
        model = MLPRegressor(
            hidden_layer_sizes=self.config.SKLEARN_HIDDEN_LAYERS,
            max_iter=self.config.SKLEARN_MAX_ITER,
            learning_rate_init=self.config.SKLEARN_LR,
            early_stopping=True,
            validation_fraction=self.config.VAL_SIZE,  # ä¸BaselineBenchmarkä¸€è‡´
            n_iter_no_change=self.config.NN_PATIENCE,
            tol=self.config.NN_TOLERANCE,
            random_state=self.config.RANDOM_STATE
        )
        
        model.fit(data['X_train'], data['y_train'])
        
        if self.config.VERBOSE:
            print(f"   è®­ç»ƒå®Œæˆ: {model.n_iter_} epochs")
        
        return model
    
    def _train_pytorch_model(self, data):
        """è®­ç»ƒPyTorchæ¨¡å‹"""
        print("ğŸ”§ è®­ç»ƒ PyTorch MLPRegressor...")
        
        model = MLPPytorchRegressor(
            hidden_layer_sizes=self.config.PYTORCH_HIDDEN_SIZES,
            max_iter=self.config.PYTORCH_EPOCHS,
            learning_rate=self.config.PYTORCH_LR,
            early_stopping=True,
            validation_fraction=self.config.VAL_SIZE,  # ä¸BaselineBenchmarkä¸€è‡´
            n_iter_no_change=self.config.PYTORCH_PATIENCE,
            tol=self.config.NN_TOLERANCE,
            alpha=0.0,  # ç»Ÿä¸€æ— æ­£åˆ™åŒ–
            batch_size=None,  # å…¨æ‰¹æ¬¡è®­ç»ƒï¼Œä¸BaselineBenchmarkä¸€è‡´
            random_state=self.config.RANDOM_STATE,
            verbose=False
        )
        
        model.fit(data['X_train'], data['y_train'])
        
        print(f"   è®­ç»ƒå®Œæˆ: {model.n_iter_} epochs")
        return model
    
    def _train_causal_model(self, data, mode):
        """è®­ç»ƒCausalEngineæ¨¡å‹"""
        print(f"ğŸ”§ è®­ç»ƒ CausalEngine ({mode})...")
        
        model = MLPCausalRegressor(
            perception_hidden_layers=self.config.CAUSAL_HIDDEN_SIZES,
            mode=mode,
            max_iter=self.config.CAUSAL_MAX_EPOCHS,
            learning_rate=self.config.CAUSAL_LR,
            early_stopping=True,
            validation_fraction=self.config.VAL_SIZE,  # ä¸BaselineBenchmarkä¸€è‡´
            n_iter_no_change=self.config.CAUSAL_PATIENCE,
            tol=self.config.CAUSAL_TOL,
            gamma_init=self.config.CAUSAL_GAMMA_INIT,
            b_noise_init=self.config.CAUSAL_B_NOISE_INIT,
            b_noise_trainable=self.config.CAUSAL_B_NOISE_TRAINABLE,
            alpha=0.0,  # ä¸BaselineBenchmarkä¸€è‡´çš„æ­£åˆ™åŒ–
            batch_size=None,  # å…¨æ‰¹æ¬¡è®­ç»ƒï¼Œä¸BaselineBenchmarkä¸€è‡´
            random_state=self.config.RANDOM_STATE,
            verbose=self.config.VERBOSE
        )
        
        model.fit(data['X_train'], data['y_train'])
        
        if self.config.VERBOSE:
            print(f"   è®­ç»ƒå®Œæˆ: {model.n_iter_} epochs")
        
        return model
    
    def _evaluate_model(self, model, data, model_name):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        # é¢„æµ‹
        val_pred_scaled = model.predict(data['X_val'])
        test_pred_scaled = model.predict(data['X_test'])
        
        # è½¬æ¢å›åŸå§‹å°ºåº¦
        val_pred = data['scaler_y'].inverse_transform(val_pred_scaled.reshape(-1, 1)).flatten()
        test_pred = data['scaler_y'].inverse_transform(test_pred_scaled.reshape(-1, 1)).flatten()
        y_val_original = data['scaler_y'].inverse_transform(data['y_val'].reshape(-1, 1)).flatten()
        
        # è®¡ç®—æŒ‡æ ‡
        val_metrics = {
            'MAE': mean_absolute_error(y_val_original, val_pred),
            'MdAE': median_absolute_error(y_val_original, val_pred),
            'RMSE': np.sqrt(mean_squared_error(y_val_original, val_pred)),
            'RÂ²': r2_score(y_val_original, val_pred)
        }
        
        test_metrics = {
            'MAE': mean_absolute_error(data['y_test'], test_pred),
            'MdAE': median_absolute_error(data['y_test'], test_pred),
            'RMSE': np.sqrt(mean_squared_error(data['y_test'], test_pred)),
            'RÂ²': r2_score(data['y_test'], test_pred)
        }
        
        return {'val': val_metrics, 'test': test_metrics}
    
    def run_comprehensive_benchmark(self, test_size=None, val_size=None, anomaly_ratio=None, verbose=None):
        """è¿è¡Œå…¨é¢çš„åŸºå‡†æµ‹è¯•"""
        # ä½¿ç”¨é…ç½®å‚æ•°ä½œä¸ºé»˜è®¤å€¼
        if test_size is None:
            test_size = self.config.TEST_SIZE
        if val_size is None:
            val_size = self.config.VAL_SIZE
        if anomaly_ratio is None:
            anomaly_ratio = self.config.ANOMALY_RATIO
        if verbose is None:
            verbose = self.config.VERBOSE
            
        if verbose:
            print("\\nğŸš€ å¼€å§‹ç»¼åˆåŸºå‡†æµ‹è¯• (Sklearn-Style)")
            print("=" * 70)
            print(f"ğŸ”§ å®éªŒé…ç½®:")
            print(f"   - æµ‹è¯•é›†æ¯”ä¾‹: {test_size:.1%}")
            print(f"   - éªŒè¯é›†æ¯”ä¾‹: {val_size:.1%}")
            print(f"   - å¼‚å¸¸æ ‡ç­¾æ¯”ä¾‹: {anomaly_ratio:.1%}")
            print(f"   - éšæœºç§å­: {self.config.RANDOM_STATE}")
            print(f"   - CausalEngineæ¨¡å¼: {', '.join(self.config.CAUSAL_MODES)}")
            print(f"   - ç½‘ç»œæ¶æ„: {self.config.CAUSAL_HIDDEN_SIZES}")
            print(f"   - æœ€å¤§è®­ç»ƒè½®æ•°: {self.config.CAUSAL_MAX_EPOCHS}")
            print(f"   - æ—©åœpatience: {self.config.CAUSAL_PATIENCE}")
        
        # å‡†å¤‡æ•°æ®
        data = self._prepare_data(test_size, val_size, anomaly_ratio, self.config.RANDOM_STATE)
        
        # è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹
        self.results = {}
        
        # 1. sklearnæ¨¡å‹
        sklearn_model = self._train_sklearn_model(data)
        self.results['sklearn_mlp'] = self._evaluate_model(sklearn_model, data, 'sklearn_mlp')
        
        # 2. PyTorchæ¨¡å‹
        pytorch_model = self._train_pytorch_model(data)
        self.results['pytorch_mlp'] = self._evaluate_model(pytorch_model, data, 'pytorch_mlp')
        
        # 3. CausalEngineæ¨¡å‹
        for mode in self.config.CAUSAL_MODES:
            causal_model = self._train_causal_model(data, mode)
            self.results[mode] = self._evaluate_model(causal_model, data, mode)
        
        if verbose:
            self._print_results(anomaly_ratio)
        
        return self.results
    
    def _print_results(self, anomaly_ratio):
        """æ‰“å°ç»“æœ"""
        print(f"\\nğŸ“Š åŸºå‡†æµ‹è¯•ç»“æœ (å¼‚å¸¸æ¯”ä¾‹: {anomaly_ratio:.0%})")
        print("=" * 120)
        print(f"{'æ–¹æ³•':<20} {'éªŒè¯é›†':<50} {'æµ‹è¯•é›†':<50}")
        print(f"{'':20} {'MAE':<10} {'MdAE':<10} {'RMSE':<10} {'RÂ²':<10} {'MAE':<10} {'MdAE':<10} {'RMSE':<10} {'RÂ²':<10}")
        print("-" * 120)
        
        for method, metrics in self.results.items():
            val_m = metrics['val']
            test_m = metrics['test']
            print(f"{method:<20} {val_m['MAE']:<10.4f} {val_m['MdAE']:<10.4f} {val_m['RMSE']:<10.4f} {val_m['RÂ²']:<10.4f} "
                  f"{test_m['MAE']:<10.4f} {test_m['MdAE']:<10.4f} {test_m['RMSE']:<10.4f} {test_m['RÂ²']:<10.4f}")
        
        print("=" * 120)
    
    def analyze_performance(self, verbose=True):
        """åˆ†ææ€§èƒ½ç»“æœ"""
        if not self.results:
            print("âŒ è¯·å…ˆè¿è¡ŒåŸºå‡†æµ‹è¯•")
            return
        
        if verbose:
            print("\\nğŸ” æ€§èƒ½åˆ†æ")
            print("=" * 60)
        
        # æå–æµ‹è¯•é›†RÂ²åˆ†æ•°
        test_r2_scores = {}
        for method, metrics in self.results.items():
            test_r2_scores[method] = metrics['test']['RÂ²']
        
        # æ‰¾åˆ°æœ€ä½³æ–¹æ³•
        best_method = max(test_r2_scores.keys(), key=lambda x: test_r2_scores[x])
        best_r2 = test_r2_scores[best_method]
        
        if verbose:
            print(f"ğŸ† æœ€ä½³æ–¹æ³•: {best_method}")
            print(f"   RÂ² = {best_r2:.4f}")
            print()
            print("ğŸ“Š æ€§èƒ½æ’å (æŒ‰RÂ²åˆ†æ•°):")
            
            sorted_methods = sorted(test_r2_scores.items(), key=lambda x: x[1], reverse=True)
            for i, (method, r2) in enumerate(sorted_methods, 1):
                improvement = ((r2 - sorted_methods[-1][1]) / abs(sorted_methods[-1][1])) * 100
                print(f"   {i}. {method:<15} RÂ² = {r2:.4f} (+ {improvement:+.1f}%)")
        
        # CausalEngineæ€§èƒ½åˆ†æ
        causal_methods = [m for m in self.results.keys() if m in ['deterministic', 'standard', 'sampling', 'exogenous', 'endogenous']]
        if causal_methods:
            best_causal = max(causal_methods, key=lambda x: test_r2_scores[x])
            traditional_methods = [m for m in self.results.keys() if m in ['sklearn_mlp', 'pytorch_mlp']]
            
            if traditional_methods and verbose:
                best_traditional = max(traditional_methods, key=lambda x: test_r2_scores[x])
                causal_improvement = ((test_r2_scores[best_causal] - test_r2_scores[best_traditional]) 
                                    / abs(test_r2_scores[best_traditional])) * 100
                
                print(f"\\nğŸ¯ CausalEngineä¼˜åŠ¿åˆ†æ:")
                print(f"   æœ€ä½³CausalEngineæ¨¡å¼: {best_causal} (RÂ² = {test_r2_scores[best_causal]:.4f})")
                print(f"   æœ€ä½³ä¼ ç»Ÿæ–¹æ³•: {best_traditional} (RÂ² = {test_r2_scores[best_traditional]:.4f})")
                print(f"   æ€§èƒ½æå‡: {causal_improvement:+.2f}%")
                
                if causal_improvement > 0:
                    print(f"   âœ… CausalEngineæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼")
                else:
                    print(f"   âš ï¸ åœ¨æ­¤æ•°æ®é›†ä¸Šä¼ ç»Ÿæ–¹æ³•è¡¨ç°æ›´å¥½")
        
        return test_r2_scores
    
    def create_performance_visualization(self, save_plot=None):
        """åˆ›å»ºæ€§èƒ½å¯è§†åŒ–å›¾è¡¨"""
        if save_plot is None:
            save_plot = self.config.SAVE_PLOTS
            
        if not self.results:
            print("âŒ è¯·å…ˆè¿è¡ŒåŸºå‡†æµ‹è¯•")
            return
        
        print("\\nğŸ“Š åˆ›å»ºæ€§èƒ½å¯è§†åŒ–å›¾è¡¨")
        print("-" * 30)
        
        # å‡†å¤‡æ•°æ®
        methods = list(self.results.keys())
        metrics = ['MAE', 'MdAE', 'RMSE', 'RÂ²']
        
        # åˆ›å»ºå­å›¾
        fig, axes = plt.subplots(2, 2, figsize=self.config.FIGURE_SIZE_PERFORMANCE)
        fig.suptitle('CausalEngine vs Traditional Methods: California Housing Performance (40% Label Noise) - Sklearn-Style', fontsize=16, fontweight='bold')
        axes = axes.flatten()  # å±•å¹³ä¸ºä¸€ç»´æ•°ç»„ä¾¿äºè®¿é—®
        
        colors = ['skyblue', 'lightcoral', 'lightgreen', 'gold', 'plum']
        
        for i, metric in enumerate(metrics):
            values = [self.results[method]['test'][metric] for method in methods]
            
            bars = axes[i].bar(methods, values, color=colors[:len(methods)], alpha=0.8, edgecolor='black')
            axes[i].set_title(f'{metric} (Test Set)', fontweight='bold')
            axes[i].set_ylabel(metric)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, value in zip(bars, values):
                height = bar.get_height()
                axes[i].text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                           f'{value:.3f}', ha='center', va='bottom', fontweight='bold')
            
            # é«˜äº®æœ€ä½³ç»“æœ
            if metric == 'RÂ²':
                best_idx = values.index(max(values))
            else:
                best_idx = values.index(min(values))
            bars[best_idx].set_color('gold')
            bars[best_idx].set_edgecolor('red')
            bars[best_idx].set_linewidth(2)
            
            axes[i].tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        
        if save_plot:
            output_path = self._get_output_path('california_housing_performance_sklearn_style.png')
            plt.savefig(output_path, dpi=self.config.FIGURE_DPI, bbox_inches='tight')
            print(f"ğŸ“Š æ€§èƒ½å›¾è¡¨å·²ä¿å­˜ä¸º {output_path}")
        
        plt.close()  # å…³é—­å›¾å½¢ï¼Œé¿å…å†…å­˜æ³„æ¼
    
    def run_robustness_test(self, anomaly_ratios=None, verbose=None):
        """è¿è¡Œé²æ£’æ€§æµ‹è¯•ï¼ˆä¸åŒå¼‚å¸¸æ¯”ä¾‹ï¼‰"""
        if anomaly_ratios is None:
            anomaly_ratios = self.config.ROBUSTNESS_ANOMALY_RATIOS
        if verbose is None:
            verbose = self.config.VERBOSE
            
        if verbose:
            print("\\nğŸ›¡ï¸ é²æ£’æ€§æµ‹è¯• (Sklearn-Style)")
            print("=" * 70)
            print("æµ‹è¯•CausalEngineåœ¨ä¸åŒå¼‚å¸¸æ ‡ç­¾æ¯”ä¾‹ä¸‹çš„è¡¨ç°")
        
        robustness_results = {}
        
        for anomaly_ratio in anomaly_ratios:
            if verbose:
                print(f"\\nğŸ”¬ æµ‹è¯•å¼‚å¸¸æ¯”ä¾‹: {anomaly_ratio:.1%}")
                print("-" * 30)
            
            # å‡†å¤‡æ•°æ®
            data = self._prepare_data(0.2, 0.2, anomaly_ratio, self.config.RANDOM_STATE)
            
            # è®­ç»ƒæ¨¡å‹
            results = {}
            
            # sklearnæ¨¡å‹
            sklearn_model = self._train_sklearn_model(data)
            results['sklearn_mlp'] = self._evaluate_model(sklearn_model, data, 'sklearn_mlp')
            
            # PyTorchæ¨¡å‹
            pytorch_model = self._train_pytorch_model(data)
            results['pytorch_mlp'] = self._evaluate_model(pytorch_model, data, 'pytorch_mlp')
            
            # CausalEngineæ¨¡å‹
            for mode in ['deterministic', 'standard']:
                causal_model = self._train_causal_model(data, mode)
                results[mode] = self._evaluate_model(causal_model, data, mode)
            
            robustness_results[anomaly_ratio] = results
            
            if verbose:
                print("RÂ² åˆ†æ•°:")
                for method in ['sklearn_mlp', 'pytorch_mlp', 'deterministic', 'standard']:
                    if method in results:
                        r2 = results[method]['test']['RÂ²']
                        print(f"  {method:<12}: {r2:.4f}")
        
        # å¯è§†åŒ–é²æ£’æ€§ç»“æœ
        if verbose:
            # æ‰“å°è¯¦ç»†çš„é²æ£’æ€§è¡¨æ ¼
            self._print_robustness_table(robustness_results, anomaly_ratios)
            # ç»˜åˆ¶é²æ£’æ€§å›¾è¡¨
            self._plot_robustness_results(robustness_results, anomaly_ratios)
            # åˆ†æé²æ£’æ€§è¶‹åŠ¿
            self._analyze_robustness_trends(robustness_results, anomaly_ratios)
        
        return robustness_results
    
    def _print_robustness_table(self, robustness_results, anomaly_ratios):
        """æ‰“å°é²æ£’æ€§æµ‹è¯•è¯¦ç»†è¡¨æ ¼ - æ˜¾ç¤ºæ‰€æœ‰å™ªå£°æ°´å¹³ä¸‹çš„æ¨¡å‹æ€§èƒ½"""
        print("\\nğŸ“Š é²æ£’æ€§æµ‹è¯•è¯¦ç»†ç»“æœè¡¨æ ¼")
        print("=" * 140)
        
        # è¡¨å¤´
        metrics = ['MAE', 'MdAE', 'RMSE', 'RÂ²']
        methods = ['sklearn_mlp', 'pytorch_mlp', 'deterministic', 'standard']
        
        # æ‰“å°è¡¨å¤´
        header_line1 = f"{'å¼‚å¸¸æ¯”ä¾‹':<8} {'æ–¹æ³•':<12}"
        header_line2 = f"{'':8} {'':12}"
        
        for split in ['éªŒè¯é›†', 'æµ‹è¯•é›†']:
            header_line1 += f" {split:<40}"
            header_line2 += f" {metrics[0]:<9} {metrics[1]:<9} {metrics[2]:<9} {metrics[3]:<9}"
        
        print(header_line1)
        print(header_line2)
        print("-" * 140)
        
        # ä¸ºæ¯ä¸ªå¼‚å¸¸æ¯”ä¾‹æ‰“å°ç»“æœ
        for ratio in anomaly_ratios:
            ratio_str = f"{ratio:.0%}"
            
            for i, method in enumerate(methods):
                if method in robustness_results[ratio]:
                    results = robustness_results[ratio][method]
                    
                    # ç¬¬ä¸€è¡Œæ˜¾ç¤ºå¼‚å¸¸æ¯”ä¾‹ï¼Œåç»­è¡Œä¸ºç©º
                    ratio_display = ratio_str if i == 0 else ""
                    
                    line = f"{ratio_display:<8} {method:<12}"
                    
                    # éªŒè¯é›†æŒ‡æ ‡
                    val_metrics = results['val']
                    line += f" {val_metrics['MAE']:<9.4f} {val_metrics['MdAE']:<9.4f} {val_metrics['RMSE']:<9.4f} {val_metrics['RÂ²']:<9.4f}"
                    
                    # æµ‹è¯•é›†æŒ‡æ ‡
                    test_metrics = results['test']
                    line += f" {test_metrics['MAE']:<9.4f} {test_metrics['MdAE']:<9.4f} {test_metrics['RMSE']:<9.4f} {test_metrics['RÂ²']:<9.4f}"
                    
                    print(line)
            
            # åœ¨æ¯ä¸ªå¼‚å¸¸æ¯”ä¾‹ç»„ä¹‹é—´æ·»åŠ åˆ†éš”çº¿
            if ratio != anomaly_ratios[-1]:
                print("-" * 140)
        
        print("=" * 140)
        print("ğŸ’¡ è§‚å¯Ÿè¦ç‚¹ï¼š")
        print("   - RÂ² è¶Šé«˜è¶Šå¥½ï¼ˆæ¥è¿‘1.0ä¸ºæœ€ä½³ï¼‰")
        print("   - MAE, MdAE, RMSE è¶Šä½è¶Šå¥½ï¼ˆæ¥è¿‘0ä¸ºæœ€ä½³ï¼‰")
        print("   - å…³æ³¨å„æ–¹æ³•åœ¨å¼‚å¸¸æ¯”ä¾‹å¢åŠ æ—¶çš„æ€§èƒ½å˜åŒ–è¶‹åŠ¿")
    
    def _plot_robustness_results(self, robustness_results, anomaly_ratios):
        """ç»˜åˆ¶é²æ£’æ€§æµ‹è¯•ç»“æœ - æ˜¾ç¤ºæ‰€æœ‰4ä¸ªå›å½’æŒ‡æ ‡"""
        fig, axes = plt.subplots(2, 2, figsize=self.config.FIGURE_SIZE_ROBUSTNESS)
        fig.suptitle('Robustness Test: Impact of Label Noise on Model Performance - Sklearn-Style', fontsize=16, fontweight='bold')
        
        methods = ['sklearn_mlp', 'pytorch_mlp', 'deterministic', 'standard']
        method_labels = ['sklearn MLP', 'PyTorch MLP', 'CausalEngine (Det)', 'CausalEngine (Std)']
        colors = ['#1f77b4', '#ff7f0e', '#d62728', '#2ca02c']  # æ›´æ¸…æ™°çš„é¢œè‰²
        markers = ['o', 's', 'v', '^']
        
        # 4ä¸ªå›å½’æŒ‡æ ‡
        metrics = ['MAE', 'MdAE', 'RMSE', 'RÂ²']
        metric_labels = ['Mean Absolute Error (MAE)', 'Median Absolute Error (MdAE)', 'Root Mean Squared Error (RMSE)', 'R-squared Score (RÂ²)']
        
        # ä¸ºæ¯ä¸ªæŒ‡æ ‡åˆ›å»ºå­å›¾
        for idx, (metric, metric_label) in enumerate(zip(metrics, metric_labels)):
            row, col = idx // 2, idx % 2
            ax = axes[row, col]
            
            # ç»˜åˆ¶æ¯ä¸ªæ–¹æ³•çš„æ›²çº¿
            for method, label, color, marker in zip(methods, method_labels, colors, markers):
                scores = []
                for ratio in anomaly_ratios:
                    if method in robustness_results[ratio]:
                        scores.append(robustness_results[ratio][method]['test'][metric])
                    else:
                        scores.append(np.nan)
                
                ax.plot(anomaly_ratios, scores, marker=marker, linewidth=2.5, 
                       markersize=8, label=label, color=color, alpha=0.8)
            
            # è®¾ç½®å­å›¾å±æ€§
            ax.set_xlabel('Label Noise Ratio', fontsize=11)
            ax.set_ylabel(metric, fontsize=11)
            ax.set_title(metric_label, fontsize=12, fontweight='bold')
            ax.grid(True, alpha=0.3)
            ax.legend(fontsize=9)
            
            # ä¸ºRÂ²æ·»åŠ ç‰¹æ®Šå¤„ç†ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰ï¼Œå…¶ä»–æŒ‡æ ‡è¶Šä½è¶Šå¥½
            if metric == 'RÂ²':
                ax.set_ylim(bottom=0)  # RÂ²ä»0å¼€å§‹æ˜¾ç¤º
            else:
                ax.set_ylim(bottom=0)  # è¯¯å·®æŒ‡æ ‡ä»0å¼€å§‹æ˜¾ç¤º
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        output_path = self._get_output_path('california_housing_robustness_sklearn_style.png')
        plt.savefig(output_path, dpi=self.config.FIGURE_DPI, bbox_inches='tight')
        print(f"ğŸ“Š é²æ£’æ€§æµ‹è¯•å›¾è¡¨å·²ä¿å­˜ä¸º {output_path}")
        plt.close()  # å…³é—­å›¾å½¢ï¼Œé¿å…å†…å­˜æ³„æ¼
    
    def _analyze_robustness_trends(self, robustness_results, anomaly_ratios):
        """åˆ†æé²æ£’æ€§è¶‹åŠ¿ - éªŒè¯CausalEngineé²æ£’æ€§å‡è®¾"""
        print("\\nğŸ”¬ é²æ£’æ€§è¶‹åŠ¿åˆ†æ")
        print("=" * 60)
        
        methods = ['sklearn_mlp', 'pytorch_mlp', 'deterministic', 'standard']
        method_names = {'sklearn_mlp': 'sklearn MLP', 'pytorch_mlp': 'PyTorch MLP', 'deterministic': 'CausalEngine (deterministic)', 'standard': 'CausalEngine (standard)'}
        
        # åˆ†æRÂ²æŒ‡æ ‡çš„å˜åŒ–è¶‹åŠ¿
        print("ğŸ“ˆ RÂ²æŒ‡æ ‡éšå¼‚å¸¸æ¯”ä¾‹å˜åŒ–è¶‹åŠ¿ï¼š")
        print("-" * 40)
        
        for method in methods:
            r2_scores = []
            for ratio in anomaly_ratios:
                if method in robustness_results[ratio]:
                    r2_scores.append(robustness_results[ratio][method]['test']['RÂ²'])
                else:
                    r2_scores.append(np.nan)
            
            # è®¡ç®—æ€§èƒ½ä¸‹é™æƒ…å†µ
            clean_r2 = r2_scores[0] if len(r2_scores) > 0 and not np.isnan(r2_scores[0]) else 0
            final_r2 = r2_scores[-1] if len(r2_scores) > 0 and not np.isnan(r2_scores[-1]) else 0
            
            if clean_r2 > 0:
                performance_retention = (final_r2 / clean_r2) * 100
                performance_drop = clean_r2 - final_r2
            else:
                performance_retention = 0
                performance_drop = float('inf')
            
            print(f"  {method_names[method]}:")
            print(f"    - é›¶å¼‚å¸¸æ—¶RÂ²: {clean_r2:.4f}")
            print(f"    - æœ€é«˜å¼‚å¸¸æ—¶RÂ²: {final_r2:.4f}")
            print(f"    - æ€§èƒ½ä¿æŒç‡: {performance_retention:.1f}%")
            print(f"    - ç»å¯¹ä¸‹é™: {performance_drop:.4f}")
        
        # éªŒè¯å‡è®¾
        print("\\nğŸ¯ å‡è®¾éªŒè¯ç»“æœï¼š")
        print("-" * 40)
        
        # æå–å…³é”®æ•°æ®
        clean_performance = {}  # é›¶å¼‚å¸¸æ—¶çš„æ€§èƒ½
        noisy_performance = {}  # é«˜å¼‚å¸¸æ—¶çš„æ€§èƒ½
        
        for method in methods:
            # é›¶å¼‚å¸¸æ€§èƒ½
            if method in robustness_results[0.0]:
                clean_performance[method] = robustness_results[0.0][method]['test']['RÂ²']
            
            # æœ€é«˜å¼‚å¸¸æ€§èƒ½ (é€‰æ‹©0.25æˆ–æœ€åä¸€ä¸ª)
            high_noise_ratio = 0.25 if 0.25 in anomaly_ratios else anomaly_ratios[-1]
            if method in robustness_results[high_noise_ratio]:
                noisy_performance[method] = robustness_results[high_noise_ratio][method]['test']['RÂ²']
        
        # æ£€æŸ¥é›¶å¼‚å¸¸æ—¶æ‰€æœ‰æ¨¡å‹æ˜¯å¦éƒ½è¡¨ç°è‰¯å¥½
        zero_noise_good = all(score > 0.6 for score in clean_performance.values())
        print(f"âœ… å‡è®¾1 - é›¶å¼‚å¸¸æ—¶æ‰€æœ‰æ¨¡å‹è¡¨ç°è‰¯å¥½: {'é€šè¿‡' if zero_noise_good else 'æœªé€šè¿‡'}")
        
        # æ£€æŸ¥CausalEngineæ˜¯å¦ä¿æŒè‰¯å¥½æ€§èƒ½ï¼ˆé€‰æ‹©è¡¨ç°æ›´å¥½çš„æ¨¡å¼ï¼‰
        causal_methods_performance = {}
        for method in ['deterministic', 'standard']:
            if method in noisy_performance:
                causal_methods_performance[method] = noisy_performance[method]
        
        if causal_methods_performance:
            best_causal_performance = max(causal_methods_performance.values())
            best_causal_method = max(causal_methods_performance.keys(), key=lambda x: causal_methods_performance[x])
            causal_robust = best_causal_performance > 0.6
            print(f"âœ… å‡è®¾2 - CausalEngineåœ¨é«˜å™ªå£°ä¸‹ä¿æŒè‰¯å¥½: {'é€šè¿‡' if causal_robust else 'æœªé€šè¿‡'}")
            print(f"   æœ€ä½³CausalEngineæ¨¡å¼: {best_causal_method} (RÂ² = {best_causal_performance:.4f})")
        else:
            causal_robust = False
            print(f"âœ… å‡è®¾2 - CausalEngineåœ¨é«˜å™ªå£°ä¸‹ä¿æŒè‰¯å¥½: æœªé€šè¿‡ (æ— æ•°æ®)")
        
        # æ£€æŸ¥ä¼ ç»Ÿæ–¹æ³•æ€§èƒ½æ˜¯å¦æ€¥å‰§ä¸‹é™
        traditional_degraded = True
        for method in ['sklearn_mlp', 'pytorch_mlp']:
            if method in clean_performance and method in noisy_performance:
                retention_rate = noisy_performance[method] / clean_performance[method]
                if retention_rate > 0.5:  # å¦‚æœä¿æŒç‡è¶…è¿‡50%ï¼Œè®¤ä¸ºæ²¡æœ‰æ€¥å‰§ä¸‹é™
                    traditional_degraded = False
                    break
        
        print(f"âœ… å‡è®¾3 - ä¼ ç»Ÿæ–¹æ³•æ€§èƒ½æ€¥å‰§ä¸‹é™: {'é€šè¿‡' if traditional_degraded else 'æœªé€šè¿‡'}")
        
        # ç»¼åˆç»“è®º
        all_passed = zero_noise_good and causal_robust and traditional_degraded
        print(f"\\nğŸ† ç»¼åˆç»“è®º: {'CausalEngineé²æ£’æ€§ä¼˜åŠ¿å¾—åˆ°éªŒè¯ï¼' if all_passed else 'éœ€è¦è¿›ä¸€æ­¥åˆ†æå®éªŒç»“æœ'}")
        
        if all_passed:
            print("   âœ¨ å®éªŒå®Œç¾è¯æ˜äº†CausalEngineåœ¨çœŸå®ä¸–ç•Œå™ªå£°ç¯å¢ƒä¸­çš„æ˜¾è‘—ä¼˜åŠ¿")
        else:
            print("   âš ï¸ å»ºè®®è°ƒæ•´å®éªŒå‚æ•°æˆ–æ£€æŸ¥æ¨¡å‹é…ç½®")


def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œå®Œæ•´çš„æ•™ç¨‹"""
    print("ğŸ  CausalEngineçœŸå®ä¸–ç•Œå›å½’æ•™ç¨‹ - Sklearn-Styleç‰ˆæœ¬")
    print("ğŸ¯ ç›®æ ‡ï¼šåœ¨åŠ å·æˆ¿ä»·é¢„æµ‹ä»»åŠ¡ä¸­å±•ç¤ºCausalEngineçš„ä¼˜è¶Šæ€§")
    print("=" * 90)
    
    # åˆ›å»ºé…ç½®å®ä¾‹ï¼ˆåœ¨è¿™é‡Œå¯ä»¥è‡ªå®šä¹‰é…ç½®ï¼‰
    config = TutorialConfig()
    
    # ğŸ”§ å¿«é€Ÿé…ç½®ç¤ºä¾‹ - å–æ¶ˆæ³¨é‡Šæ¥ä¿®æ”¹å‚æ•°ï¼š
    # config.CAUSAL_MODES = ['deterministic', 'standard', 'sampling']  # æ·»åŠ æ›´å¤šæ¨¡å¼
    # config.CAUSAL_MAX_EPOCHS = 500  # å‡å°‘è®­ç»ƒè½®æ•°ä»¥åŠ å¿«é€Ÿåº¦
    # config.ANOMALY_RATIO = 0.1      # æ·»åŠ 10%å¼‚å¸¸æ ‡ç­¾
    # config.RUN_ROBUSTNESS_TEST = False  # è·³è¿‡é²æ£’æ€§æµ‹è¯•
    
    print(f"ğŸ”§ å½“å‰é…ç½®:")
    print(f"   - CausalEngineæ¨¡å¼: {', '.join(config.CAUSAL_MODES)}")
    print(f"   - ç½‘ç»œæ¶æ„: {config.CAUSAL_HIDDEN_SIZES}")
    print(f"   - æœ€å¤§è½®æ•°: {config.CAUSAL_MAX_EPOCHS}")
    print(f"   - æ—©åœpatience: {config.CAUSAL_PATIENCE}")
    print(f"   - å¼‚å¸¸æ¯”ä¾‹: {config.ANOMALY_RATIO:.1%}")
    print(f"   - è¿è¡Œé²æ£’æ€§æµ‹è¯•: {'æ˜¯' if config.RUN_ROBUSTNESS_TEST else 'å¦'}")
    print(f"   - è¾“å‡ºç›®å½•: {config.OUTPUT_DIR}/")
    print()
    
    # åˆ›å»ºæ•™ç¨‹å®ä¾‹
    tutorial = CaliforniaHousingTutorialSklearnStyle(config)
    
    # 1. åŠ è½½å’Œæ¢ç´¢æ•°æ®
    tutorial.load_and_explore_data()
    
    # 2. æ•°æ®å¯è§†åŒ–
    tutorial.visualize_data()
    
    # 3. è¿è¡Œç»¼åˆåŸºå‡†æµ‹è¯•
    tutorial.run_comprehensive_benchmark()
    
    # 4. æ€§èƒ½åˆ†æ
    tutorial.analyze_performance()
    
    # 5. åˆ›å»ºæ€§èƒ½å¯è§†åŒ–
    tutorial.create_performance_visualization()
    
    # 6. é²æ£’æ€§æµ‹è¯•ï¼ˆå¯é€‰ï¼‰
    if config.RUN_ROBUSTNESS_TEST:
        tutorial.run_robustness_test()
    else:
        print("\\nğŸ›¡ï¸ è·³è¿‡é²æ£’æ€§æµ‹è¯•ï¼ˆé…ç½®ä¸­ç¦ç”¨ï¼‰")
    
    print("\\nğŸ‰ æ•™ç¨‹å®Œæˆï¼")
    print("ğŸ“‹ æ€»ç»“:")
    print("   - ä½¿ç”¨äº†çœŸå®ä¸–ç•Œçš„åŠ å·æˆ¿ä»·æ•°æ®é›†")
    print(f"   - æ¯”è¾ƒäº†{len(config.CAUSAL_MODES) + 2}ç§ä¸åŒçš„æ–¹æ³•")
    print("   - å±•ç¤ºäº†CausalEngineçš„æ€§èƒ½ä¼˜åŠ¿")
    if config.RUN_ROBUSTNESS_TEST:
        print("   - æµ‹è¯•äº†æ¨¡å‹çš„é²æ£’æ€§")
    print("   - æä¾›äº†è¯¦ç»†çš„å¯è§†åŒ–åˆ†æ")
    print("   - ä½¿ç”¨sklearn-style regressorå®ç°")
    print("\\nğŸ“Š ç”Ÿæˆçš„æ–‡ä»¶:")
    if config.SAVE_PLOTS:
        print(f"   - {config.OUTPUT_DIR}/california_housing_analysis_sklearn_style.png     (æ•°æ®åˆ†æå›¾)")
        print(f"   - {config.OUTPUT_DIR}/california_housing_performance_sklearn_style.png  (æ€§èƒ½å¯¹æ¯”å›¾)")
        if config.RUN_ROBUSTNESS_TEST:
            print(f"   - {config.OUTPUT_DIR}/california_housing_robustness_sklearn_style.png   (é²æ£’æ€§æµ‹è¯•å›¾ - 4ä¸ªæŒ‡æ ‡)")
    
    print("\\nğŸ’¡ æç¤ºï¼šåœ¨è„šæœ¬é¡¶éƒ¨çš„TutorialConfigç±»ä¸­ä¿®æ”¹å‚æ•°æ¥è‡ªå®šä¹‰å®éªŒï¼")


if __name__ == "__main__":
    main()