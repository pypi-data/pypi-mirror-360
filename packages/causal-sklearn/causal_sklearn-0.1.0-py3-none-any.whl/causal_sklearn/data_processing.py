import numpy as np
import pandas as pd
from typing import Tuple, Dict, Any, Optional
from sklearn.datasets import (
    fetch_california_housing, 
    load_diabetes, 
    load_linnerud,
    fetch_openml
)
from sklearn.preprocessing import StandardScaler, LabelEncoder
import warnings

def inject_shuffle_noise(
    y: np.ndarray,
    noise_ratio: float,
    random_state: int = None
) -> Tuple[np.ndarray, np.ndarray]:
    """
    å¯¹ä¸€éƒ¨åˆ†æ ·æœ¬çš„æ ‡ç­¾è¿›è¡Œå…¨å±€éšæœºæ´—ç‰Œï¼Œä»¥åˆ‡æ–­å…¶ä¸ç‰¹å¾çš„å› æœå…³ç³»ã€‚
    è¿™æ˜¯é¡¹ç›®MVPé˜¶æ®µå”¯ä¸€çš„å› æœæ ‡ç­¾å¼‚å¸¸æ³¨å…¥é€»è¾‘ã€‚

    è¯¥æ–¹æ³•åŸºäºä¸€ä¸ªç†è®ºä¸Šæ›´ä¼˜çš„ç­–ç•¥ï¼š
    1. åˆ›å»ºä¸€ä¸ªå…¨å±€æ´—ç‰Œåçš„æ ‡ç­¾å‰¯æœ¬ y_shuffledã€‚
    2. éšæœºé€‰æ‹©ä¸€éƒ¨åˆ†ç´¢å¼•ã€‚
    3. å°†è¿™äº›ç´¢å¼•ä½ç½®çš„åŸå§‹æ ‡ç­¾æ›¿æ¢ä¸º y_shuffled ä¸­å¯¹åº”ä½ç½®çš„æ ‡ç­¾ã€‚
    è¿™ç¡®ä¿äº†è¢«æ±¡æŸ“çš„æ ‡ç­¾ä¸åŸå§‹ç‰¹å¾å®Œå…¨æ— å…³ã€‚

    Args:
        y: åŸå§‹æ ‡ç­¾æ•°ç»„ã€‚
        noise_ratio: éœ€è¦æ³¨å…¥å™ªå£°çš„æ ·æœ¬æ¯”ä¾‹ (0.0 åˆ° 1.0)ã€‚
        random_state: éšæœºç§å­ï¼Œç”¨äºå¤ç°ã€‚

    Returns:
        A tuple containing:
        - y_noisy (np.ndarray): æ³¨å…¥äº†æ´—ç‰Œå™ªå£°çš„æ ‡ç­¾æ•°ç»„ã€‚
        - noise_indices (np.ndarray): è¢«æ³¨å…¥å™ªå£°çš„æ ·æœ¬çš„åŸå§‹ç´¢å¼•ã€‚
    """
    if random_state is not None:
        np.random.seed(random_state)

    if not (0.0 <= noise_ratio <= 1.0):
        raise ValueError("noise_ratio å¿…é¡»åœ¨ 0.0 å’Œ 1.0 ä¹‹é—´")

    n_samples = len(y)
    if noise_ratio == 0 or n_samples == 0:
        return y.copy(), np.array([], dtype=int)

    # 1. åˆ›å»ºä¸€ä¸ªå…¨å±€æ´—ç‰Œåçš„æ ‡ç­¾å‰¯æœ¬ y'
    y_shuffled = y.copy()
    np.random.shuffle(y_shuffled)

    # 2. éšæœºé€‰æ‹©è¦æ±¡æŸ“çš„ç´¢å¼•
    n_noisy = int(n_samples * noise_ratio)
    noise_indices = np.random.choice(n_samples, size=n_noisy, replace=False)

    # 3. åˆ›å»ºä¸€ä¸ªæ–°çš„ y_noisy å‘é‡
    y_noisy = y.copy()
    
    # 4. åœ¨é€‰å®šçš„ç´¢å¼•å¤„ï¼Œç”¨ y' çš„å€¼æ›¿æ¢ y çš„å€¼
    y_noisy[noise_indices] = y_shuffled[noise_indices]
    
    return y_noisy, noise_indices

# =============================================================================
# æ‰©å±•å›å½’æ•°æ®é›†åŠ è½½åŠŸèƒ½
# =============================================================================

# æ‰©å±•å›å½’æ•°æ®é›†é…ç½®
EXTENDED_REGRESSION_DATASETS = {
    # sklearnå†…ç½®æ•°æ®é›†
    'california_housing': {
        'source': 'sklearn',
        'loader': fetch_california_housing,
        'name': 'California Housing',
        'description': 'California housing prices (20,640 samples, 8 features)',
        'n_samples': 20640,
        'n_features': 8,
        'target_type': 'continuous'
    },
    'diabetes': {
        'source': 'sklearn',
        'loader': load_diabetes,
        'name': 'Diabetes',
        'description': 'Diabetes progression (442 samples, 10 features)',
        'n_samples': 442,
        'n_features': 10,
        'target_type': 'continuous'
    },
    'linnerud': {
        'source': 'sklearn',
        'loader': load_linnerud,
        'name': 'Linnerud Physical Exercise',
        'description': 'Physical exercise physiological data (20 samples, 3 features)',
        'n_samples': 20,
        'n_features': 3,
        'target_type': 'continuous',
        'multi_output': True
    },
    
    # OpenMLæ•°æ®é›†
    'boston_housing': {
        'source': 'openml',
        'name': 'Boston Housing (OpenML)',
        'description': 'Boston housing prices from OpenML (506 samples, 13 features)', 
        'openml_id': 531,  # Boston housing dataset ID
        'target_column': 'medv',
        'n_samples': 506,
        'n_features': 13,
        'target_type': 'continuous'
    },
    'auto_mpg': {
        'source': 'openml',
        'name': 'Auto MPG',
        'description': 'Auto fuel efficiency prediction (392 samples, 7 features)',
        'openml_id': 196,
        'target_column': 'mpg',
        'n_samples': 392,
        'n_features': 7,
        'target_type': 'continuous'
    },
    'wine_quality_red': {
        'source': 'openml',
        'name': 'Wine Quality (Red)',
        'description': 'Red wine quality rating (1599 samples, 11 features)',
        'openml_id': 287,
        'target_column': 'quality',
        'n_samples': 1599,
        'n_features': 11,
        'target_type': 'continuous'
    },
    'wine_quality_white': {
        'source': 'openml',
        'name': 'Wine Quality (White)',
        'description': 'White wine quality rating (4898 samples, 11 features)',
        'openml_id': 1497,
        'target_column': 'quality',
        'n_samples': 4898,
        'n_features': 11,
        'target_type': 'continuous'
    },
    'concrete_strength': {
        'source': 'openml',
        'name': 'Concrete Compressive Strength',
        'description': 'Concrete strength prediction (1030 samples, 8 features)',
        'openml_id': 4353,
        'target_column': 'class',
        'n_samples': 1030,
        'n_features': 8,
        'target_type': 'continuous'
    },
    'energy_efficiency_heating': {
        'source': 'openml',
        'name': 'Energy Efficiency (Heating)',
        'description': 'Building energy efficiency heating load (768 samples, 8 features)',
        'openml_id': 4514,
        'target_column': 'Y1',
        'n_samples': 768,
        'n_features': 8,
        'target_type': 'continuous'
    },
    'energy_efficiency_cooling': {
        'source': 'openml',
        'name': 'Energy Efficiency (Cooling)',
        'description': 'Building energy efficiency cooling load (768 samples, 8 features)',
        'openml_id': 4514,
        'target_column': 'Y2',
        'n_samples': 768,
        'n_features': 8,
        'target_type': 'continuous'
    },
    'abalone': {
        'source': 'openml',
        'name': 'Abalone Age',
        'description': 'Abalone age prediction (4177 samples, 8 features)',
        'openml_id': 183,
        'target_column': 'Rings',
        'n_samples': 4177,
        'n_features': 8,
        'target_type': 'continuous'
    },
    'communities_crime': {
        'source': 'openml',
        'name': 'Communities and Crime',
        'description': 'Predict community crime rates (1994 samples, 127 features)',
        'openml_id': 183,
        'target_column': 'ViolentCrimesPerPop',
        'n_samples': 1994,
        'n_features': 127,
        'target_type': 'continuous'
    },
    'bike_sharing': {
        'source': 'openml',
        'name': 'Bike Sharing',
        'description': 'Predict hourly bike rental counts (17379 samples, 16 features)',
        'openml_id': 42712,
        'target_column': 'cnt',
        'n_samples': 17379,
        'n_features': 16,
        'target_type': 'continuous'
    },
    'parkinsons_motor': {
        'source': 'openml',
        'name': 'Parkinsons Motor UPDRS',
        'description': 'Predict motor UPDRS scores for Parkinsons (5875 samples, 20 features)',
        'openml_id': 189,
        'target_column': 'motor_UPDRS',
        'n_samples': 5875,
        'n_features': 20,
        'target_type': 'continuous'
    },
    'parkinsons_total': {
        'source': 'openml',
        'name': 'Parkinsons Total UPDRS',
        'description': 'Predict total UPDRS scores for Parkinsons (5875 samples, 20 features)',
        'openml_id': 189,
        'target_column': 'total_UPDRS',
        'n_samples': 5875,
        'n_features': 20,
        'target_type': 'continuous'
    }
}

def load_extended_regression_dataset(
    dataset_name: str, 
    random_state: int = 42,
    return_info: bool = True,
    handle_missing: str = 'auto',
    standardize_features: bool = False
) -> Tuple[np.ndarray, np.ndarray, Optional[Dict[str, Any]]]:
    """
    åŠ è½½æ‰©å±•çš„çœŸå®å›å½’æ•°æ®é›†
    
    æ”¯æŒsklearnå†…ç½®æ•°æ®é›†å’ŒOpenMLæ•°æ®é›†ï¼Œæä¾›ç»Ÿä¸€çš„æ¥å£å’Œæ•°æ®é¢„å¤„ç†ã€‚
    
    Args:
        dataset_name: æ•°æ®é›†åç§°ï¼Œå¿…é¡»åœ¨EXTENDED_REGRESSION_DATASETSä¸­å®šä¹‰
        random_state: éšæœºç§å­ï¼Œç”¨äºæ•°æ®åŠ è½½ä¸­çš„éšæœºæ“ä½œ
        return_info: æ˜¯å¦è¿”å›æ•°æ®é›†ä¿¡æ¯å­—å…¸
        handle_missing: ç¼ºå¤±å€¼å¤„ç†æ–¹å¼ ('auto', 'drop', 'fill_mean', 'fill_median')
        standardize_features: æ˜¯å¦æ ‡å‡†åŒ–ç‰¹å¾
        
    Returns:
        - X (np.ndarray): ç‰¹å¾çŸ©é˜µ
        - y (np.ndarray): ç›®æ ‡å˜é‡
        - dataset_info (Dict, optional): æ•°æ®é›†ä¿¡æ¯å­—å…¸
        
    Raises:
        ValueError: å¦‚æœæ•°æ®é›†åç§°ä¸æ”¯æŒ
        RuntimeError: å¦‚æœæ•°æ®åŠ è½½å¤±è´¥
    """
    if dataset_name not in EXTENDED_REGRESSION_DATASETS:
        available_datasets = list(EXTENDED_REGRESSION_DATASETS.keys())
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {dataset_name}. å¯é€‰æ‹©: {available_datasets}")
    
    dataset_config = EXTENDED_REGRESSION_DATASETS[dataset_name]
    
    print(f"ğŸ“Š åŠ è½½æ‰©å±•å›å½’æ•°æ®é›†: {dataset_config['name']}")
    print(f"ğŸ“‹ æ•°æ®é›†æè¿°: {dataset_config['description']}")
    
    try:
        if dataset_config['source'] == 'sklearn':
            X, y = _load_sklearn_dataset(dataset_config, random_state)
        elif dataset_config['source'] == 'openml':
            X, y = _load_openml_dataset(dataset_config, random_state)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®æº: {dataset_config['source']}")
        
        print(f"âœ… æ•°æ®é›†åŠ è½½æˆåŠŸ: {X.shape[0]} samples, {X.shape[1]} features")
        
        # æ•°æ®é¢„å¤„ç†
        X, y = _preprocess_dataset(X, y, handle_missing, standardize_features)
        
        if return_info:
            dataset_info = _create_dataset_info(dataset_config, X, y)
            return X, y, dataset_info
        else:
            return X, y
            
    except Exception as e:
        raise RuntimeError(f"æ•°æ®é›† {dataset_name} åŠ è½½å¤±è´¥: {str(e)}")

def _load_sklearn_dataset(dataset_config: Dict[str, Any], random_state: int) -> Tuple[np.ndarray, np.ndarray]:
    """åŠ è½½sklearnå†…ç½®æ•°æ®é›†"""
    loader = dataset_config['loader']
    
    if 'linnerud' in dataset_config.get('name', '').lower():
        # Linnerudæ˜¯å¤šè¾“å‡ºï¼Œå–ç¬¬ä¸€ä¸ªè¾“å‡º
        data = loader()
        X, y = data.data, data.target
        if y.ndim > 1:
            y = y[:, 0]  # å–ç¬¬ä¸€ä¸ªç›®æ ‡å˜é‡
    else:
        data = loader()
        X, y = data.data, data.target
    
    return X, y

def _load_openml_dataset(dataset_config: Dict[str, Any], random_state: int) -> Tuple[np.ndarray, np.ndarray]:
    """åŠ è½½OpenMLæ•°æ®é›†"""
    openml_id = dataset_config['openml_id']
    target_column = dataset_config.get('target_column')
    
    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        data = fetch_openml(data_id=openml_id, as_frame=True, parser='auto')
    
    # å¤„ç†ç‰¹å¾å’Œç›®æ ‡
    X = data.data
    
    # å¤„ç†ç›®æ ‡å˜é‡
    if target_column and target_column in data.data.columns:
        # ç›®æ ‡å˜é‡åœ¨ç‰¹å¾çŸ©é˜µä¸­
        y = data.data[target_column]
        X = data.data.drop(columns=[target_column])
    else:
        # ç›®æ ‡å˜é‡å•ç‹¬å­˜åœ¨
        y = data.target
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    X = _convert_to_numeric_array(X, 'features')
    y = _convert_to_numeric_array(y, 'target')
    
    return X, y

def _convert_to_numeric_array(data, data_type: str) -> np.ndarray:
    """å°†pandas DataFrame/Seriesè½¬æ¢ä¸ºæ•°å€¼å‹numpyæ•°ç»„"""
    if isinstance(data, pd.DataFrame):
        # åˆ›å»ºå‰¯æœ¬é¿å…è­¦å‘Š
        data = data.copy()
        
        # å¤„ç†åˆ†ç±»å˜é‡
        for col in data.columns:
            if data[col].dtype == 'object' or data[col].dtype.name == 'category':
                # ä½¿ç”¨LabelEncoderå¤„ç†åˆ†ç±»å˜é‡
                le = LabelEncoder()
                data.loc[:, col] = le.fit_transform(data[col].astype(str))
        
        return data.values.astype(np.float64)
    
    elif isinstance(data, pd.Series):
        # å¤„ç†ç›®æ ‡å˜é‡
        if data.dtype == 'object' or data.dtype.name == 'category':
            le = LabelEncoder() 
            data = le.fit_transform(data.astype(str))
        
        return np.array(data, dtype=np.float64)
    
    else:
        return np.array(data, dtype=np.float64)

def _preprocess_dataset(
    X: np.ndarray, 
    y: np.ndarray, 
    handle_missing: str,
    standardize_features: bool
) -> Tuple[np.ndarray, np.ndarray]:
    """æ•°æ®é¢„å¤„ç†"""
    
    # å¤„ç†ç¼ºå¤±å€¼
    if handle_missing != 'auto':
        X, y = _handle_missing_values(X, y, handle_missing)
    else:
        # è‡ªåŠ¨å¤„ç†ï¼šå¦‚æœæœ‰ç¼ºå¤±å€¼åˆ™ä½¿ç”¨å‡å€¼å¡«å……
        if np.isnan(X).any() or np.isnan(y).any():
            X, y = _handle_missing_values(X, y, 'fill_mean')
    
    # ç‰¹å¾æ ‡å‡†åŒ–
    if standardize_features:
        scaler = StandardScaler()
        X = scaler.fit_transform(X)
    
    return X, y

def _handle_missing_values(
    X: np.ndarray, 
    y: np.ndarray, 
    method: str
) -> Tuple[np.ndarray, np.ndarray]:
    """å¤„ç†ç¼ºå¤±å€¼"""
    
    if method == 'drop':
        # åˆ é™¤åŒ…å«ç¼ºå¤±å€¼çš„è¡Œ
        missing_mask = np.isnan(X).any(axis=1) | np.isnan(y)
        valid_indices = ~missing_mask
        return X[valid_indices], y[valid_indices]
    
    elif method == 'fill_mean':
        # ç”¨å‡å€¼å¡«å……
        col_means = np.nanmean(X, axis=0)
        for i in range(X.shape[1]):
            X[np.isnan(X[:, i]), i] = col_means[i]
        
        if np.isnan(y).any():
            y_mean = np.nanmean(y)
            y[np.isnan(y)] = y_mean
            
    elif method == 'fill_median':
        # ç”¨ä¸­ä½æ•°å¡«å……
        col_medians = np.nanmedian(X, axis=0)
        for i in range(X.shape[1]):
            X[np.isnan(X[:, i]), i] = col_medians[i]
        
        if np.isnan(y).any():
            y_median = np.nanmedian(y)
            y[np.isnan(y)] = y_median
    
    return X, y

def _create_dataset_info(
    dataset_config: Dict[str, Any], 
    X: np.ndarray, 
    y: np.ndarray
) -> Dict[str, Any]:
    """åˆ›å»ºæ•°æ®é›†ä¿¡æ¯å­—å…¸"""
    return {
        'name': dataset_config['name'],
        'description': dataset_config['description'],
        'source': dataset_config['source'],
        'n_samples': X.shape[0],
        'n_features': X.shape[1],
        'target_type': dataset_config['target_type'],
        'y_min': float(np.min(y)),
        'y_max': float(np.max(y)),
        'y_mean': float(np.mean(y)),
        'y_std': float(np.std(y)),
        'has_missing': np.isnan(X).any() or np.isnan(y).any()
    }

def list_available_regression_datasets() -> None:
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„å›å½’æ•°æ®é›†"""
    print("ğŸ¯ å¯ç”¨çš„æ‰©å±•å›å½’æ•°æ®é›†:")
    print("=" * 70)
    
    sklearn_datasets = []
    openml_datasets = []
    
    for name, config in EXTENDED_REGRESSION_DATASETS.items():
        if config['source'] == 'sklearn':
            sklearn_datasets.append((name, config))
        else:
            openml_datasets.append((name, config))
    
    if sklearn_datasets:
        print("\nğŸ“¦ sklearnå†…ç½®æ•°æ®é›†:")
        for name, config in sklearn_datasets:
            print(f"  â€¢ {name}: {config['description']}")
    
    if openml_datasets:
        print("\nğŸŒ OpenMLæ•°æ®é›†:")
        for name, config in openml_datasets:
            print(f"  â€¢ {name}: {config['description']}")
    
    print(f"\nâœ… æ€»è®¡ {len(EXTENDED_REGRESSION_DATASETS)} ä¸ªæ•°æ®é›†å¯ç”¨")

def get_dataset_info(dataset_name: str) -> Dict[str, Any]:
    """è·å–æŒ‡å®šæ•°æ®é›†çš„è¯¦ç»†ä¿¡æ¯"""
    if dataset_name not in EXTENDED_REGRESSION_DATASETS:
        raise ValueError(f"æœªçŸ¥æ•°æ®é›†: {dataset_name}")
    
    return EXTENDED_REGRESSION_DATASETS[dataset_name].copy()