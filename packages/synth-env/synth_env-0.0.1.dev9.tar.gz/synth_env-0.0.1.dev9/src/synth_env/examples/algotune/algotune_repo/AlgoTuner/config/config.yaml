global:
  spend_limit: 1.0 # in USD
  total_messages: 9999
  max_messages_in_history: 5

benchmark:
  dev_runs: 2               
  eval_runs: 10
  baseline_timeout: 60000 # in milliseconds
  manager_refresh_interval: 50 # Refresh multiprocessing.Manager after this many subprocess spawns to prevent resource exhaustion              
  validation_pool:       
    num_workers: 1       # Max number of parallel workers (1 = sequential, null = os.cpu_count())
    maxtasksperchild: null # Disable worker recycling for debugging (was 10)
    memory_limit_gb_per_worker: 14 # Increased for baseline algorithms (was 6GB)
    disable_rlimit_as: true # Disable RLIMIT_AS to prevent virtual memory allocation failures

dataset:
  train_size: 100
  test_size: 100

# Note: Keep the set of api_key_env values in this file up-to-date with the
# invocation to main.py in run_agent_no_singularity.sh!
models:
  o4-mini:
    api_key_env: "OPENAI_API_KEY"
    temperature: 0.0
    # OpenAI o4-mini with maximum reasoning effort and context
    reasoning_effort: "high"
    max_tokens: 100000
    drop_params: true
  o3:
    api_key_env: "OPENAI_API_KEY"
    temperature: 0.0
    # OpenAI o4-mini with maximum reasoning effort and context
    reasoning_effort: "high"
    max_tokens: 100000
    drop_params: true
  claude-opus-4-20250514:
    api_key_env: "CLAUDE_API_KEY"
    temperature: 1.0
    top_p: 0.95
    modify_params: true
    # Claude Opus 4 with maximum thinking within API limits (thinking + output â‰¤ 32K total)
    max_tokens: 32000
    thinking:
      type: "enabled"
      budget_tokens: 24000
  deepseek/deepseek-reasoner:
    api_key_env: "DEEPSEEK_API_KEY"
    max_tokens: 64000
  gemini/gemini-2.5-pro:
    api_key_env: "GEMINI_API_KEY"
    temperature: 0.0
    top_p: 0.95
    modify_params: true
    # Gemini 2.5 Pro with MAXIMUM thinking and context (Google AI Studio)
    max_tokens: 64000
    thinking_budget: 32768   # MAXIMUM thinking budget for Gemini 2.5 Pro (128-32,768 range)
    include_thoughts: true   # Include full reasoning process in response
  deepseek-ai/DeepSeek-R1:
    api_key_env: "TOGETHER_API_KEY"
    temperature: 0.6
    top_p: 0.95
    # DeepSeek-R1-0528 via Together API with maximum context
    max_tokens: 32000
    model_provider: "together"


