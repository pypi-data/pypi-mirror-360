# update_docs/core.py

import os
import re
import json
import hashlib
import subprocess
from pathlib import Path
import difflib
from collections import defaultdict
from datetime import datetime

HEADER_RE = re.compile(
    r"^(#{1,6})\s+(.*?)\s*(?:<!--\s*id:([\w\-]+)\s*-->)?\s*$",
    re.MULTILINE,
)
INCLUDE_RE = re.compile(r"<!--\s*include:([^\s#]+)#([\w\-]+)\s*-->")

def slugify(text):
    return re.sub(r"[^\w\-]", "-", text.lower())


def generate_path_based_id(relative_path):
    """–°–æ–∑–¥–∞–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª—É"""
    path_id = relative_path.replace("/", "-").replace("\\", "-")
    path_id = re.sub(r"[^\w\-.]", "-", path_id.lower())
    path_id = re.sub(r"-+", "-", path_id)  # –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–µ—Ñ–∏—Å—ã
    return path_id.strip("-")


def extract_content_preview(file_path, max_chars=200):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–µ–≤—å—é —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ñ–∞–π–ª–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""
    try:
        with open(file_path, encoding="utf-8") as f:
            content = f.read()
        content = re.sub(r"^#.*$", "", content, flags=re.MULTILINE)
        content = re.sub(r"<!--.*?-->", "", content, flags=re.DOTALL)
        return content[:max_chars].strip()
    except Exception:
        return ""

def extract_headers(file_path):
    with open(file_path, encoding="utf-8") as f:
        content = f.read()
    headers = []
    lines = content.splitlines()
    parent_stack = []
    
    for match in HEADER_RE.finditer(content):
        level = len(match.group(1))
        title = match.group(2).strip()
        id_tag = match.group(3) or slugify(title)
        
        start_line = content[:match.start()].count('\n')
        excerpt = ""
        for j in range(start_line + 1, min(start_line + 5, len(lines))):
            if j < len(lines) and not lines[j].strip().startswith('#'):
                excerpt += lines[j].strip() + " "
                if len(excerpt) > 100:
                    break
        excerpt = excerpt[:100].strip()
        if len(excerpt) == 100:
            excerpt += "..."
        
        while parent_stack and parent_stack[-1]['level'] >= level:
            parent_stack.pop()
        
        parent_id = parent_stack[-1]['id'] if parent_stack else None
        
        header_obj = {
            "id": id_tag,
            "level": level,
            "title": title,
            "excerpt": excerpt,
            "parent_id": parent_id
        }
        
        headers.append(header_obj)
        parent_stack.append(header_obj)
    
    return headers

def build_toc(docs_dir):
    toc = []
    header_map = {}
    for root, dirs, files in os.walk(docs_dir):
        dirs.sort()
        for file in sorted(files):
            if file.endswith(".md"):
                rel_path = os.path.relpath(os.path.join(root, file), docs_dir)
                headers = extract_headers(os.path.join(docs_dir, rel_path))
                if headers:
                    toc.append({
                        "file": rel_path.replace("\\", "/"),
                        "headers": headers
                    })
                    for h in headers:
                        header_map[(rel_path.replace("\\", "/"), h["id"])] = {
                            "title": h["title"],
                            "level": h["level"],
                            "file": rel_path.replace("\\", "/")
                        }
    return toc, header_map


def build_comprehensive_toc(root_dir, exclude_patterns=None):
    """–°–∫–∞–Ω–∏—Ä—É–µ—Ç –≤—Å–µ —É—Ä–æ–≤–Ω–∏ –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç–∏ –Ω–∞—á–∏–Ω–∞—è —Å –∫–æ—Ä–Ω–µ–≤–æ–π –ø–∞–ø–∫–∏ –ø—Ä–æ–µ–∫—Ç–∞"""
    if exclude_patterns is None:
        exclude_patterns = ['.git', 'node_modules', '__pycache__', '.venv', 'build', 'dist']
    
    toc = []
    header_map = {}
    all_documents = []
    
    for root, dirs, files in os.walk(root_dir):
        dirs[:] = [d for d in dirs if not any(pattern in d for pattern in exclude_patterns)]
        
        for file in sorted(files):
            if file.endswith(".md"):
                file_path = os.path.join(root, file)
                rel_path = os.path.relpath(file_path, root_dir)
                
                unique_id = generate_path_based_id(rel_path)
                
                headers = extract_headers(file_path)
                file_stats = os.stat(file_path)
                
                doc_entry = {
                    "path": file_path,
                    "relative_path": rel_path.replace("\\", "/"),
                    "unique_id": unique_id,
                    "headers": headers,
                    "size": file_stats.st_size,
                    "modified": file_stats.st_mtime,
                    "content_preview": extract_content_preview(file_path)
                }
                
                toc.append(doc_entry)
                all_documents.append(doc_entry)
                
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ header_map
                for h in headers:
                    header_map[(rel_path.replace("\\", "/"), h["id"])] = {
                        "title": h["title"],
                        "level": h["level"],
                        "file": rel_path.replace("\\", "/")
                    }
    
    return toc, header_map, all_documents

def extract_section(file_path, section_id, level):
    with open(file_path, encoding="utf-8") as f:
        lines = f.read().splitlines()
    section = []
    found = False
    current_level = None
    for i, line in enumerate(lines):
        match = HEADER_RE.match(line)
        if match:
            l = len(match.group(1))
            t = match.group(2).strip()
            id_tag = match.group(3) or slugify(t)
            if id_tag == section_id:
                found = True
                current_level = l
                continue
            if found and l <= current_level:
                break
        if found:
            section.append(line)
    return "\n".join(section).strip()

def update_includes(docs_dir, header_map):
    errors = []
    for root, dirs, files in os.walk(docs_dir):
        dirs.sort()
        for file in sorted(files):
            if file.endswith(".md"):
                file_path = os.path.join(root, file)
                rel_path = os.path.relpath(file_path, docs_dir)
                with open(file_path, encoding="utf-8") as f:
                    content = f.read()

                updated = content
                changed = False

                for match in INCLUDE_RE.finditer(content):
                    include_file, include_id = match.groups()
                    key = (include_file, include_id)
                    if key not in header_map:
                        errors.append(f"‚ö†Ô∏è include not found: {include_file}#{include_id} in {rel_path}")
                        continue

                    original_path = os.path.join(docs_dir, include_file)
                    level = header_map[key]["level"]
                    text = extract_section(original_path, include_id, level)

                    block = f"<!-- BEGIN include:{include_file}#{include_id} -->\n{text}\n<!-- END include -->"
                    block_re = re.compile(
                        rf"<!--\s*BEGIN\s+include:{re.escape(include_file)}#{re.escape(include_id)}\s*-->.*?<!--\s*END\s+include\s*-->",
                        re.DOTALL,
                    )
                    if block_re.search(updated):
                        updated = block_re.sub(block, updated)
                        changed = True
                    else:
                        insert_point = match.end()
                        updated = updated[:insert_point] + "\n" + block + updated[insert_point:]
                        changed = True

                if changed:
                    with open(file_path, "w", encoding="utf-8") as f:
                        f.write(updated)
    return errors


def write_markdown_toc(toc, toc_md_path):
    with open(toc_md_path, "w", encoding="utf-8") as f:
        f.write("# Table of Contents\n\n")
        for entry in toc:
            file_path = entry["file"]
            full_path = f"docs/{file_path}"
            anchor = slugify(file_path.replace("/", "-"))
            f.write(f'- <a id="{anchor}"></a>[{file_path}]({full_path})\n')




def get_file_title(entry):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Ñ–∞–π–ª–∞ –∏–∑ –ø–µ—Ä–≤–æ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∏–ª–∏ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞"""
    if entry["headers"]:
        return entry["headers"][0]["title"]
    return os.path.basename(entry["relative_path"])




def find_exact_header_matches(files):
    """–ù–∞—Ö–æ–¥–∏—Ç —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –º–µ–∂–¥—É —Ñ–∞–π–ª–∞–º–∏"""
    header_matches = defaultdict(list)
    
    for entry in files:
        for header in entry.get("headers", []):
            title = header["title"].strip()
            rel_path = entry.get("relative_path", entry.get("file", ""))
            header_matches[title].append({
                "file": rel_path,
                "header_id": header["id"],
                "level": header["level"],
                "link": f"docs/{rel_path}#{header['id']}"
            })
    
    duplicates = {title: locations for title, locations in header_matches.items() 
                 if len(locations) > 1}
    
    return duplicates

def find_project_root():
    """–ù–∞—Ö–æ–¥–∏—Ç –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ (–ø–∞–ø–∫—É —Å .git)"""
    path = Path.cwd()
    while not (path / ".git").exists():
        if path.parent == path:
            break
        path = path.parent
    return path

def get_git_file_authors(file_path, repo_root=None):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∞–≤—Ç–æ—Ä–∞—Ö —Ñ–∞–π–ª–∞ –∏–∑ git –∏—Å—Ç–æ—Ä–∏–∏"""
    if repo_root is None:
        repo_root = find_project_root()
    
    try:
        result = subprocess.run([
            'git', 'log', '-1', '--pretty=format:%an|%ae|%at', '--', file_path
        ], cwd=repo_root, capture_output=True, text=True, timeout=10)
        
        if result.returncode != 0:
            return None
        
        if not result.stdout.strip():
            return None
            
        author_name, author_email, timestamp = result.stdout.strip().split('|')
        
        all_authors_result = subprocess.run([
            'git', 'log', '--pretty=format:%an|%ae', '--', file_path
        ], cwd=repo_root, capture_output=True, text=True, timeout=15)
        
        all_authors = []
        if all_authors_result.returncode == 0:
            for line in all_authors_result.stdout.strip().split('\n'):
                if line.strip():
                    name, email = line.split('|')
                    if (name, email) not in all_authors:
                        all_authors.append((name, email))
        
        return {
            'last_author_name': author_name,
            'last_author_email': author_email,
            'last_modified_timestamp': int(timestamp),
            'all_authors': all_authors
        }
        
    except (subprocess.TimeoutExpired, subprocess.CalledProcessError, ValueError):
        return None

def classify_author_from_git(git_info):
    """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –∞–≤—Ç–æ—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ git –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
    if not git_info:
        return "human"
    
    email = git_info['last_author_email'].lower()
    name = git_info['last_author_name'].lower()
    
    ai_patterns = [
        r'.*ai.*@.*',
        r'.*bot.*@.*', 
        r'.*gpt.*@.*',
        r'.*claude.*@.*',
        r'.*assistant.*@.*',
        r'.*devin.*@.*',
        r'.*copilot.*@.*',
        r'noreply@.*',
        r'.*automated.*@.*'
    ]
    
    generator_patterns = [
        r'.*generator.*@.*',
        r'.*auto.*@.*',
        r'.*system.*@.*',
        r'.*build.*@.*',
        r'.*ci.*@.*',
        r'.*deploy.*@.*'
    ]
    
    for pattern in ai_patterns:
        if re.match(pattern, email):
            return "ai"
    
    for pattern in generator_patterns:
        if re.match(pattern, email):
            return "generator"
    
    ai_name_patterns = ['ai', 'bot', 'gpt', 'claude', 'assistant', 'devin', 'copilot']
    generator_name_patterns = ['generator', 'auto', 'system', 'build', 'ci', 'deploy']
    
    for pattern in ai_name_patterns:
        if pattern in name:
            return "ai"
            
    for pattern in generator_name_patterns:
        if pattern in name:
            return "generator"
    
    return "human"

def detect_author_type_enhanced(file_path, content, git_info=None):
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∞–≤—Ç–æ—Ä–∞ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–æ–π —Å–∏—Å—Ç–µ–º–æ–π"""
    
    generator_check = check_generator_registry(file_path)
    if generator_check:
        return "generator", "registry_lookup"
    
    auto_patterns = [
        r'<!-- AUTO-GENERATED -->',
        r'# AUTO-GENERATED',
        r'This file was automatically generated',
        r'Generated by update-docs',
        r'Generated by.*\.py',
        r'–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ',
        r'–°–æ–∑–¥–∞–Ω–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏',
        r'Auto-generated by',
        r'–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ —Å–∫—Ä–∏–ø—Ç–æ–º',
        r'Generated by example_doc_generator\.py'
    ]
    
    for pattern in auto_patterns:
        if re.search(pattern, content, re.IGNORECASE):
            return "generator", "comment_marker"
    
    if '/auto_generated/' in file_path.replace('\\', '/'):
        return "generator", "file_location"
    
    filename = os.path.basename(file_path).lower()
    auto_filename_patterns = [
        r'.*_auto\.md$',
        r'.*_generated\.md$', 
        r'changelog_auto\.md$',
        r'api_documentation\.md$',
        r'metrics_report\.md$'
    ]
    
    for pattern in auto_filename_patterns:
        if re.match(pattern, filename):
            return "generator", "filename_pattern"
    
    ai_patterns = [
        r'<!-- AI-GENERATED -->',
        r'Generated by AI',
        r'Created by.*AI',
        r'ChatGPT|Claude|GPT-|Devin',
        r'–°–æ–∑–¥–∞–Ω–æ –ò–ò|–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –ò–ò'
    ]
    
    for pattern in ai_patterns:
        if re.search(pattern, content, re.IGNORECASE):
            return "ai", "comment_marker"
    
    if git_info:
        git_classification = classify_author_from_git(git_info)
        if git_classification != "human":
            return git_classification, "git_history"
        
        all_authors = git_info.get('all_authors', [])
        if len(all_authors) > 1:
            author_types = set()
            for name, email in all_authors:
                temp_git_info = {'last_author_email': email, 'last_author_name': name}
                author_type = classify_author_from_git(temp_git_info)
                author_types.add(author_type)
            
            if len(author_types) > 1:
                return "mixed", "git_history"
    
    return "human", "default"

def check_generator_registry(file_path):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ñ–∞–π–ª –≤ —Ä–µ–µ—Å—Ç—Ä–µ –∞–≤—Ç–æ–≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–≤"""
    registry_path = "docs/auto_generated/generator_registry.json"
    
    if not os.path.exists(registry_path):
        return False
    
    try:
        with open(registry_path, 'r', encoding='utf-8') as f:
            registry = json.load(f)
        
        normalized_path = file_path.replace('\\', '/')
        
        for generator in registry.get('generators', []):
            for output_file in generator.get('output_files', []):
                if normalized_path.endswith(output_file) or output_file.endswith(normalized_path):
                    return True
        
        return False
    except (json.JSONDecodeError, FileNotFoundError):
        return False

def scan_for_generator_functions():
    """–°–∫–∞–Ω–∏—Ä—É–µ—Ç –∫–æ–¥–æ–≤—É—é –±–∞–∑—É –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç —Ñ—É–Ω–∫—Ü–∏–π-–≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏"""
    generator_functions = []
    
    generator_patterns = [
        r'def.*generate.*doc',
        r'def.*create.*doc',
        r'def.*write.*doc',
        r'def.*build.*doc',
        r'def.*make.*doc'
    ]
    
    for root, dirs, files in os.walk('.'):
        dirs[:] = [d for d in dirs if not d.startswith('.') and d not in ['__pycache__', 'node_modules']]
        
        for file in files:
            if file.endswith('.py'):
                file_path = os.path.join(root, file)
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    for pattern in generator_patterns:
                        matches = re.finditer(pattern, content, re.IGNORECASE)
                        for match in matches:
                            line = content[match.start():content.find('\n', match.start())]
                            func_name = re.search(r'def\s+(\w+)', line)
                            if func_name:
                                generator_functions.append({
                                    'file': file_path,
                                    'function': func_name.group(1),
                                    'line': line.strip()
                                })
                
                except (UnicodeDecodeError, PermissionError):
                    continue
    
    return generator_functions

def determine_editability(author_type, author_source):
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞"""
    if author_type in ["generator"]:
        return False
    elif author_type == "mixed":
        return True
    else:
        return True

def generate_persistent_file_id(file_path, content=None):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç persistent file_id –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ñ–∞–π–ª–∞"""
    if content is None:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
    
    content_sample = content[:200].strip()
    content_hash = hashlib.md5(content_sample.encode()).hexdigest()[:8]
    
    base_name = os.path.splitext(os.path.basename(file_path))[0]
    return f"{slugify(base_name)}-{content_hash}"

def load_existing_file_ids(content_json_path):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ file_id –∏–∑ Content.json –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–∏ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–∏"""
    if not os.path.exists(content_json_path):
        return {}
    
    try:
        with open(content_json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if isinstance(data, list):
            return {entry.get('path', entry.get('file', '')): entry.get('file_id', '') 
                   for entry in data if entry.get('file_id')}
        else:
            return {}
    except (json.JSONDecodeError, KeyError):
        return {}

def match_file_by_content(file_path, content, existing_entries):
    """–ù–∞—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π file_id –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É –ø—Ä–∏ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–∏ —Ñ–∞–π–ª–∞"""
    current_hash = hashlib.md5(content[:200].encode()).hexdigest()[:8]
    
    for entry in existing_entries:
        if entry.get('file_id', '').endswith(f"-{current_hash}"):
            return entry['file_id']
    
    return None


def write_toc_from_json(toc_json_path, toc_md_path, annotations=None):
    """–°–æ–∑–¥–∞–µ—Ç Markdown –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ toc.json —Ñ–∞–π–ª–∞ —Å –ø–µ—Ä–µ–∫—Ä–µ—Å—Ç–Ω—ã–º–∏ —Å—Å—ã–ª–∫–∞–º–∏"""
    if annotations is None:
        annotations = {}
    
    with open(toc_json_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    if isinstance(data, dict) and "files" in data:
        files = data["files"]
        metadata = data.get("metadata", {})
    else:
        files = data
        metadata = {"total_files": len(files)}
    
    header_duplicates = find_exact_header_matches(files)
    
    with open(toc_md_path, "w", encoding="utf-8") as f:
        f.write("# –û–≥–ª–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞\n\n")
        
        if metadata.get("generated_at"):
            f.write(f"*–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: {metadata['generated_at']}*  \n")
        f.write(f"*–í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {metadata.get('total_files', len(files))}*\n")
        if header_duplicates:
            f.write(f"*–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤: {len(header_duplicates)}*\n")
        f.write("\n")
        
        dir_groups = defaultdict(list)
        for entry in files:
            rel_path = entry.get("relative_path", entry.get("file", ""))
            dir_path = os.path.dirname(rel_path)
            if not dir_path:
                dir_path = "–∫–æ—Ä–µ–Ω—å"
            dir_groups[dir_path].append(entry)
        
        f.write("## üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏\n\n")
        for dir_path in sorted(dir_groups.keys()):
            f.write(f"### {dir_path}\n\n")
            for entry in sorted(dir_groups[dir_path], key=lambda x: x.get("relative_path", x.get("file", ""))):
                rel_path = entry.get("relative_path", entry.get("file", ""))
                file_title = get_file_title_from_entry(entry)
                unique_id = entry.get("unique_id", rel_path.replace("/", "-").replace("\\", "-"))
                
                full_path = f"docs/{rel_path}"
                f.write(f"- [üìÑ {file_title}]({full_path})")
                
                if unique_id and unique_id != rel_path:
                    f.write(f" `{unique_id}`")
                f.write("\n")
                
                if "size" in entry:
                    size_kb = entry["size"] // 1024
                    f.write(f"  - **–†–∞–∑–º–µ—Ä**: {size_kb} KB\n")
                
                file_annotation = annotations.get(rel_path)
                if file_annotation:
                    f.write(f"  - **–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ**: {file_annotation}\n")
                
                headers = entry.get("headers", [])
                if headers:
                    f.write("  - **–ó–∞–≥–æ–ª–æ–≤–∫–∏**:\n")
                    for header in headers:
                        indent = "    " + "  " * (header["level"] - 1)
                        header_key = f"{rel_path}#{header['id']}"
                        header_annotation = annotations.get(header_key, "")
                        title = header["title"].strip()
                        
                        if title in header_duplicates and len(header_duplicates[title]) > 1:
                            f.write(f"{indent}- [{header['title']}](docs/{rel_path}#{header['id']}) ‚ö†Ô∏è **–î—É–±–ª–∏–∫–∞—Ç**")
                            if header_annotation:
                                f.write(f" ‚Äî *{header_annotation}*")
                            f.write("\n")
                            
                            other_locations = [loc for loc in header_duplicates[title] 
                                             if loc["link"] != f"docs/{rel_path}#{header['id']}"]
                            if other_locations:
                                f.write(f"{indent}  - *–¢–∞–∫–∂–µ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤:*\n")
                                for loc in other_locations:
                                    f.write(f"{indent}    - [{os.path.basename(loc['file'])}]({loc['link']})\n")
                        else:
                            f.write(f"{indent}- [{header['title']}](docs/{rel_path}#{header['id']})")
                            if header_annotation:
                                f.write(f" ‚Äî *{header_annotation}*")
                            f.write("\n")
                
                f.write("\n")
        
        if header_duplicates:
            f.write("## üîç –ü–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –∑–∞–≥–æ–ª–æ–≤–∫–∏\n\n")
            f.write("*–ó–∞–≥–æ–ª–æ–≤–∫–∏ —Å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º–∏ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞–º–∏ –≤ —Ä–∞–∑–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö:*\n\n")
            
            for title, locations in sorted(header_duplicates.items()):
                f.write(f"### \"{title}\"\n")
                f.write(f"*–í—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤ {len(locations)} –º–µ—Å—Ç–∞—Ö:*\n\n")
                
                for loc in locations:
                    f.write(f"- [{os.path.basename(loc['file'])}]({loc['link']}) (—É—Ä–æ–≤–µ–Ω—å {loc['level']})\n")
                f.write("\n")
        
        f.write("## üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n\n")
        f.write(f"- **–í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤**: {metadata.get('total_files', len(files))}\n")
        total_headers = sum(len(entry.get("headers", [])) for entry in files)
        f.write(f"- **–í—Å–µ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤**: {total_headers}\n")
        if header_duplicates:
            f.write(f"- **–ü–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤**: {len(header_duplicates)}\n")
        if annotations:
            f.write(f"- **–ê–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤**: {len(annotations)}\n")

def get_file_title_from_entry(entry):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Ñ–∞–π–ª–∞ –∏–∑ –∑–∞–ø–∏—Å–∏ TOC"""
    headers = entry.get("headers", [])
    if headers:
        return headers[0]["title"]
    
    rel_path = entry.get("relative_path", entry.get("file", ""))
    return os.path.basename(rel_path)

def find_exact_header_matches(files):
    """–ù–∞—Ö–æ–¥–∏—Ç —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –º–µ–∂–¥—É —Ñ–∞–π–ª–∞–º–∏"""
    header_matches = defaultdict(list)
    
    for entry in files:
        for header in entry.get("headers", []):
            title = header["title"].strip()
            rel_path = entry.get("relative_path", entry.get("file", ""))
            header_matches[title].append({
                "file": rel_path,
                "header_id": header["id"],
                "level": header["level"],
                "link": f"docs/{rel_path}#{header['id']}"
            })
    
    duplicates = {title: locations for title, locations in header_matches.items() 
                 if len(locations) > 1}
    
    return duplicates


def write_toc_from_json(toc_json_path, toc_md_path, annotations=None):
    """–°–æ–∑–¥–∞–µ—Ç Markdown –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ toc.json —Ñ–∞–π–ª–∞ —Å –ø–µ—Ä–µ–∫—Ä–µ—Å—Ç–Ω—ã–º–∏ —Å—Å—ã–ª–∫–∞–º–∏"""
    if annotations is None:
        annotations = {}
    
    with open(toc_json_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    if isinstance(data, dict) and "files" in data:
        files = data["files"]
        metadata = data.get("metadata", {})
    else:
        files = data
        metadata = {"total_files": len(files)}
    
    header_duplicates = find_exact_header_matches(files)
    
    with open(toc_md_path, "w", encoding="utf-8") as f:
        f.write("# –û–≥–ª–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞\n\n")
        
        if metadata.get("generated_at"):
            f.write(f"*–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: {metadata['generated_at']}*  \n")
        f.write(f"*–í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {metadata.get('total_files', len(files))}*\n")
        if header_duplicates:
            f.write(f"*–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤: {len(header_duplicates)}*\n")
        f.write("\n")
        
        dir_groups = defaultdict(list)
        for entry in files:
            rel_path = entry.get("relative_path", entry.get("file", ""))
            dir_path = os.path.dirname(rel_path)
            if not dir_path:
                dir_path = "–∫–æ—Ä–µ–Ω—å"
            dir_groups[dir_path].append(entry)
        
        f.write("## üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏\n\n")
        for dir_path in sorted(dir_groups.keys()):
            f.write(f"### {dir_path}\n\n")
            for entry in sorted(dir_groups[dir_path], key=lambda x: x.get("relative_path", x.get("file", ""))):
                rel_path = entry.get("relative_path", entry.get("file", ""))
                file_title = get_file_title_from_entry(entry)
                unique_id = entry.get("unique_id", rel_path.replace("/", "-").replace("\\", "-"))
                
                f.write(f"- [üìÑ {file_title}](docs/{rel_path})")
                
                if unique_id and unique_id != rel_path:
                    f.write(f" `{unique_id}`")
                f.write("\n")
                
                if "size" in entry:
                    size_kb = entry["size"] // 1024
                    f.write(f"  - **–†–∞–∑–º–µ—Ä**: {size_kb} KB\n")
                
                file_annotation = annotations.get(rel_path)
                if file_annotation:
                    f.write(f"  - **–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ**: {file_annotation}\n")
                
                headers = entry.get("headers", [])
                if headers:
                    f.write("  - **–ó–∞–≥–æ–ª–æ–≤–∫–∏**:\n")
                    for header in headers:
                        indent = "    " + "  " * (header["level"] - 1)
                        header_key = f"{rel_path}#{header['id']}"
                        header_annotation = annotations.get(header_key, "")
                        title = header["title"].strip()
                        
                        if title in header_duplicates and len(header_duplicates[title]) > 1:
                            f.write(f"{indent}- [{header['title']}](docs/{rel_path}#{header['id']}) ‚ö†Ô∏è **–î—É–±–ª–∏–∫–∞—Ç**")
                            if header_annotation:
                                f.write(f" ‚Äî *{header_annotation}*")
                            f.write("\n")
                            
                            other_locations = [loc for loc in header_duplicates[title] 
                                             if loc["link"] != f"docs/{rel_path}#{header['id']}"]
                            if other_locations:
                                f.write(f"{indent}  - *–¢–∞–∫–∂–µ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤:*\n")
                                for loc in other_locations:
                                    f.write(f"{indent}    - [{os.path.basename(loc['file'])}]({loc['link']})\n")
                        else:
                            f.write(f"{indent}- [{header['title']}](docs/{rel_path}#{header['id']})")
                            if header_annotation:
                                f.write(f" ‚Äî *{header_annotation}*")
                            f.write("\n")
                
                f.write("\n")
        
        if header_duplicates:
            f.write("## üîç –ü–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –∑–∞–≥–æ–ª–æ–≤–∫–∏\n\n")
            f.write("*–ó–∞–≥–æ–ª–æ–≤–∫–∏ —Å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º–∏ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞–º–∏ –≤ —Ä–∞–∑–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö:*\n\n")
            
            for title, locations in sorted(header_duplicates.items()):
                f.write(f"### \"{title}\"\n")
                f.write(f"*–í—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤ {len(locations)} –º–µ—Å—Ç–∞—Ö:*\n\n")
                
                for loc in locations:
                    f.write(f"- [{os.path.basename(loc['file'])}]({loc['link']}) (—É—Ä–æ–≤–µ–Ω—å {loc['level']})\n")
                f.write("\n")
        
        f.write("## üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n\n")
        f.write(f"- **–í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤**: {metadata.get('total_files', len(files))}\n")
        total_headers = sum(len(entry.get("headers", [])) for entry in files)
        f.write(f"- **–í—Å–µ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤**: {total_headers}\n")
        if header_duplicates:
            f.write(f"- **–ü–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤**: {len(header_duplicates)}\n")
        if annotations:
            f.write(f"- **–ê–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤**: {len(annotations)}\n")


def get_file_title_from_entry(entry):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Ñ–∞–π–ª–∞ –∏–∑ –∑–∞–ø–∏—Å–∏ TOC"""
    headers = entry.get("headers", [])
    if headers:
        return headers[0]["title"]
    
    rel_path = entry.get("relative_path", entry.get("file", ""))
    return os.path.basename(rel_path)


def build_content_json(docs_dir, existing_file_ids=None, existing_entries=None):
    """–°–æ–∑–¥–∞–µ—Ç Content.json —Å–æ–≥–ª–∞—Å–Ω–æ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å git –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π"""
    if existing_file_ids is None:
        existing_file_ids = {}
    if existing_entries is None:
        existing_entries = []
    
    content_entries = []
    repo_root = find_project_root()
    
    for root, dirs, files in os.walk(docs_dir):
        dirs[:] = [d for d in dirs if d != 'content']
        dirs.sort()
        
        for file in sorted(files):
            if file.endswith(".md"):
                file_path = os.path.join(root, file)
                rel_path = os.path.relpath(file_path, docs_dir).replace("\\", "/")
                
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                git_info = get_git_file_authors(rel_path, repo_root)
                
                if rel_path in existing_file_ids:
                    file_id = existing_file_ids[rel_path]
                else:
                    matched_id = match_file_by_content(file_path, content, existing_entries)
                    if matched_id:
                        file_id = matched_id
                    else:
                        file_id = generate_persistent_file_id(file_path, content)
                
                author_type, author_source = detect_author_type_enhanced(file_path, content, git_info)
                editable = determine_editability(author_type, author_source)
                
                headers = extract_headers(file_path)
                
                title = headers[0]["title"] if headers else os.path.splitext(file)[0]
                
                entry = {
                    "file_id": file_id,
                    "title": title,
                    "path": rel_path,
                    "editable": editable,
                    "author": author_type,
                    "headers": headers
                }
                
                if git_info:
                    entry["git_info"] = {
                        "last_author": f"{git_info['last_author_name']} <{git_info['last_author_email']}>",
                        "last_modified": git_info['last_modified_timestamp'],
                        "all_authors": [f"{name} <{email}>" for name, email in git_info['all_authors']],
                        "author_source": author_source
                    }
                
                content_entries.append(entry)
    
    return content_entries

def write_description_for_agents(content_entries, output_path, docs_dir='docs'):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç Description_for_agents.md –Ω–∞ –æ—Å–Ω–æ–≤–µ Content.json"""
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write("# üìò –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞\n\n")
        f.write("*–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ —Å–∏—Å—Ç–µ–º–æ–π update-docs*\n\n")
        f.write("<!-- AUTO-GENERATED -->\n\n")
        
        author_stats = defaultdict(int)
        for entry in content_entries:
            author_stats[entry["author"]] += 1
        
        f.write("## üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∞–≤—Ç–æ—Ä—Å—Ç–≤–∞\n\n")
        author_icons = {"human": "üë§", "ai": "ü§ñ", "generator": "‚öôÔ∏è", "mixed": "üîÑ"}
        for author_type, count in sorted(author_stats.items()):
            icon = author_icons.get(author_type, "‚ùì")
            f.write(f"- {icon} **{author_type}**: {count} —Ñ–∞–π–ª–æ–≤\n")
        f.write("\n")
        
        dir_groups = defaultdict(list)
        for entry in content_entries:
            dir_path = os.path.dirname(entry["path"])
            if not dir_path:
                dir_path = "–∫–æ—Ä–µ–Ω—å"
            dir_groups[dir_path].append(entry)
        
        output_dir = os.path.dirname(output_path)
        
        for dir_path in sorted(dir_groups.keys()):
            f.write(f"## üìÅ {dir_path}\n\n")
            
            for entry in sorted(dir_groups[dir_path], key=lambda x: x["title"]):
                editable_icon = "‚úèÔ∏è" if entry["editable"] else "üîí"
                author_icon = author_icons.get(entry["author"], "‚ùì")
                
                target_file_path = os.path.join(docs_dir, entry["path"])
                relative_path = os.path.relpath(target_file_path, output_dir).replace("\\", "/")
                
                if not os.path.exists(target_file_path):
                    continue
                
                f.write(f"### {editable_icon} {author_icon} [{entry['title']}]({relative_path})\n")
                f.write(f"**File ID:** `{entry['file_id']}`  \n")
                f.write(f"**–ê–≤—Ç–æ—Ä:** {entry['author']} | **–†–µ–¥–∞–∫—Ç–∏—Ä—É–µ–º—ã–π:** {'–î–∞' if entry['editable'] else '–ù–µ—Ç'}\n")
                
                if "git_info" in entry:
                    git_info = entry["git_info"]
                    f.write(f"**–ü–æ—Å–ª–µ–¥–Ω–∏–π –∞–≤—Ç–æ—Ä:** {git_info['last_author']}  \n")
                    if len(git_info['all_authors']) > 1:
                        f.write(f"**–í—Å–µ –∞–≤—Ç–æ—Ä—ã:** {', '.join(git_info['all_authors'][:3])}")
                        if len(git_info['all_authors']) > 3:
                            f.write(f" –∏ –µ—â–µ {len(git_info['all_authors']) - 3}")
                        f.write("  \n")
                
                f.write("\n")
                
                if entry["headers"]:
                    f.write("**–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤:**\n")
                    for header in entry["headers"]:
                        indent = "  " * (header["level"] - 1)
                        if os.path.exists(target_file_path):
                            f.write(f"{indent}- [{header['title']}]({relative_path}#{header['id']})")
                            if header["excerpt"]:
                                f.write(f" ‚Äî *{header['excerpt']}*")
                            f.write("\n")
                
                f.write("\n")

def update_content_system(docs_dir, content_json_path, description_md_path):
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã Content.json –∏ Description_for_agents.md"""
    
    content_dir = os.path.dirname(content_json_path)
    os.makedirs(content_dir, exist_ok=True)
    
    existing_file_ids = load_existing_file_ids(content_json_path)
    existing_entries = []
    if os.path.exists(content_json_path):
        try:
            with open(content_json_path, 'r', encoding='utf-8') as f:
                existing_entries = json.load(f)
            if not isinstance(existing_entries, list):
                existing_entries = []
        except (json.JSONDecodeError, FileNotFoundError):
            existing_entries = []
    
    content_entries = build_content_json(docs_dir, existing_file_ids, existing_entries)
    
    with open(content_json_path, 'w', encoding='utf-8') as f:
        json.dump(content_entries, f, indent=2, ensure_ascii=False)
    
    if description_md_path:
        write_description_for_agents(content_entries, description_md_path, docs_dir)
    
    header_map = {}
    for entry in content_entries:
        for header in entry["headers"]:
            key = (entry["path"], header["id"])
            header_map[key] = {
                "title": header["title"],
                "level": header["level"],
                "file": entry["path"]
            }
    
    include_errors = update_includes(docs_dir, header_map)
    
    clean_broken_back_links(docs_dir)
    if description_md_path:
        inject_back_to_toc_links_russian(docs_dir, description_md_path, content_entries)
    
    print(f"‚úÖ Content.json —Å–æ–∑–¥–∞–Ω: {content_json_path}")
    if description_md_path:
        print(f"‚úÖ Description_for_agents.md —Å–æ–∑–¥–∞–Ω: {description_md_path}")
        print("‚úÖ –ù–∞–≤–∏–≥–∞—Ü–∏–æ–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –æ–±–Ω–æ–≤–ª–µ–Ω—ã")
    
    if include_errors:
        print("‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –æ—à–∏–±–∫–∏ include:")
        for error in include_errors:
            print(f"  {error}")
    else:
        print("‚úÖ –í—Å–µ include –±–ª–æ–∫–∏ –≤–∞–ª–∏–¥–Ω—ã")
    
    return include_errors

def update_all_from_json(toc_json_path, toc_md_path, annotations=None):
    """–°–æ–∑–¥–∞–µ—Ç Markdown TOC –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ JSON —Ñ–∞–π–ª–∞"""
    write_toc_from_json(toc_json_path, toc_md_path, annotations)
    print(f"‚úÖ Markdown TOC created from {toc_json_path} and saved to: {toc_md_path}")


def clean_broken_back_links(docs_dir):
    """–£–¥–∞–ª—è–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –±–∏—Ç—ã–µ —Å—Å—ã–ª–∫–∏ 'Back to TOC' –∏–∑ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏"""
    for root, dirs, files in os.walk(docs_dir):
        for file in files:
            if file.endswith(".md"):
                file_path = os.path.join(root, file)
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                lines = content.split('\n')
                cleaned_lines = []
                skip_next_empty = False
                
                for line in lines:
                    if ('Back to TOC' in line and 
                        ('basic_toc.md' in line or 'comprehensive_toc.md' in line)):
                        skip_next_empty = True
                        continue
                    elif skip_next_empty and line.strip() == '':
                        skip_next_empty = False
                        continue
                    else:
                        cleaned_lines.append(line)
                        skip_next_empty = False
                
                cleaned_content = '\n'.join(cleaned_lines)
                if cleaned_content != content:
                    with open(file_path, 'w', encoding='utf-8') as f:
                        f.write(cleaned_content)


def inject_back_to_toc_links_russian(docs_dir, description_md_path, content_entries, readme_path='README.md'):
    """–î–æ–±–∞–≤–ª—è–µ—Ç —Ä—É—Å—Å–∫–∏–µ –Ω–∞–≤–∏–≥–∞—Ü–∏–æ–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏ '–î–æ–º–æ–π' –∏ '–ù–∞–∑–∞–¥' –≤ —Ñ–∞–π–ª—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏"""
    abs_description = os.path.abspath(description_md_path)
    abs_readme = os.path.abspath(readme_path)
    
    for entry in content_entries:
        rel_file = entry["path"]
        file_path = os.path.join(docs_dir, rel_file)
        abs_file = os.path.abspath(file_path)
        
        if abs_file == abs_description or abs_file == abs_readme:
            continue
            
        with open(file_path, encoding="utf-8") as f:
            content = f.read()
        
        relative_to_readme = os.path.relpath(readme_path, os.path.dirname(file_path)).replace("\\", "/")
        relative_to_description = os.path.relpath(description_md_path, os.path.dirname(file_path)).replace("\\", "/")
        
        home_link = f"[–î–æ–º–æ–π]({relative_to_readme})"
        back_link = f"[–ù–∞–∑–∞–¥]({relative_to_description})"
        
        if home_link in content and back_link in content:
            continue
        
        navigation = f"{home_link} | {back_link}\n\n"
        updated = f"{navigation}{content}"
        
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(updated)


def inject_back_to_toc_links(docs_dir, toc_md_path, toc):
    abs_toc = os.path.abspath(toc_md_path)
    for entry in toc:
        rel_file = entry["file"]
        file_path = os.path.join(docs_dir, rel_file)
        if os.path.abspath(file_path) == abs_toc:
            continue
        with open(file_path, encoding="utf-8") as f:
            content = f.read()
        anchor = slugify(rel_file.replace("/", "-"))
        relative = os.path.relpath(toc_md_path, os.path.dirname(file_path)).replace("\\", "/")
        link = f"[Back to TOC]({relative}#{anchor})"
        if link in content:
            continue
        updated = f"{link}\n\n{content}"
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(updated)

def update_all(docs_dir, toc_path, toc_md_path=None):
    toc, header_map = build_toc(docs_dir)
    with open(toc_path, "w", encoding="utf-8") as f:
        json.dump(toc, f, indent=2, ensure_ascii=False)
    if toc_md_path:
        write_markdown_toc(toc, toc_md_path)
    errors = update_includes(docs_dir, header_map)
    if toc_md_path:
        inject_back_to_toc_links(docs_dir, toc_md_path, toc)
    print(f"‚úÖ TOC updated and saved to: {toc_path}")
    if errors:
        print("\n".join(errors))
    else:
        print("‚úÖ All includes are valid.")


def update_all_comprehensive(docs_dir, toc_path, toc_md_path=None, comprehensive=False, 
                            similarity_threshold=0.8, exclude_patterns=None):
    """–û–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –±–µ–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""
    if comprehensive:
        root_dir = find_project_root()
        toc, header_map, all_documents = build_comprehensive_toc(root_dir, exclude_patterns)
        
        metadata = {
            "generated_at": datetime.now().isoformat(),
            "root_directory": str(root_dir),
            "total_files": len(toc)
        }
        
        extended_toc = {
            "metadata": metadata,
            "files": toc
        }
        
        with open(toc_path, "w", encoding="utf-8") as f:
            json.dump(extended_toc, f, indent=2, ensure_ascii=False)
        
        if toc_md_path:
            write_toc_from_json(toc_path, toc_md_path)
    else:
        toc, header_map = build_toc(docs_dir)
        with open(toc_path, "w", encoding="utf-8") as f:
            json.dump(toc, f, indent=2, ensure_ascii=False)
        if toc_md_path:
            write_markdown_toc(toc, toc_md_path)
    
    errors = update_includes(docs_dir, header_map)
    if toc_md_path and not comprehensive:
        inject_back_to_toc_links(docs_dir, toc_md_path, toc)
    
    print(f"‚úÖ TOC updated and saved to: {toc_path}")
    if errors:
        print("\n".join(errors))
    else:
        print("‚úÖ All includes are valid.")

def update_all_from_json(toc_json_path, toc_md_path, annotations=None):
    """–°–æ–∑–¥–∞–µ—Ç Markdown TOC –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ JSON —Ñ–∞–π–ª–∞"""
    write_toc_from_json(toc_json_path, toc_md_path, annotations)
    print(f"‚úÖ Markdown TOC created from {toc_json_path} and saved to: {toc_md_path}")
