Metadata-Version: 2.1
Name: scarabs
Version: 0.1.2
Summary: scarab: llm training paradigm
Home-page: https://github.com/zhu2856061/scarabs
Author: merlin
Author-email: zhipeng19930220@gmail.com
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.10
Requires-Python: >=3.10
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: scikit-learn >=1.6.0
Requires-Dist: datasets >=2.16.1
Requires-Dist: torch >=2.3.0
Requires-Dist: transformers >=4.47.1
Requires-Dist: evaluate >=0.4.3
Requires-Dist: einops >=0.8.0
Requires-Dist: sentencepiece >=0.2.0
Requires-Dist: accelerate >=1.2.1
Requires-Dist: peft >=0.7.1
Requires-Dist: ipywidgets >=8.1.5
Requires-Dist: tensorboardX ==2.6.2.2
Requires-Dist: torchinfo >=1.8.0
Requires-Dist: prettytable >=3.12.0
Requires-Dist: trl >=0.15.2
Requires-Dist: numpy >=1.26.4
Requires-Dist: colorlog >=6.9.0

# scarabså¹³å°: ä¸€æ¬¾åŸºäº transformers çš„ é€šç”¨æ¨¡å‹è®­ç»ƒæ¡†æ¶ï¼Œ
å¯ä»¥tabular dataè®­ç»ƒï¼Œtext dataè®­ç»ƒï¼Œimage dataè®­ç»ƒï¼ŒLLMè®­ç»ƒ

![scarabså¹³å°](doc/scarabs.jpg)


### ğŸ“˜ core:
  - âœ… Training of tabular data, For example, CTR used in recommendation systems
  - Training of text data, For example, text classification
  - Training of image data, For example, image classification
  - Training of LLM, For example, llm pretrain

### ğŸ“˜ very easy to use
``` shell
pip install scarabs
```

### ğŸ“˜ In detail

âœ… 1. Tabular Data
You can refer to tabular_ctr in the examples folder

2. Text Data
You can refer to llm_classification in the examples folder

3. LLM
You can refer to llm_pretrain in the examples folder

4. refer to github https://github.com/zhu2856061/scarabs

#### ğŸ“˜ arguments
â„¹ï¸ task_name_or_path: ä»»åŠ¡åï¼Œæ‰€æœ‰è®­ç»ƒäº§ç”Ÿçš„ä¸­é—´ç»“æœå’Œæœ€ç»ˆç»“æœéƒ½ä¼šåœ¨è¯¥ç›®å½•ä¸‹

â„¹ï¸ data_format: æ•°æ®çš„æ ¼å¼ï¼ŒåŒ…å«[text, csv, json, parquet], tabularæ•°æ®æ¨èç”¨parquetæ ¼å¼-å¹³æ—¶å°†è‡ªå·±çš„æ•°æ®å‡†å¤‡æˆparquetæ ¼å¼ï¼Œ æ–‡æœ¬ç±»æ•°æ®æ¨èé‡‡ç”¨jsonæ ¼å¼

â„¹ï¸ train_file: è®­ç»ƒæ•°æ®çš„è·¯å¾„ï¼Œå¯ä»¥ç»™æ•°æ®çš„æ–‡ä»¶å¤¹(ä¼šè¯»å–æ–‡ä»¶å¤¹å†…çš„æ–‡ä»¶)ï¼Œä¹Ÿå¯ä»¥ç»™æ•°æ®çš„æ–‡ä»¶è·¯å¾„ï¼Œ

â„¹ï¸ valid_file: è¯„ä¼°æ•°æ®çš„è·¯å¾„ï¼Œå¯ä»¥ç»™æ•°æ®çš„æ–‡ä»¶å¤¹(ä¼šè¯»å–æ–‡ä»¶å¤¹å†…çš„æ–‡ä»¶)ï¼Œä¹Ÿå¯ä»¥ç»™æ•°æ®çš„æ–‡ä»¶è·¯å¾„ï¼Œ

â„¹ï¸ test_file: è®­ç»ƒæ•°æ®çš„è·¯å¾„ï¼Œå¯ä»¥ç»™æ•°æ®çš„æ–‡ä»¶å¤¹(ä¼šè¯»å–æ–‡ä»¶å¤¹å†…çš„æ–‡ä»¶)ï¼Œä¹Ÿå¯ä»¥ç»™æ•°æ®çš„æ–‡ä»¶è·¯å¾„ï¼Œ

â„¹ï¸ preprocessing_num_workers: å¯¹æ•°æ®è¿›è¡Œå¤„ç†çš„æ—¶å€™ï¼Œå¯åŠ¨å‡ ä¸ªè¿›ç¨‹workerè¿›è¡Œå¹¶è¡Œå¤„ç†æ•°æ®

â„¹ï¸ labels: æ•°æ®çš„Yæ ‡ï¼Œâš ï¸æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œæ–¹ä¾¿-ã€å¤šç›®æ ‡çš„æ¨¡å‹ã€‘

â„¹ï¸ load_resume_from_checkpoint: æ£€æŸ¥ç‚¹çš„è·¯å¾„-æ–‡ä»¶å¤¹ï¼Œç”¨äºå¯¼å…¥æ£€æŸ¥ç‚¹ï¼Œå¹¶ç»§ç»­è®­ç»ƒï¼Œä¼šå…ˆåŠ è½½æ¨¡å‹-> å†è¿›è¡Œè®­ç»ƒ

â„¹ï¸ incremental_resume_from_checkpoint: å¯¹embeddingå±‚è¿›è¡Œå¢é‡è®­ç»ƒï¼ŒåŸºäºå…ˆå‰çš„æ¨¡å‹ï¼Œå…¶ä¸­çš„ç‰¹å¾å€¼/tokenæ•°é‡æ˜¯å›ºå®šçš„ï¼Œä¸€æ—¦åŸºäºå…ˆå‰æ¨¡å‹è¿›è¡Œä¸‹æ¬¡çš„ç»§ç»­è®­ç»ƒçš„æ—¶å€™ï¼Œå‡ºç°å…¨æ–°çš„ç‰¹å¾å€¼/tokençš„æ—¶å€™ï¼Œå°±ä¼šå‡ºç°æ— æ³•è¯†åˆ«ï¼Œè¢«å½“ä½œUNKå¯¹å¾…äº†ï¼Œæ•…éœ€è¦è®¾ç½®è¿™ä¸ªæ£€æŸ¥ç‚¹çš„è·¯å¾„ï¼Œä¼šå¯åŠ¨å¢é‡è®­ç»ƒ

#### ğŸ”” ctrè®­ç»ƒ [update]

##### æ­£å¸¸è®­ç»ƒ
[å‚è€ƒexamples/tabular]
|_ arguments.yaml è®­ç»ƒæ‰€éœ€è®¾ç½®çš„å‚æ•°
|_ config.json æ¨¡å‹å‚æ•°
|_ main.py è®­ç»ƒä¸»ç¨‹åº

å…¶ä¸­ arguments.yaml æ–‡ä»¶ä¸­å‚æ•°è®¾ç½®å¦‚ä¸‹ï¼š
```yaml
task_name_or_path: "encode"

overwrite_output_dir: true
output_dir: "model"

# data
data_format: "csv"
train_file: "../data/movielens/train"
valid_file: "../data/movielens/valid"
preprocessing_num_workers: 2

# model
# load_resume_from_checkpoint: "./encode/model/checkpoint-1029"
# incremental_resume_from_checkpoint: "./encode/model/checkpoint-1029"

# runtimes metric
do_train: true
seed: 2025
use_cpu: false
report_to: "tensorboard"
save_safetensors: true
save_total_limit: 1
early_stopping_patience: 3
early_stopping_threshold: 1.0e-7
remove_unused_columns: false
metric_for_best_model: "eval_roc_auc"
greater_is_better: true

# optim
optim: "adamw_torch"
learning_rate: 1.0e-3
lr_scheduler_type: "reduce_lr_on_plateau"
lr_scheduler_kwargs: 
  mode: "max"
  factor: 0.1
  patience: 1
  verbose: true
weight_decay: 0
max_grad_norm: 10.0
gradient_accumulation_steps: 1

# data
label_names: ["label"]
per_device_train_batch_size: 4096
per_device_eval_batch_size: 4096
dataloader_num_workers: 4

# view
eval_strategy: "epoch"
logging_strategy: "epoch"
save_strategy: "epoch"
load_best_model_at_end: True
```

config.jsonæ–‡ä»¶è®¾ç½®å‚è€ƒå…·ä½“æ¨¡å‹çš„config[scarabs/nova/models]

main.pyæ–‡ä»¶å¦‚ä¸‹ï¼š
```python
from __future__ import absolute_import, division, print_function
import os
from transformers.hf_argparser import HfArgumentParser
from scarabs.nova.models.ctr_with_dnn import CtrWithDNN, CtrWithDNNConfig
from scarabs.task_factory import TaskArguments, TaskFactoryWithTabularCtr


def feature_engineering(args):
    config = CtrWithDNNConfig.from_pretrained("config.json")
    task = TaskFactoryWithTabularCtr(args, config=config)
    task.create_feature2meta_in_config()

def train(args):
    # # Train
    config = CtrWithDNNConfig.from_pretrained(
        os.path.join(
            args.task_name_or_path,
            "data/meta/config.json",
        )
    )
    task = TaskFactoryWithTabularCtr(args, config)
    task.train(model=CtrWithDNN(config))

def continue_train(args):
    # # Train
    config = CtrWithDNNConfig.from_pretrained(
        os.path.join(
            args.load_resume_from_checkpoint,
            "config.json",
        )
    )
    task = TaskFactoryWithTabularCtr(args, config)
    task.train(model=CtrWithDNN(config))

def incremental_continue_feature_engineering(args):
    config = CtrWithDNNConfig.from_pretrained(
        os.path.join(
            args.incremental_resume_from_checkpoint,
            "config.json",
        )
    )
    task = TaskFactoryWithTabularCtr(args, config=config)
    task.create_feature2meta_in_config()

def incremental_continue_train(args):
    # # Train
    config = CtrWithDNNConfig.from_pretrained(
        os.path.join(
            args.task_name_or_path,
            "data/meta/config.json",
        )
    )
    task = TaskFactoryWithTabularCtr(args, config)
    task.train(model=CtrWithDNN(config))

def eval():
    # Predict
    task = TaskFactoryWithTabularCtr()
    model_path = "./encode/model"
    task.inference_with_load_model(model_path, CtrWithDNN)

    import pandas as pd
    from sklearn.metrics import roc_auc_score

    preds = []
    label = []
    ds = pd.read_csv("../../data/movielens/valid/valid.csv")
    for line in ds.to_dict("records"):
        label.append(line["label"])
        res = task.inference(X=line)
        preds.append(res["logits"][0].item())
    print(roc_auc_score(label, preds))

if __name__ == "__main__":
    parser = HfArgumentParser(TaskArguments)  # type: ignore
    args = parser.parse_yaml_file("arguments.yaml")[0]
    # # feature
    feature_engineering(args)
    # # Train
    train(args)


```


##### ç»§ç»­è®­ç»ƒ
[å‚è€ƒexamples/tabular]
|_ arguments.yaml è®­ç»ƒæ‰€éœ€è®¾ç½®çš„å‚æ•°
|_ model/ æ¨¡å‹æ–‡ä»¶å¤¹ - æ£€æŸ¥ç‚¹checkpoint-**** - config.json å’Œ models.safetensors
|_ main.py è®­ç»ƒä¸»ç¨‹åº

å…¶ä¸­ arguments.yaml æ–‡ä»¶ä¸­å‚æ•°è®¾ç½®å¦‚ä¸‹ï¼š
```yaml
task_name_or_path: "encode"

overwrite_output_dir: true
output_dir: "model"

# data
data_format: "csv"
train_file: "../data/movielens/train"
valid_file: "../data/movielens/valid"
preprocessing_num_workers: 2

# model
load_resume_from_checkpoint: "./model/checkpoint-1029"
# incremental_resume_from_checkpoint: "./encode/model/checkpoint-1029"

# runtimes metric
do_train: true
seed: 2025
use_cpu: false
report_to: "tensorboard"
save_safetensors: true
save_total_limit: 1
early_stopping_patience: 3
early_stopping_threshold: 1.0e-7
remove_unused_columns: false
metric_for_best_model: "eval_roc_auc"
greater_is_better: true

# optim
optim: "adamw_torch"
learning_rate: 1.0e-3
lr_scheduler_type: "reduce_lr_on_plateau"
lr_scheduler_kwargs: 
  mode: "max"
  factor: 0.1
  patience: 1
  verbose: true
weight_decay: 0
max_grad_norm: 10.0
gradient_accumulation_steps: 1

# data
label_names: ["label"]
per_device_train_batch_size: 4096
per_device_eval_batch_size: 4096
dataloader_num_workers: 4

# view
eval_strategy: "epoch"
logging_strategy: "epoch"
save_strategy: "epoch"
load_best_model_at_end: True
```

main.pyæ–‡ä»¶å¦‚ä¸‹ï¼š
```python
from __future__ import absolute_import, division, print_function
import os
from transformers.hf_argparser import HfArgumentParser
from scarabs.nova.models.ctr_with_dnn import CtrWithDNN, CtrWithDNNConfig
from scarabs.task_factory import TaskArguments, TaskFactoryWithTabularCtr


def feature_engineering(args):
    config = CtrWithDNNConfig.from_pretrained("config.json")
    task = TaskFactoryWithTabularCtr(args, config=config)
    task.create_feature2meta_in_config()

def train(args):
    # # Train
    config = CtrWithDNNConfig.from_pretrained(
        os.path.join(
            args.task_name_or_path,
            "data/meta/config.json",
        )
    )
    task = TaskFactoryWithTabularCtr(args, config)
    task.train(model=CtrWithDNN(config))

def continue_train(args):
    # # Train
    config = CtrWithDNNConfig.from_pretrained(
        os.path.join(
            args.load_resume_from_checkpoint,
            "config.json",
        )
    )
    task = TaskFactoryWithTabularCtr(args, config)
    task.train(model=CtrWithDNN(config))

def incremental_continue_feature_engineering(args):
    config = CtrWithDNNConfig.from_pretrained(
        os.path.join(
            args.incremental_resume_from_checkpoint,
            "config.json",
        )
    )
    task = TaskFactoryWithTabularCtr(args, config=config)
    task.create_feature2meta_in_config()

def incremental_continue_train(args):
    # # Train
    config = CtrWithDNNConfig.from_pretrained(
        os.path.join(
            args.task_name_or_path,
            "data/meta/config.json",
        )
    )
    task = TaskFactoryWithTabularCtr(args, config)
    task.train(model=CtrWithDNN(config))

def eval():
    # Predict
    task = TaskFactoryWithTabularCtr()
    model_path = "./encode/model"
    task.inference_with_load_model(model_path, CtrWithDNN)

    import pandas as pd
    from sklearn.metrics import roc_auc_score

    preds = []
    label = []
    ds = pd.read_csv("../../data/movielens/valid/valid.csv")
    for line in ds.to_dict("records"):
        label.append(line["label"])
        res = task.inference(X=line)
        preds.append(res["logits"][0].item())
    print(roc_auc_score(label, preds))

if __name__ == "__main__":
    parser = HfArgumentParser(TaskArguments)  # type: ignore
    args = parser.parse_yaml_file("arguments.yaml")[0]
    # # Train
    continue_train(args)
```



##### å¢é‡inputs ids embeddingç»§ç»­è®­ç»ƒ
[å‚è€ƒexamples/tabular]
|_ arguments.yaml è®­ç»ƒæ‰€éœ€è®¾ç½®çš„å‚æ•°
|_ model/ æ¨¡å‹æ–‡ä»¶å¤¹ - æ£€æŸ¥ç‚¹checkpoint-**** - config.json å’Œ models.safetensors
|_ main.py è®­ç»ƒä¸»ç¨‹åº

å…¶ä¸­ arguments.yaml æ–‡ä»¶ä¸­å‚æ•°è®¾ç½®å¦‚ä¸‹ï¼š
```yaml
task_name_or_path: "encode"

overwrite_output_dir: true
output_dir: "model"

# data
data_format: "csv"
train_file: "../data/movielens/train"
valid_file: "../data/movielens/valid"
preprocessing_num_workers: 2

# model
# load_resume_from_checkpoint: "./model/checkpoint-1029"
incremental_resume_from_checkpoint: "./encode/model/checkpoint-1029"

# runtimes metric
do_train: true
seed: 2025
use_cpu: false
report_to: "tensorboard"
save_safetensors: true
save_total_limit: 1
early_stopping_patience: 3
early_stopping_threshold: 1.0e-7
remove_unused_columns: false
metric_for_best_model: "eval_roc_auc"
greater_is_better: true

# optim
optim: "adamw_torch"
learning_rate: 1.0e-3
lr_scheduler_type: "reduce_lr_on_plateau"
lr_scheduler_kwargs: 
  mode: "max"
  factor: 0.1
  patience: 1
  verbose: true
weight_decay: 0
max_grad_norm: 10.0
gradient_accumulation_steps: 1

# data
label_names: ["label"]
per_device_train_batch_size: 4096
per_device_eval_batch_size: 4096
dataloader_num_workers: 4

# view
eval_strategy: "epoch"
logging_strategy: "epoch"
save_strategy: "epoch"
load_best_model_at_end: True
```

main.pyæ–‡ä»¶å¦‚ä¸‹ï¼š
```python
from __future__ import absolute_import, division, print_function
import os
from transformers.hf_argparser import HfArgumentParser
from scarabs.nova.models.ctr_with_dnn import CtrWithDNN, CtrWithDNNConfig
from scarabs.task_factory import TaskArguments, TaskFactoryWithTabularCtr


def feature_engineering(args):
    config = CtrWithDNNConfig.from_pretrained("config.json")
    task = TaskFactoryWithTabularCtr(args, config=config)
    task.create_feature2meta_in_config()

def train(args):
    # # Train
    config = CtrWithDNNConfig.from_pretrained(
        os.path.join(
            args.task_name_or_path,
            "data/meta/config.json",
        )
    )
    task = TaskFactoryWithTabularCtr(args, config)
    task.train(model=CtrWithDNN(config))

def continue_train(args):
    # # Train
    config = CtrWithDNNConfig.from_pretrained(
        os.path.join(
            args.load_resume_from_checkpoint,
            "config.json",
        )
    )
    task = TaskFactoryWithTabularCtr(args, config)
    task.train(model=CtrWithDNN(config))

def incremental_continue_feature_engineering(args):
    config = CtrWithDNNConfig.from_pretrained(
        os.path.join(
            args.incremental_resume_from_checkpoint,
            "config.json",
        )
    )
    task = TaskFactoryWithTabularCtr(args, config=config)
    task.create_feature2meta_in_config()

def incremental_continue_train(args):
    # # Train
    config = CtrWithDNNConfig.from_pretrained(
        os.path.join(
            args.task_name_or_path,
            "data/meta/config.json",
        )
    )
    task = TaskFactoryWithTabularCtr(args, config)
    task.train(model=CtrWithDNN(config))

def eval():
    # Predict
    task = TaskFactoryWithTabularCtr()
    model_path = "./encode/model"
    task.inference_with_load_model(model_path, CtrWithDNN)

    import pandas as pd
    from sklearn.metrics import roc_auc_score

    preds = []
    label = []
    ds = pd.read_csv("../../data/movielens/valid/valid.csv")
    for line in ds.to_dict("records"):
        label.append(line["label"])
        res = task.inference(X=line)
        preds.append(res["logits"][0].item())
    print(roc_auc_score(label, preds))

if __name__ == "__main__":
    parser = HfArgumentParser(TaskArguments)  # type: ignore
    args = parser.parse_yaml_file("arguments.yaml")[0]
    # # Train
    incremental_continue_feature_engineering(args)
    incremental_continue_train(args)
```
è¿›è¡Œå¢é‡è®­ç»ƒï¼Œåœ¨è®­ç»ƒçš„æ—¥å¿—éƒ¨åˆ†ä¼šæœ‰å¢é‡æ¨¡å‹éƒ¨åˆ†çŸ©é˜µæ”¹å˜çš„æ—¥å¿—æ‰“å°ï¼Œè¯·ç•™æ„
```python
logger.warning(f"{v} shape mismatched, current: {model_dict[v].shape} != history:{state_dict[k].shape}")
```
ç»™å‡ºå½“å‰æ¨¡å‹çŸ©é˜µå’Œå†å²æ¨¡å‹çŸ©é˜µçš„å½¢çŠ¶ä¸ä¸€è‡´ï¼Œè¯·ç•™æ„
```python
logger.warning(f"{key} is updated from history:{history_size} to current:{current_size}")
```
ç»™å‡ºå†å²æ¨¡å‹çŸ©é˜µå·²ç»ä¿®æ­£æˆæ–°çš„çŸ©é˜µå¤§å°


#### ğŸ”” å¤§æ¨¡å‹è®­ç»ƒ [update]

##### 1 çº¯é¢„è®­ç»ƒï¼Œ ä»0-1ï¼Œå¦èµ·ä¸€åº§å±±å³° ï¼Œ ä»¥è®­ç»ƒä¸€ä¸ªqwen3-0.1bçš„æ¨¡å‹ä¸ºä¾‹
ç¬¬ä¸€æ­¥ï¼Œå…ˆé€‰å®šä¸€ä¸ªæ¨¡å‹ï¼Œæ¯”å¦‚ qwen3-0.6b æˆ–è€… qwen3-7béƒ½å¯ä»¥,ä»¥ [qwen3-0.6b](https://huggingface.co/Qwen/Qwen3-0.6B/tree/main)] ä¸ºä¾‹ï¼Œæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ä¸­
tokenizer.json å’Œ tokenizer_config.json å’Œ config.json æ–‡ä»¶

ç¬¬äºŒæ­¥ï¼Œåˆ›å»ºä¸€ä¸ªæ–‡ä»¶å¤¹æ¯”å¦‚ï¼š qwen3-0.1b ï¼Œ ç„¶åä¿®æ”¹config.json æ–‡ä»¶ï¼Œå°†å…¶ä¸­çš„ä¸€äº›å½±å“æ¨¡å‹å¤§å°çš„å‚æ•°æ”¹ä¸ºå°ä¸€äº›ï¼Œæ¯”å¦‚ï¼š
```json
{
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "head_dim": 32,
  "hidden_act": "silu",
  "hidden_size": 128,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 4096,
  "max_window_layers": 28,
  "model_type": "qwen3",
  "num_attention_heads": 8,
  "num_hidden_layers": 6,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.51.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}
```
ç¬¬ä¸‰æ­¥ï¼š å‡†å¤‡æ•°æ®ï¼Œæ•°æ®æ ·å¼å¦‚ä¸‹ï¼š
{"text": "æ ¹æ®æè¿°ï¼Œ..."}
{"text": "å¯¹äºä¸€å60å²ç”·æ€§æ‚£è€…ï¼Œ..."}

ç¬¬å››æ­¥ï¼š å‡†å¤‡è®­ç»ƒå‚æ•°æ–‡ä»¶ï¼Œå‚ç…§ [arguments.yaml](./examples/pretrain/arguments.yaml)

ç¬¬äº”æ­¥ï¼šå†™è®­ç»ƒè„šæœ¬ï¼Œå‚ç…§ [train.py](./examples/pretrain/train.py)

ç¬¬å…­æ­¥ï¼š æ‰§è¡Œè®­ç»ƒï¼Œ 
``` shell
torchrun --standalone --nnodes=1 --nproc_per_node=1 main.py
```


##### 2 ç»§ç»­é¢„è®­ç»ƒï¼Œ ä»¥è®­ç»ƒä¸€ä¸ªqwen3-0.1bçš„æ¨¡å‹ä¸ºä¾‹
ç¬¬ä¸€æ­¥ï¼Œå…ˆé€‰å®šä¸€ä¸ªæ¨¡å‹ï¼Œæ¯”å¦‚ qwen3-0.6b æˆ–è€… qwen3-7béƒ½å¯ä»¥,ä»¥ [qwen3-0.6b](https://huggingface.co/Qwen/Qwen3-0.6B/tree/main)] ä¸ºä¾‹ï¼Œéœ€è¦ä¸‹è½½æ¨¡å‹çš„æ‰€æœ‰æ–‡ä»¶ï¼Œå¹¶ä¿å­˜åœ¨æŒ‡å®šçš„ç›®å½•ä¸‹ï¼Œæ¯”å¦‚ï¼šè¿™é‡Œå¯ä»¥æ‹¿çº¯é¢„è®­ç»ƒçš„é‚£ä¸ªæ¨¡å‹æ¥è¿›è¡Œç»§ç»­é¢„è®­ç»ƒ[qwen3-0.1b](./examples/continue_pretrain/qwen3-0.2b/)
å…¶ä»–æ­¥éª¤ï¼ˆå»æ‰ä¸Šè¿°çš„ç¬¬äºŒæ­¥ï¼‰åŒä¸Š
**ç‰¹åˆ«** éœ€è¦å¯¹train.pyè¿›è¡Œä¿®æ”¹ï¼Œå‚ç…§ [train.py](./examples/continue_pretrain/train.py)


##### 2 å¾®è°ƒè®­ç»ƒï¼Œä»¥è®­ç»ƒä¸€ä¸ªqwen3-0.1bçš„æ¨¡å‹ä¸ºä¾‹
ç¬¬ä¸€æ­¥ï¼Œå…ˆé€‰å®šä¸€ä¸ªæ¨¡å‹ï¼Œæ¯”å¦‚ qwen3-0.6b æˆ–è€… qwen3-7béƒ½å¯ä»¥,ä»¥ [qwen3-0.6b](https://huggingface.co/Qwen/Qwen3-0.6B/tree/main)] ä¸ºä¾‹ï¼Œéœ€è¦ä¸‹è½½æ¨¡å‹çš„æ‰€æœ‰æ–‡ä»¶ï¼Œå¹¶ä¿å­˜åœ¨æŒ‡å®šçš„ç›®å½•ä¸‹ï¼Œæ¯”å¦‚ï¼šè¿™é‡Œå¯ä»¥æ‹¿çº¯é¢„è®­ç»ƒçš„é‚£ä¸ªæ¨¡å‹æ¥è¿›è¡Œç»§ç»­é¢„è®­ç»ƒ[qwen3-0.1b](./examples/continue_pretrain/qwen3-0.2b/)

ç¬¬äºŒæ­¥ï¼š å‡†å¤‡æ•°æ®ï¼Œæ•°æ®æ ·å¼å¦‚ä¸‹- è¿™é‡Œé‡‡ç”¨ prompt + completion æ ·å¼ï¼ˆè¯¥æ–¹å¼æœ€å¥½ç®¡ç†ï¼‰ï¼š
{"prompt": [{"role": "user", "content": "What color is the sky?"}],"completion": [{"role": "assistant", "content": "It is blue."}]}
{"prompt": [{"role": "user", "content": "What color is the sky?"}],"completion": [{"role": "assistant", "content": "It is blue."}]}

ç¬¬å››æ­¥ï¼š å‡†å¤‡è®­ç»ƒå‚æ•°æ–‡ä»¶ï¼Œå‚ç…§ [arguments.yaml](./examples/sft/arguments.yaml)

ç¬¬äº”æ­¥ï¼šå†™è®­ç»ƒè„šæœ¬ï¼Œå‚ç…§ [train.py](./examples/sft/train.py)

ç¬¬å…­æ­¥ï¼š æ‰§è¡Œè®­ç»ƒï¼Œ 
``` shell
torchrun --standalone --nnodes=1 --nproc_per_node=1 main.py
```
