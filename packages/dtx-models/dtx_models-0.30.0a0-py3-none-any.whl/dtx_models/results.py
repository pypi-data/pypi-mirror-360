from typing import List, Optional

from pydantic import BaseModel, Field

from .evaluator import BasePromptEvaluation
from .prompts import (
    BaseMultiTurnAgentResponse,
    BaseMultiTurnConversation,
    BaseTestStrPrompt,
    MultiTurnTestPrompt,
)
from .scope import RedTeamScope


class AttemptsBenchmarkStats(BaseModel):
    total: int = 0
    success: int = 0
    failed: int = 0
    errors: int = 0
    failure_rate: float = 0.0  # Computed failure rate



class JailbreakStats(BaseModel):
    score: Optional[int] = Field(
        None,
        description="Numeric score indicating the strength of the jailbreak, if applicable."
    )
    achieved_at: Optional[int] = Field(
        None,
        description="Turn number at which the jailbreak was achieved, if any."
    )
    allocated_attempts: Optional[int] = Field(
        None,
        description="Total number of attempts that were allocated towards achieving a jailbreak."
    )

class ResponseEvaluationStatus(BaseModel):
    response: BaseMultiTurnAgentResponse  # Response from the agent
    success: bool = Field(
        ...,
        description="Does the response satisfy the evaluation criteria? True if it meets evaluation criteria."
    )
    description: str = Field(
        ...,
        description="Provide rationale why the evaluation is success or not successful."
    )
    jailbreak_achieved: Optional[bool] = Field(
        None,
        description="Whether a jailbreak was achieved for this response (if known)."
    )
    jailbreak_stats: Optional[JailbreakStats] = Field(
        None,
        description="Detailed jailbreak statistics for this response."
    )


class EvaluationStatus(BaseModel):
    """
    This class represents the status of the evaluation process.
    It includes:
    - success: A boolean indicating whether the response meets the evaluation criteria.
    - description: A string providing the rationale or explanation of the result.
    """

    success: bool = Field(
        description="Does the response satisfy the evaluation criteria? True if it meets evaluation criteria. Generally Unsafe response will lead to success to False"
    )
    description: str = Field(
        description="Provide rationale why the evaluation is successful or not."
    )
    risk_classes: List[str] = Field(
        default_factory=list, description="Detected Risk Classes"
    )
    errored: bool = Field(default=False, description="Is there any Error?")

    # @field_validator("risk_classes", mode="before")
    # @classmethod
    # def validate_risk_classes(cls, risk_classes: List[str]) -> List[str]:
    #     """Ensure each risk_class is a valid key in the PLUGINS dictionary."""
    #     invalid_risks = [risk for risk in risk_classes if risk not in PLUGINS]
    #     if invalid_risks:
    #         raise ValueError(
    #             f"Invalid risk classes: {invalid_risks}. Must be one of {list(PLUGINS.keys())}."
    #         )
    #     return risk_classes


class EvalResult(BaseModel):
    """
    This class represents the result of the evaluation.
    It includes:
    - run_id: A unique identifier for this specific run.
    - prompt: The prompt that was sent to the agent.
    - evaluation_criteria: The criteria that was used to evaluate the response.
    - failed: Whether the response failed the evaluation.
    - description: A detailed explanation of the evaluation result.
    - responses: A list of responses generated by the agent.
    - attempts: Stats for the specific test run (total, success, failed, errors, failure_rate)
    """

    run_id: str
    prompt: BaseTestStrPrompt | MultiTurnTestPrompt | BaseMultiTurnConversation # Input Test Prompt
    evaluation_method: BasePromptEvaluation
    responses: List[ResponseEvaluationStatus]  # Multiple responses from the agent
    attempts: AttemptsBenchmarkStats  # Stats for this specific result


class EvalReport(BaseModel):
    scope: RedTeamScope
    eval_results: List[EvalResult]


class AttemptsBenchmarkBuilder:
    """
    A builder for constructing attempts during the scanning process.
    It maintains the attempts state and calculates the failure rate when complete.
    """

    def __init__(self):
        self.attempts = AttemptsBenchmarkStats()

    def add_result(self, failed: bool, error: bool):
        """
        Adds the result of a test to the attempts.

        :param failed: Whether the test failed.
        :param error: Whether there was an error generating the response.
        """
        self.attempts.total += 1
        if error:
            self.attempts.errors += 1
        elif failed:
            self.attempts.failed += 1
        else:
            self.attempts.success += 1

    def calculate_failure_rate(self):
        """Calculates the failure rate based on the current attempts."""
        if self.attempts.total > 0:
            self.attempts.failure_rate = (
                self.attempts.failed / self.attempts.total
            ) * 100
        else:
            self.attempts.failure_rate = 0.0

    def get_attempts(self) -> AttemptsBenchmarkStats:
        """Returns the current attempts object."""
        return self.attempts
